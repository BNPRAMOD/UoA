{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BNPRAMOD/UoA/blob/main/Adaptive%20Multi-Teacher%20Distillation%20for%20Data-Efficient%20Vision%20Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "outputId": "92416d92-16ce-4dbc-cada-7d0be27eadfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "52594570-a99f-448f-96ef-d40443fc4783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "c837055e-7f00-4b34-a904-4a7b161a91b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 22.93 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "fec1b49c-7ad5-4ff0-8f92-ff13a3630217"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "bfcd75dd-1c0c-4e5c-e6c3-44a3ccd5c7d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "id": "rNoLOzs5xUxa",
        "outputId": "a682d51d-3437-4ceb-c140-30357ed06b79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install -U pip setuptools wheel\n",
        "!pip -q install -U \"timm>=1.0.0\""
      ],
      "metadata": {
        "id": "q0Mim13um2k4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "395a6e9c-68d3-46c1-dc65-59fe0aea82d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 1.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "21aedd51-3736-4aaf-9d4e-ced39500e01e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runtime → Restart runtime"
      ],
      "metadata": {
        "id": "M0ZDDe3uvU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install timm submitit"
      ],
      "metadata": {
        "id": "H3T5zLnQukuK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "3d856743-9592-4f61-ede0-bc6ab2d51664"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/content/deit/models.py:62: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:77: UserWarning: Overwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:92: UserWarning: Overwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:107: UserWarning: Overwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:122: UserWarning: Overwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:137: UserWarning: Overwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:152: UserWarning: Overwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:167: UserWarning: Overwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "3b4557a7-8524-44f7-d671-596ac7424bf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "1.0.24\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vrbd2wbQyqMV",
        "outputId": "7949bddb-04de-434e-9eed-2bd905a8452d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "b390b717-10d6-4586-aeda-bb1d1c124b36",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "06473548-67ca-4e45-da47-5300390514ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "ff6dcbb0-8e2c-48b8-fc49-8b7a499ae2e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n04507155\n",
            "/content/tiny-imagenet-200/val/n04311004\n",
            "/content/tiny-imagenet-200/val/n09332890\n",
            "/content/tiny-imagenet-200/val/n02917067\n",
            "/content/tiny-imagenet-200/val/n07579787\n",
            "/content/tiny-imagenet-200/val/n02123394\n",
            "/content/tiny-imagenet-200/val/n04356056\n",
            "/content/tiny-imagenet-200/val/n01774384\n",
            "/content/tiny-imagenet-200/val/n02437312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "4ff5b42e-63ef-493c-c5da-ce27b88600ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched augment.py for timm compatibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!sed -n '1,200p' models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cm_gVMz1-_",
        "outputId": "fb9f892a-3a01-47c6-ce87-880175b2d7db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "# Copyright (c) 2015-present, Facebook, Inc.\n",
            "# All rights reserved.\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from functools import partial\n",
            "\n",
            "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
            "from timm.models.registry import register_model\n",
            "from timm.models.layers import trunc_normal_\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
            "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
            "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
            "    'deit_base_distilled_patch16_384',\n",
            "]\n",
            "\n",
            "\n",
            "class DistilledVisionTransformer(VisionTransformer):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
            "        num_patches = self.patch_embed.num_patches\n",
            "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
            "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
            "\n",
            "        trunc_normal_(self.dist_token, std=.02)\n",
            "        trunc_normal_(self.pos_embed, std=.02)\n",
            "        self.head_dist.apply(self._init_weights)\n",
            "\n",
            "    def forward_features(self, x):\n",
            "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
            "        # with slight modifications to add the dist_token\n",
            "        B = x.shape[0]\n",
            "        x = self.patch_embed(x)\n",
            "\n",
            "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
            "        dist_token = self.dist_token.expand(B, -1, -1)\n",
            "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
            "\n",
            "        x = x + self.pos_embed\n",
            "        x = self.pos_drop(x)\n",
            "\n",
            "        for blk in self.blocks:\n",
            "            x = blk(x)\n",
            "\n",
            "        x = self.norm(x)\n",
            "        return x[:, 0], x[:, 1]\n",
            "\n",
            "    def forward(self, x):\n",
            "        x, x_dist = self.forward_features(x)\n",
            "        x = self.head(x)\n",
            "        x_dist = self.head_dist(x_dist)\n",
            "        if self.training:\n",
            "            return x, x_dist\n",
            "        else:\n",
            "            # during inference, return the average of both classifier predictions\n",
            "            return (x + x_dist) / 2\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_384(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "171d855a-38a3-4846-87a6-2fe12e36909c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"pretrained_cfg\" /content/deit/models.py"
      ],
      "metadata": {
        "id": "lxOmdCb90Ymg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "81c39950-4ccd-46ca-fc94-2ad216d8fd71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "V409XjDO1cdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"cache_dir\" /content/deit/models.py"
      ],
      "metadata": {
        "id": "LIFoOP5c1dbu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        "  --model deit_tiny_patch16_224 \\\n",
        "  --data-path /content/tiny-imagenet-200 \\\n",
        "  --epochs 10 \\\n",
        "  --batch-size 128 \\\n",
        "  --num_workers 4 \\\n",
        "  --opt adamw \\\n",
        "  --lr 5e-4 \\\n",
        "  --weight-decay 0.05 \\\n",
        "  --sched cosine \\\n",
        "  --aa rand-m9-mstd0.5 \\\n",
        "  --reprob 0.25 \\\n",
        "  --remode pixel \\\n",
        "  --recount 1 \\\n",
        "  --output_dir /content/deit_runs/tiny_imagenet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "2aea1063-4f2e-4268-b3e4-71d1b8374d69"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/content/deit/models.py:62: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:77: UserWarning: Overwriting deit_small_patch16_224 in registry with models.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:92: UserWarning: Overwriting deit_base_patch16_224 in registry with models.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:107: UserWarning: Overwriting deit_tiny_distilled_patch16_224 in registry with models.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:122: UserWarning: Overwriting deit_small_distilled_patch16_224 in registry with models.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:137: UserWarning: Overwriting deit_base_distilled_patch16_224 in registry with models.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:152: UserWarning: Overwriting deit_base_patch16_384 in registry with models.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/deit/models.py:167: UserWarning: Overwriting deit_base_distilled_patch16_384 in registry with models.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=10, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/deit/main.py\", line 489, in <module>\n",
            "    main(args)\n",
            "  File \"/content/deit/main.py\", line 263, in main\n",
            "    model = create_model(\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py\", line 138, in create_model\n",
            "    model = create_fn(\n",
            "            ^^^^^^^^^^\n",
            "  File \"/content/deit/models.py\", line 64, in deit_tiny_patch16_224\n",
            "    model = VisionTransformer(\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "TypeError: VisionTransformer.__init__() got an unexpected keyword argument 'pretrained_cfg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}