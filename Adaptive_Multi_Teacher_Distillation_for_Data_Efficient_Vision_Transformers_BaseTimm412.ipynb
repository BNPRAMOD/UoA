{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b46a967-6a70-4539-9f34-063b4c3039b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "a85d025e-c811-4a26-d27c-986f38d93222"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "bd3e72ba-1d2d-4195-9776-ec19757b6d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 22.75 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "7d137041-f72d-4ef1-a388-448acc5b18b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "2b095442-e302-4bab-d4b4-d60279a61ccc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "ef1077b8-11ad-4a4f-e222-48909297ba55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "# !pip -q install timm==0.6.13 submitit\n",
        "!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q uninstall -y timm\n",
        "#!pip -q install -U pip setuptools wheel\n",
        "#!pip -q install -U \"timm>=1.0.0\""
      ],
      "metadata": {
        "id": "q0Mim13um2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\"\n",
        "#0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "7d81256c-606f-4df5-c3a6-03d7691acb97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "1163b470-e91a-460f-ad73-c941e3a3726c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runtime → Restart runtime"
      ],
      "metadata": {
        "id": "M0ZDDe3uvU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install timm submitit"
      ],
      "metadata": {
        "id": "H3T5zLnQukuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "16ba2e2a-4aae-4028-8a75-391180a27ab9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "7eef0459-1fc0-4d57-9b61-7c2b9fda3f1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.4.12\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "31ab67fd-2263-4d7c-b52a-992bd2de0d07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "17e7909e-92ff-4ffd-b228-959d70706b28",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "dde09e95-fcdc-455c-cc77-e511467c9033"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "088e6052-293a-4b87-8f00-9f25525f860f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n03891332\n",
            "/content/tiny-imagenet-200/val/n04275548\n",
            "/content/tiny-imagenet-200/val/n07695742\n",
            "/content/tiny-imagenet-200/val/n01950731\n",
            "/content/tiny-imagenet-200/val/n01770393\n",
            "/content/tiny-imagenet-200/val/n03393912\n",
            "/content/tiny-imagenet-200/val/n03179701\n",
            "/content/tiny-imagenet-200/val/n09256479\n",
            "/content/tiny-imagenet-200/val/n04070727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "b2b0542b-f927-4067-c8b3-01e6568bdcdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Feb  9  2015 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Jan 25 23:11 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Jan 25 22:50 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "9e109fbe-53d5-4b3f-d0fa-d02b69cf5922"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "ℹ️ Expected import line not found; augment.py may already be patched or different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!sed -n '1,200p' models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cm_gVMz1-_",
        "outputId": "a1eefaa3-e4a7-40d8-fdde-31dc1913714d",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "# Copyright (c) 2015-present, Facebook, Inc.\n",
            "# All rights reserved.\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from functools import partial\n",
            "\n",
            "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
            "from timm.models.registry import register_model\n",
            "from timm.models.layers import trunc_normal_\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
            "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
            "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
            "    'deit_base_distilled_patch16_384',\n",
            "]\n",
            "\n",
            "\n",
            "class DistilledVisionTransformer(VisionTransformer):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
            "        num_patches = self.patch_embed.num_patches\n",
            "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
            "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
            "\n",
            "        trunc_normal_(self.dist_token, std=.02)\n",
            "        trunc_normal_(self.pos_embed, std=.02)\n",
            "        self.head_dist.apply(self._init_weights)\n",
            "\n",
            "    def forward_features(self, x):\n",
            "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
            "        # with slight modifications to add the dist_token\n",
            "        B = x.shape[0]\n",
            "        x = self.patch_embed(x)\n",
            "\n",
            "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
            "        dist_token = self.dist_token.expand(B, -1, -1)\n",
            "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
            "\n",
            "        x = x + self.pos_embed\n",
            "        x = self.pos_drop(x)\n",
            "\n",
            "        for blk in self.blocks:\n",
            "            x = blk(x)\n",
            "\n",
            "        x = self.norm(x)\n",
            "        return x[:, 0], x[:, 1]\n",
            "\n",
            "    def forward(self, x):\n",
            "        x, x_dist = self.forward_features(x)\n",
            "        x = self.head(x)\n",
            "        x_dist = self.head_dist(x_dist)\n",
            "        if self.training:\n",
            "            return x, x_dist\n",
            "        else:\n",
            "            # during inference, return the average of both classifier predictions\n",
            "            return (x + x_dist) / 2\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RizknqA6MBXb",
        "outputId": "61e6a6c0-fc7c-4514-96d0-48be2be65c03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "ls: cannot access 'multiteacher_loss.py': No such file or directory\n",
            "✅ old file removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Normalize teacher weights along the last dimension.\n",
        "    lmb: (T,) or (B,T)\n",
        "    \"\"\"\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(\n",
        "    teacher_logits: Dict[str, torch.Tensor],\n",
        "    teacher_order: List[str],\n",
        "    lambdas: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    teacher_logits: dict{name -> (B,C)}\n",
        "    teacher_order: stable list of teacher names\n",
        "    lambdas: (B,T) or (T,)\n",
        "    returns: (B,C)\n",
        "    \"\"\"\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)  # (B,T,C)\n",
        "\n",
        "    lambdas = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lambdas.dim() == 1:\n",
        "        lambdas = lambdas.unsqueeze(0).expand(stacked.size(0), -1)  # (B,T)\n",
        "\n",
        "    return (stacked * lambdas.unsqueeze(-1)).sum(dim=1)  # (B,C)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    KL( teacher || student ) with temperature scaling.\n",
        "    \"\"\"\n",
        "    T = float(T)\n",
        "    teacher_logits = teacher_logits.to(student_logits.device)\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Hard distillation: use teacher argmax labels.\n",
        "    \"\"\"\n",
        "    y_t = teacher_logits.argmax(dim=-1)\n",
        "    return F.cross_entropy(student_logits, y_t)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Teachers\n",
        "# ---------------------------\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Loads timm teachers (ImageNet pretrained), freezes them, and returns logits dict.\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        models = {}\n",
        "        for name in teacher_names:\n",
        "            m = timm.create_model(name, pretrained=True, num_classes=1000)\n",
        "            m.eval()\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            models[name] = m.to(device)\n",
        "        self.models = nn.ModuleDict(models)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "class TeacherLogitAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps teacher logits (1000) -> student_num_classes (e.g., 200 for Tiny-ImageNet).\n",
        "    This adapter MUST be optimized in main.py by adding its params to optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_keys: List[str], student_num_classes: int):\n",
        "        super().__init__()\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            k: nn.Linear(1000, student_num_classes, bias=False) for k in teacher_keys\n",
        "        })\n",
        "\n",
        "    def forward(self, teacher_logits: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        return {k: self.adapters[k](v) for k, v in teacher_logits.items()}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# HDTSE (mixup-safe)\n",
        "# ---------------------------\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic teacher weights based on teacher confidence on the target.\n",
        "    Works for:\n",
        "      - hard labels: targets shape (B,)\n",
        "      - mixup/cutmix soft labels: targets shape (B,C)\n",
        "    \"\"\"\n",
        "    def __init__(self, temp: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.temp = float(temp)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        student_logits: torch.Tensor,  # unused, but kept for extensibility\n",
        "        teacher_logits: Dict[str, torch.Tensor],\n",
        "        teacher_order: List[str],\n",
        "        targets: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "        stacked = torch.stack(logits_list, dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Case A: hard labels (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Case B: soft labels from mixup/cutmix (B,C)\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)  # (B,C)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)  # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Multi-teacher distillation loss\n",
        "# ---------------------------\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Called from engine as:\n",
        "        loss = criterion(samples, outputs, targets)\n",
        "\n",
        "    - base_criterion: CE / SoftTargetCE etc (from timm)\n",
        "    - distillation_type: \"soft\" or \"hard\"\n",
        "    - alpha: weight for KD\n",
        "    - tau: temperature for KD\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion: nn.Module,\n",
        "        student_num_classes: int,\n",
        "        teacher_names: List[str],\n",
        "        distillation_type: str = \"soft\",\n",
        "        alpha: float = 0.5,\n",
        "        tau: float = 2.0,\n",
        "        device: Optional[torch.device] = None,\n",
        "        use_adapter: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = distillation_type\n",
        "        self.alpha = float(alpha)\n",
        "        self.tau = float(tau)\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, device=self.device)\n",
        "        self.teacher_order = self.teachers.teacher_order\n",
        "\n",
        "        self.adapter = TeacherLogitAdapter(self.teacher_order, student_num_classes).to(self.device) if use_adapter else None\n",
        "        self.hdtse = HDTSEConfidence(temp=1.0)\n",
        "\n",
        "        # Optional: store last batch lambdas for debugging/logging\n",
        "        self.last_lambdas: Optional[torch.Tensor] = None  # (B,T)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        # Base loss (handles hard or soft labels depending on what base_criterion is)\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        # Teacher forward (frozen)\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)  # dict of (B,1000)\n",
        "\n",
        "        # Adapt to student class space\n",
        "        if self.adapter is not None:\n",
        "            t_logits = self.adapter(t_logits)  # dict of (B,C_student)\n",
        "\n",
        "        # Dynamic weights (mixup-safe)\n",
        "        order = self.teacher_order\n",
        "        with torch.no_grad():\n",
        "            lambdas = self.hdtse(outputs.detach(), t_logits, order, targets)  # (B,T)\n",
        "        self.last_lambdas = lambdas\n",
        "\n",
        "        # Fuse teacher logits\n",
        "        fused = fuse_logits(t_logits, order, lambdas)  # (B,C_student)\n",
        "\n",
        "        # KD loss\n",
        "        if self.distillation_type == \"hard\":\n",
        "            distill_loss = kd_hard(outputs, fused)\n",
        "        else:\n",
        "            distill_loss = kd_soft(outputs, fused, self.tau)\n",
        "\n",
        "        # Final loss\n",
        "        return (1.0 - self.alpha) * base_loss + self.alpha * distill_loss\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4jzkzbMHD-",
        "outputId": "af19da7c-9380-44d9-a0c5-d5fff4476d72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "File written: multiteacher_loss.py\n",
            "File size (bytes): 7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, shutil\n",
        "from datetime import datetime\n",
        "import py_compile\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "assert MAIN.exists(), f\"main.py not found at {MAIN}\"\n",
        "\n",
        "# -----------------------\n",
        "# Backup\n",
        "# -----------------------\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "bak = MAIN.with_name(f\"main.py.bak_finalpatch_{ts}\")\n",
        "shutil.copy2(MAIN, bak)\n",
        "print(\"✅ Backup created:\", bak)\n",
        "\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# -----------------------\n",
        "# Patch A: Ensure import exists\n",
        "# -----------------------\n",
        "if \"from multiteacher_loss import MultiTeacherDistillationLoss\" not in txt:\n",
        "    if \"from losses import DistillationLoss\" in txt:\n",
        "        txt = txt.replace(\n",
        "            \"from losses import DistillationLoss\",\n",
        "            \"from losses import DistillationLoss\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\",\n",
        "            1,\n",
        "        )\n",
        "        print(\"✅ Added MultiTeacherDistillationLoss import\")\n",
        "    else:\n",
        "        # fallback insertion near end of imports\n",
        "        txt = txt.replace(\n",
        "            \"import utils\",\n",
        "            \"import utils\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\",\n",
        "            1,\n",
        "        )\n",
        "        print(\"✅ Added MultiTeacherDistillationLoss import (fallback)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch B: Add --teacher-models CLI arg (after --teacher-path)\n",
        "# -----------------------\n",
        "if \"--teacher-models\" not in txt:\n",
        "    anchor = \"parser.add_argument('--teacher-path', type=str, default='')\"\n",
        "    addition = (\n",
        "        \"    parser.add_argument('--teacher-models', type=str, default='',\\n\"\n",
        "        \"                        help='Comma-separated timm model names for multi-teacher distillation')\\n\"\n",
        "    )\n",
        "    if anchor in txt:\n",
        "        txt = txt.replace(anchor, anchor + \"\\n\" + addition, 1)\n",
        "        print(\"✅ Added --teacher-models argument\")\n",
        "    else:\n",
        "        raise RuntimeError(\"❌ Could not find '--teacher-path' to insert '--teacher-models' after it.\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch C: Relax teacher-path assertion (teacher-path OR teacher-models)\n",
        "# -----------------------\n",
        "# Replace strict assertion if present\n",
        "txt, n_assert = re.subn(\n",
        "    r\"assert\\s+args\\.teacher_path\\s*,\\s*['\\\"]need to specify teacher-path when using distillation['\\\"]\",\n",
        "    \"assert (args.teacher_path or args.teacher_models), 'need to specify teacher-path OR teacher-models when using distillation'\",\n",
        "    txt,\n",
        ")\n",
        "if n_assert:\n",
        "    print(f\"✅ Relaxed teacher-path assertion ({n_assert} replacements)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch D: Allow finetune + MULTI-teacher distillation (block only single-teacher)\n",
        "# -----------------------\n",
        "finetune_pat = re.compile(\n",
        "    r\"if\\s+args\\.distillation_type\\s*!=\\s*'none'\\s+and\\s+args\\.finetune\\s+and\\s+not\\s+args\\.eval:\\s*\\n\\s*raise\\s+NotImplementedError\\([\\\"']Finetuning with distillation not yet supported[\\\"']\\)\",\n",
        "    re.MULTILINE,\n",
        ")\n",
        "if finetune_pat.search(txt):\n",
        "    txt = finetune_pat.sub(\n",
        "        \"if args.distillation_type != 'none' and args.finetune and not args.eval and not args.teacher_models:\\n\"\n",
        "        \"        raise NotImplementedError(\\\"Finetuning with single-teacher distillation not yet supported\\\")\",\n",
        "        txt,\n",
        "        count=1,\n",
        "    )\n",
        "    print(\"✅ Patched finetune+distill guard (multi-teacher allowed)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch E: Ensure multi-teacher branch prints confirmation\n",
        "# (print teachers list right after parsing teacher_names)\n",
        "# -----------------------\n",
        "# Insert after teacher_names assignment if not already present\n",
        "if \"Multi-teacher distillation enabled. Teachers:\" not in txt:\n",
        "    pat_teacher_names = re.compile(\n",
        "        r\"(teacher_names\\s*=\\s*\\[t\\.strip\\(\\)\\s*for\\s*t\\s*in\\s*args\\.teacher_models\\.split\\(','\\)\\s*if\\s*t\\.strip\\(\\)\\]\\s*\\n)\",\n",
        "        re.MULTILINE,\n",
        "    )\n",
        "    ins = (\n",
        "        r\"\\1\"\n",
        "        \"            print(\\\"✅ Multi-teacher distillation enabled. Teachers:\\\", teacher_names)\\n\"\n",
        "    )\n",
        "    txt2, n = pat_teacher_names.subn(ins, txt, count=1)\n",
        "    if n == 1:\n",
        "        txt = txt2\n",
        "        print(\"✅ Added multi-teacher confirmation print\")\n",
        "    else:\n",
        "        print(\"ℹ️ Could not auto-insert teacher confirmation print (teacher_names line not matched).\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch F: Insert adapter->optimizer hook after MultiTeacherDistillationLoss(...)\n",
        "# -----------------------\n",
        "if \"Added adapter parameters to optimizer\" not in txt:\n",
        "    mt_pat = re.compile(\n",
        "        r\"(criterion\\s*=\\s*MultiTeacherDistillationLoss\\([\\s\\S]*?\\)\\n)\",\n",
        "        re.MULTILINE,\n",
        "    )\n",
        "    adapter_block = (\n",
        "        \"\\n\"\n",
        "        \"            # ✅ IMPORTANT: train the adapter (otherwise KD hurts accuracy)\\n\"\n",
        "        \"            if hasattr(criterion, \\\"adapter\\\") and criterion.adapter is not None:\\n\"\n",
        "        \"                optimizer.add_param_group({\\n\"\n",
        "        \"                    \\\"params\\\": criterion.adapter.parameters(),\\n\"\n",
        "        \"                    \\\"lr\\\": args.lr,\\n\"\n",
        "        \"                    \\\"weight_decay\\\": 0.0,\\n\"\n",
        "        \"                })\\n\"\n",
        "        \"                print(\\\"✅ Added adapter parameters to optimizer\\\")\\n\"\n",
        "    )\n",
        "    txt2, n = mt_pat.subn(r\"\\1\" + adapter_block, txt, count=1)\n",
        "    if n == 1:\n",
        "        txt = txt2\n",
        "        print(\"✅ Inserted adapter->optimizer hook after MultiTeacherDistillationLoss(...)\")\n",
        "    else:\n",
        "        print(\"ℹ️ Could not insert adapter hook automatically (MultiTeacherDistillationLoss block not found uniquely).\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch 7 (CRITICAL): Fix scheduler creation safely\n",
        "#   - remove ALL existing create_scheduler lines anywhere\n",
        "#   - insert exactly once BEFORE 'output_dir = Path(args.output_dir)' with 4-space indent\n",
        "# -----------------------\n",
        "txt = re.sub(r\"^\\s*lr_scheduler\\s*,\\s*_\\s*=\\s*create_scheduler\\(\\s*args\\s*,\\s*optimizer\\s*\\)\\s*$\", \"\", txt, flags=re.MULTILINE)\n",
        "\n",
        "insert_anchor = \"    output_dir = Path(args.output_dir)\"\n",
        "if insert_anchor not in txt:\n",
        "    raise RuntimeError(\"❌ Could not find 'output_dir = Path(args.output_dir)' anchor to insert scheduler safely.\")\n",
        "\n",
        "if \"lr_scheduler, _ = create_scheduler(args, optimizer)\" not in txt:\n",
        "    txt = txt.replace(\n",
        "        insert_anchor,\n",
        "        \"    lr_scheduler, _ = create_scheduler(args, optimizer)\\n\\n\" + insert_anchor,\n",
        "        1,\n",
        "    )\n",
        "    print(\"✅ Inserted scheduler creation safely before output_dir\")\n",
        "\n",
        "# Clean up excessive blank lines caused by removals\n",
        "txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
        "\n",
        "# -----------------------\n",
        "# Write + compile check\n",
        "# -----------------------\n",
        "MAIN.write_text(txt)\n",
        "print(\"✅ Wrote patched main.py\")\n",
        "\n",
        "py_compile.compile(str(MAIN), doraise=True)\n",
        "print(\"✅ main.py compiles successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdJZ4F-NoE-",
        "outputId": "8b4ac125-01b5-4dda-ba9b-3548481c23f7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Backup created: /content/deit/main.py.bak_finalpatch_20260126_001359\n",
            "✅ Inserted scheduler creation safely before output_dir\n",
            "✅ Wrote patched main.py\n",
            "✅ main.py compiles successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "236589a9-748d-4018-f4e2-849e76cd5c50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"pretrained_cfg\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxOmdCb90Ymg",
        "outputId": "dd6a7436-be78-4419-ce52-30b599619f04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('pretrained_cfg', None)\n",
            "66:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "67:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "73:    kwargs.pop('pretrained_cfg', None)\n",
            "74:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "75:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "92:    kwargs.pop('pretrained_cfg', None)\n",
            "93:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "94:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "100:    kwargs.pop('pretrained_cfg', None)\n",
            "101:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "102:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "119:    kwargs.pop('pretrained_cfg', None)\n",
            "120:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "121:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "127:    kwargs.pop('pretrained_cfg', None)\n",
            "128:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "129:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "146:    kwargs.pop('pretrained_cfg', None)\n",
            "147:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "148:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "154:    kwargs.pop('pretrained_cfg', None)\n",
            "155:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "156:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "173:    kwargs.pop('pretrained_cfg', None)\n",
            "174:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "175:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "181:    kwargs.pop('pretrained_cfg', None)\n",
            "182:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "183:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "200:    kwargs.pop('pretrained_cfg', None)\n",
            "201:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "202:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "208:    kwargs.pop('pretrained_cfg', None)\n",
            "209:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "210:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "227:    kwargs.pop('pretrained_cfg', None)\n",
            "228:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "229:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "235:    kwargs.pop('pretrained_cfg', None)\n",
            "236:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "237:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "254:    kwargs.pop('pretrained_cfg', None)\n",
            "255:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "256:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "262:    kwargs.pop('pretrained_cfg', None)\n",
            "263:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "264:    kwargs.pop('pretrained_cfg_priority', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "6cd6950e-5b8f-4492-bcdf-c6769cc182e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "V409XjDO1cdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"cache_dir\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFoOP5c1dbu",
        "outputId": "46d42610-fc92-4053-bd5b-cd352502da96"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('cache_dir', None)\n",
            "73:    kwargs.pop('cache_dir', None)\n",
            "77:    kwargs.pop('cache_dir', None)\n",
            "100:    kwargs.pop('cache_dir', None)\n",
            "108:    kwargs.pop('cache_dir', None)\n",
            "112:    kwargs.pop('cache_dir', None)\n",
            "135:    kwargs.pop('cache_dir', None)\n",
            "143:    kwargs.pop('cache_dir', None)\n",
            "147:    kwargs.pop('cache_dir', None)\n",
            "170:    kwargs.pop('cache_dir', None)\n",
            "178:    kwargs.pop('cache_dir', None)\n",
            "182:    kwargs.pop('cache_dir', None)\n",
            "205:    kwargs.pop('cache_dir', None)\n",
            "213:    kwargs.pop('cache_dir', None)\n",
            "217:    kwargs.pop('cache_dir', None)\n",
            "240:    kwargs.pop('cache_dir', None)\n",
            "248:    kwargs.pop('cache_dir', None)\n",
            "252:    kwargs.pop('cache_dir', None)\n",
            "275:    kwargs.pop('cache_dir', None)\n",
            "283:    kwargs.pop('cache_dir', None)\n",
            "287:    kwargs.pop('cache_dir', None)\n",
            "310:    kwargs.pop('cache_dir', None)\n",
            "318:    kwargs.pop('cache_dir', None)\n",
            "322:    kwargs.pop('cache_dir', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 10 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 0 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.2 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b0,tf_efficientnet_b1,mobilenetv3_large_100\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "9239e894-91bf-4953-a514-5733d1ff90bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=10, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.00025, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m6-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.1, remode='pixel', recount=1, resplit=False, mixup=0.2, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', teacher_models='tf_efficientnet_b0,tf_efficientnet_b1,mobilenetv3_large_100', distillation_type='soft', distillation_alpha=0.2, distillation_tau=2.0, cosub=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "number of params: 5717416\n",
            "/usr/local/lib/python3.12/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "✅ Multi-teacher distillation enabled. Teachers: ['tf_efficientnet_b0', 'tf_efficientnet_b1', 'mobilenetv3_large_100']\n",
            "✅ Added adapter parameters to optimizer\n",
            "Start training for 10 epochs\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch: [0]  [  0/781]  eta: 2:52:45  lr: 0.000063  loss: 7.0836 (7.0836)  time: 13.2715  data: 0.9701  max mem: 4873\n",
            "Epoch: [0]  [ 10/781]  eta: 0:16:56  lr: 0.000063  loss: 5.2837 (5.6351)  time: 1.3184  data: 0.0885  max mem: 4873\n",
            "Epoch: [0]  [ 20/781]  eta: 0:09:29  lr: 0.000063  loss: 5.0631 (5.3389)  time: 0.1229  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 30/781]  eta: 0:06:51  lr: 0.000063  loss: 4.9178 (5.1845)  time: 0.1235  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 40/781]  eta: 0:05:29  lr: 0.000063  loss: 4.8102 (5.0886)  time: 0.1246  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 50/781]  eta: 0:04:38  lr: 0.000063  loss: 4.7610 (5.0162)  time: 0.1234  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 60/781]  eta: 0:04:04  lr: 0.000063  loss: 4.7142 (4.9630)  time: 0.1234  data: 0.0013  max mem: 4873\n",
            "Epoch: [0]  [ 70/781]  eta: 0:03:39  lr: 0.000063  loss: 4.6678 (4.9189)  time: 0.1258  data: 0.0039  max mem: 4873\n",
            "Epoch: [0]  [ 80/781]  eta: 0:03:22  lr: 0.000063  loss: 4.6339 (4.8836)  time: 0.1348  data: 0.0132  max mem: 4873\n",
            "Epoch: [0]  [ 90/781]  eta: 0:03:07  lr: 0.000063  loss: 4.6130 (4.8515)  time: 0.1379  data: 0.0148  max mem: 4873\n",
            "Epoch: [0]  [100/781]  eta: 0:02:55  lr: 0.000063  loss: 4.5807 (4.8227)  time: 0.1335  data: 0.0085  max mem: 4873\n",
            "Epoch: [0]  [110/781]  eta: 0:02:45  lr: 0.000063  loss: 4.5463 (4.7975)  time: 0.1303  data: 0.0058  max mem: 4873\n",
            "Epoch: [0]  [120/781]  eta: 0:02:36  lr: 0.000063  loss: 4.5262 (4.7734)  time: 0.1285  data: 0.0039  max mem: 4873\n",
            "Epoch: [0]  [130/781]  eta: 0:02:28  lr: 0.000063  loss: 4.4922 (4.7519)  time: 0.1298  data: 0.0061  max mem: 4873\n",
            "Epoch: [0]  [140/781]  eta: 0:02:22  lr: 0.000063  loss: 4.4758 (4.7326)  time: 0.1322  data: 0.0104  max mem: 4873\n",
            "Epoch: [0]  [150/781]  eta: 0:02:15  lr: 0.000063  loss: 4.4504 (4.7133)  time: 0.1308  data: 0.0067  max mem: 4873\n",
            "Epoch: [0]  [160/781]  eta: 0:02:10  lr: 0.000063  loss: 4.4098 (4.6948)  time: 0.1250  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [170/781]  eta: 0:02:05  lr: 0.000063  loss: 4.4095 (4.6782)  time: 0.1254  data: 0.0026  max mem: 4873\n",
            "Epoch: [0]  [180/781]  eta: 0:02:00  lr: 0.000063  loss: 4.4017 (4.6620)  time: 0.1324  data: 0.0093  max mem: 4873\n",
            "Epoch: [0]  [190/781]  eta: 0:01:56  lr: 0.000063  loss: 4.3850 (4.6472)  time: 0.1360  data: 0.0125  max mem: 4873\n",
            "Epoch: [0]  [200/781]  eta: 0:01:53  lr: 0.000063  loss: 4.3603 (4.6321)  time: 0.1371  data: 0.0134  max mem: 4873\n",
            "Epoch: [0]  [210/781]  eta: 0:01:49  lr: 0.000063  loss: 4.3164 (4.6176)  time: 0.1328  data: 0.0094  max mem: 4873\n",
            "Epoch: [0]  [220/781]  eta: 0:01:45  lr: 0.000063  loss: 4.2859 (4.6025)  time: 0.1290  data: 0.0063  max mem: 4873\n",
            "Epoch: [0]  [230/781]  eta: 0:01:42  lr: 0.000063  loss: 4.2551 (4.5887)  time: 0.1281  data: 0.0049  max mem: 4873\n",
            "Epoch: [0]  [240/781]  eta: 0:01:39  lr: 0.000063  loss: 4.2244 (4.5731)  time: 0.1233  data: 0.0005  max mem: 4873\n",
            "Epoch: [0]  [250/781]  eta: 0:01:36  lr: 0.000063  loss: 4.2192 (4.5590)  time: 0.1234  data: 0.0006  max mem: 4873\n",
            "Epoch: [0]  [260/781]  eta: 0:01:33  lr: 0.000063  loss: 4.2266 (4.5461)  time: 0.1323  data: 0.0098  max mem: 4873\n",
            "Epoch: [0]  [270/781]  eta: 0:01:30  lr: 0.000063  loss: 4.1867 (4.5330)  time: 0.1393  data: 0.0169  max mem: 4873\n",
            "Epoch: [0]  [280/781]  eta: 0:01:28  lr: 0.000063  loss: 4.1534 (4.5186)  time: 0.1404  data: 0.0173  max mem: 4873\n",
            "Epoch: [0]  [290/781]  eta: 0:01:26  lr: 0.000063  loss: 4.1024 (4.5055)  time: 0.1396  data: 0.0170  max mem: 4873\n",
            "Epoch: [0]  [300/781]  eta: 0:01:23  lr: 0.000063  loss: 4.0860 (4.4921)  time: 0.1434  data: 0.0219  max mem: 4873\n",
            "Epoch: [0]  [310/781]  eta: 0:01:21  lr: 0.000063  loss: 4.0790 (4.4797)  time: 0.1457  data: 0.0250  max mem: 4873\n",
            "Epoch: [0]  [320/781]  eta: 0:01:19  lr: 0.000063  loss: 3.9942 (4.4637)  time: 0.1347  data: 0.0140  max mem: 4873\n",
            "Epoch: [0]  [330/781]  eta: 0:01:16  lr: 0.000063  loss: 3.9942 (4.4520)  time: 0.1286  data: 0.0072  max mem: 4873\n",
            "Epoch: [0]  [340/781]  eta: 0:01:14  lr: 0.000063  loss: 4.0111 (4.4417)  time: 0.1327  data: 0.0110  max mem: 4873\n",
            "Epoch: [0]  [350/781]  eta: 0:01:12  lr: 0.000063  loss: 4.0405 (4.4319)  time: 0.1291  data: 0.0077  max mem: 4873\n",
            "Epoch: [0]  [360/781]  eta: 0:01:10  lr: 0.000063  loss: 3.9872 (4.4192)  time: 0.1224  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [370/781]  eta: 0:01:08  lr: 0.000063  loss: 3.9082 (4.4063)  time: 0.1234  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [380/781]  eta: 0:01:06  lr: 0.000063  loss: 3.9068 (4.3952)  time: 0.1273  data: 0.0041  max mem: 4873\n",
            "Epoch: [0]  [390/781]  eta: 0:01:04  lr: 0.000063  loss: 3.8858 (4.3820)  time: 0.1358  data: 0.0133  max mem: 4873\n",
            "Epoch: [0]  [400/781]  eta: 0:01:02  lr: 0.000063  loss: 3.8586 (4.3715)  time: 0.1332  data: 0.0115  max mem: 4873\n",
            "Epoch: [0]  [410/781]  eta: 0:01:00  lr: 0.000063  loss: 3.8460 (4.3597)  time: 0.1245  data: 0.0028  max mem: 4873\n",
            "Epoch: [0]  [420/781]  eta: 0:00:58  lr: 0.000063  loss: 3.8161 (4.3494)  time: 0.1243  data: 0.0021  max mem: 4873\n",
            "Epoch: [0]  [430/781]  eta: 0:00:56  lr: 0.000063  loss: 3.8064 (4.3380)  time: 0.1248  data: 0.0029  max mem: 4873\n",
            "Epoch: [0]  [440/781]  eta: 0:00:54  lr: 0.000063  loss: 3.8064 (4.3266)  time: 0.1255  data: 0.0035  max mem: 4873\n",
            "Epoch: [0]  [450/781]  eta: 0:00:52  lr: 0.000063  loss: 3.7215 (4.3144)  time: 0.1264  data: 0.0033  max mem: 4873\n",
            "Epoch: [0]  [460/781]  eta: 0:00:50  lr: 0.000063  loss: 3.7645 (4.3043)  time: 0.1273  data: 0.0042  max mem: 4873\n",
            "Epoch: [0]  [470/781]  eta: 0:00:49  lr: 0.000063  loss: 3.7680 (4.2932)  time: 0.1320  data: 0.0086  max mem: 4873\n",
            "Epoch: [0]  [480/781]  eta: 0:00:47  lr: 0.000063  loss: 3.7366 (4.2835)  time: 0.1419  data: 0.0182  max mem: 4873\n",
            "Epoch: [0]  [490/781]  eta: 0:00:45  lr: 0.000063  loss: 3.7361 (4.2747)  time: 0.1355  data: 0.0132  max mem: 4873\n",
            "Epoch: [0]  [500/781]  eta: 0:00:43  lr: 0.000063  loss: 3.7452 (4.2648)  time: 0.1235  data: 0.0024  max mem: 4873\n",
            "Epoch: [0]  [510/781]  eta: 0:00:42  lr: 0.000063  loss: 3.6426 (4.2515)  time: 0.1256  data: 0.0048  max mem: 4873\n",
            "Epoch: [0]  [520/781]  eta: 0:00:40  lr: 0.000063  loss: 3.5844 (4.2390)  time: 0.1260  data: 0.0050  max mem: 4873\n",
            "Epoch: [0]  [530/781]  eta: 0:00:38  lr: 0.000063  loss: 3.5564 (4.2262)  time: 0.1230  data: 0.0021  max mem: 4873\n",
            "Epoch: [0]  [540/781]  eta: 0:00:37  lr: 0.000063  loss: 3.5808 (4.2175)  time: 0.1227  data: 0.0019  max mem: 4873\n",
            "Epoch: [0]  [550/781]  eta: 0:00:35  lr: 0.000063  loss: 3.5732 (4.2060)  time: 0.1239  data: 0.0028  max mem: 4873\n",
            "Epoch: [0]  [560/781]  eta: 0:00:33  lr: 0.000063  loss: 3.5244 (4.1957)  time: 0.1248  data: 0.0027  max mem: 4873\n",
            "Epoch: [0]  [570/781]  eta: 0:00:32  lr: 0.000063  loss: 3.5323 (4.1845)  time: 0.1323  data: 0.0087  max mem: 4873\n",
            "Epoch: [0]  [580/781]  eta: 0:00:30  lr: 0.000063  loss: 3.4681 (4.1743)  time: 0.1341  data: 0.0103  max mem: 4873\n",
            "Epoch: [0]  [590/781]  eta: 0:00:29  lr: 0.000063  loss: 3.4500 (4.1634)  time: 0.1281  data: 0.0057  max mem: 4873\n",
            "Epoch: [0]  [600/781]  eta: 0:00:27  lr: 0.000063  loss: 3.4718 (4.1534)  time: 0.1298  data: 0.0083  max mem: 4873\n",
            "Epoch: [0]  [610/781]  eta: 0:00:25  lr: 0.000063  loss: 3.5322 (4.1439)  time: 0.1310  data: 0.0097  max mem: 4873\n",
            "Epoch: [0]  [620/781]  eta: 0:00:24  lr: 0.000063  loss: 3.4701 (4.1330)  time: 0.1281  data: 0.0070  max mem: 4873\n",
            "Epoch: [0]  [630/781]  eta: 0:00:22  lr: 0.000063  loss: 3.3831 (4.1212)  time: 0.1275  data: 0.0065  max mem: 4873\n",
            "Epoch: [0]  [640/781]  eta: 0:00:21  lr: 0.000063  loss: 3.3710 (4.1103)  time: 0.1294  data: 0.0082  max mem: 4873\n",
            "Epoch: [0]  [650/781]  eta: 0:00:19  lr: 0.000063  loss: 3.3661 (4.1006)  time: 0.1281  data: 0.0070  max mem: 4873\n",
            "Epoch: [0]  [660/781]  eta: 0:00:18  lr: 0.000063  loss: 3.3759 (4.0917)  time: 0.1291  data: 0.0072  max mem: 4873\n",
            "Epoch: [0]  [670/781]  eta: 0:00:16  lr: 0.000063  loss: 3.3697 (4.0829)  time: 0.1335  data: 0.0106  max mem: 4873\n",
            "Epoch: [0]  [680/781]  eta: 0:00:15  lr: 0.000063  loss: 3.3597 (4.0735)  time: 0.1322  data: 0.0103  max mem: 4873\n",
            "Epoch: [0]  [690/781]  eta: 0:00:13  lr: 0.000063  loss: 3.3054 (4.0626)  time: 0.1291  data: 0.0080  max mem: 4873\n",
            "Epoch: [0]  [700/781]  eta: 0:00:12  lr: 0.000063  loss: 3.3130 (4.0550)  time: 0.1306  data: 0.0092  max mem: 4873\n",
            "Epoch: [0]  [710/781]  eta: 0:00:10  lr: 0.000063  loss: 3.3303 (4.0446)  time: 0.1315  data: 0.0094  max mem: 4873\n",
            "Epoch: [0]  [720/781]  eta: 0:00:09  lr: 0.000063  loss: 3.3394 (4.0347)  time: 0.1307  data: 0.0078  max mem: 4873\n",
            "Epoch: [0]  [730/781]  eta: 0:00:07  lr: 0.000063  loss: 3.3394 (4.0276)  time: 0.1282  data: 0.0053  max mem: 4873\n",
            "Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000063  loss: 3.2470 (4.0170)  time: 0.1335  data: 0.0119  max mem: 4873\n",
            "Epoch: [0]  [750/781]  eta: 0:00:04  lr: 0.000063  loss: 3.2123 (4.0084)  time: 0.1344  data: 0.0119  max mem: 4873\n",
            "Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000063  loss: 3.2249 (3.9998)  time: 0.1281  data: 0.0043  max mem: 4873\n",
            "Epoch: [0]  [770/781]  eta: 0:00:01  lr: 0.000063  loss: 3.2249 (3.9916)  time: 0.1347  data: 0.0086  max mem: 4873\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000063  loss: 3.1818 (3.9812)  time: 0.1336  data: 0.0093  max mem: 4873\n",
            "Epoch: [0] Total time: 0:01:54 (0.1469 s / it)\n",
            "Averaged stats: lr: 0.000063  loss: 3.1818 (3.9812)\n",
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Test:  [ 0/53]  eta: 0:00:49  loss: 1.5835 (1.5835)  acc1: 67.1875 (67.1875)  acc5: 85.4167 (85.4167)  time: 0.9246  data: 0.8187  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 2.6435 (2.3690)  acc1: 43.7500 (46.0227)  acc5: 75.5208 (75.7102)  time: 0.1699  data: 0.1308  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 2.6653 (2.5771)  acc1: 35.9375 (41.7163)  acc5: 68.2292 (72.3214)  time: 0.1053  data: 0.0738  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 2.6910 (2.5956)  acc1: 41.1458 (43.4140)  acc5: 68.2292 (72.3622)  time: 0.1264  data: 0.0957  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 2.6910 (2.7066)  acc1: 41.6667 (41.2221)  acc5: 66.1458 (69.9441)  time: 0.1222  data: 0.0916  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 2.6669 (2.7102)  acc1: 33.8542 (40.9109)  acc5: 66.1458 (69.6691)  time: 0.1268  data: 0.0962  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 2.6669 (2.7157)  acc1: 33.8542 (40.8600)  acc5: 66.1458 (69.7700)  time: 0.1282  data: 0.0981  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1334 s / it)\n",
            "* Acc@1 40.860 Acc@5 69.770 loss 2.716\n",
            "Accuracy of the network on the 10000 test images: 40.9%\n",
            "Max accuracy: 40.86%\n",
            "Epoch: [1]  [  0/781]  eta: 0:12:12  lr: 0.000063  loss: 3.1035 (3.1035)  time: 0.9381  data: 0.7910  max mem: 4873\n",
            "Epoch: [1]  [ 10/781]  eta: 0:02:37  lr: 0.000063  loss: 3.2296 (3.3689)  time: 0.2038  data: 0.0758  max mem: 4873\n",
            "Epoch: [1]  [ 20/781]  eta: 0:02:15  lr: 0.000063  loss: 3.2155 (3.2906)  time: 0.1405  data: 0.0155  max mem: 4873\n",
            "Epoch: [1]  [ 30/781]  eta: 0:02:04  lr: 0.000063  loss: 3.1626 (3.3045)  time: 0.1457  data: 0.0136  max mem: 4873\n",
            "Epoch: [1]  [ 40/781]  eta: 0:01:57  lr: 0.000063  loss: 3.1512 (3.2902)  time: 0.1377  data: 0.0070  max mem: 4873\n",
            "Epoch: [1]  [ 50/781]  eta: 0:01:52  lr: 0.000063  loss: 3.0863 (3.2813)  time: 0.1347  data: 0.0137  max mem: 4873\n",
            "Epoch: [1]  [ 60/781]  eta: 0:01:49  lr: 0.000063  loss: 3.0845 (3.2747)  time: 0.1380  data: 0.0168  max mem: 4873\n",
            "Epoch: [1]  [ 70/781]  eta: 0:01:46  lr: 0.000063  loss: 3.1375 (3.2832)  time: 0.1380  data: 0.0150  max mem: 4873\n",
            "Epoch: [1]  [ 80/781]  eta: 0:01:44  lr: 0.000063  loss: 3.1375 (3.2761)  time: 0.1381  data: 0.0145  max mem: 4873\n",
            "Epoch: [1]  [ 90/781]  eta: 0:01:41  lr: 0.000063  loss: 3.1057 (3.2662)  time: 0.1384  data: 0.0155  max mem: 4873\n",
            "Epoch: [1]  [100/781]  eta: 0:01:40  lr: 0.000063  loss: 3.1098 (3.2777)  time: 0.1451  data: 0.0217  max mem: 4873\n",
            "Epoch: [1]  [110/781]  eta: 0:01:39  lr: 0.000063  loss: 3.0594 (3.2579)  time: 0.1504  data: 0.0280  max mem: 4873\n",
            "Epoch: [1]  [120/781]  eta: 0:01:37  lr: 0.000063  loss: 3.0214 (3.2406)  time: 0.1473  data: 0.0258  max mem: 4873\n",
            "Epoch: [1]  [130/781]  eta: 0:01:35  lr: 0.000063  loss: 3.0384 (3.2476)  time: 0.1427  data: 0.0212  max mem: 4873\n",
            "Epoch: [1]  [140/781]  eta: 0:01:33  lr: 0.000063  loss: 3.0384 (3.2327)  time: 0.1370  data: 0.0154  max mem: 4873\n",
            "Epoch: [1]  [150/781]  eta: 0:01:31  lr: 0.000063  loss: 2.9845 (3.2235)  time: 0.1334  data: 0.0110  max mem: 4873\n",
            "Epoch: [1]  [160/781]  eta: 0:01:29  lr: 0.000063  loss: 3.0082 (3.2245)  time: 0.1311  data: 0.0086  max mem: 4873\n",
            "Epoch: [1]  [170/781]  eta: 0:01:27  lr: 0.000063  loss: 3.0230 (3.2214)  time: 0.1282  data: 0.0064  max mem: 4873\n",
            "Epoch: [1]  [180/781]  eta: 0:01:25  lr: 0.000063  loss: 3.0215 (3.2166)  time: 0.1256  data: 0.0029  max mem: 4873\n",
            "Epoch: [1]  [190/781]  eta: 0:01:23  lr: 0.000063  loss: 3.0112 (3.2054)  time: 0.1336  data: 0.0093  max mem: 4873\n",
            "Epoch: [1]  [200/781]  eta: 0:01:22  lr: 0.000063  loss: 2.9428 (3.2148)  time: 0.1394  data: 0.0149  max mem: 4873\n",
            "Epoch: [1]  [210/781]  eta: 0:01:20  lr: 0.000063  loss: 3.0214 (3.2169)  time: 0.1333  data: 0.0107  max mem: 4873\n",
            "Epoch: [1]  [220/781]  eta: 0:01:18  lr: 0.000063  loss: 2.9829 (3.2046)  time: 0.1272  data: 0.0060  max mem: 4873\n",
            "Epoch: [1]  [230/781]  eta: 0:01:17  lr: 0.000063  loss: 2.8674 (3.1914)  time: 0.1272  data: 0.0054  max mem: 4873\n",
            "Epoch: [1]  [240/781]  eta: 0:01:15  lr: 0.000063  loss: 2.9505 (3.1860)  time: 0.1292  data: 0.0068  max mem: 4873\n",
            "Epoch: [1]  [250/781]  eta: 0:01:13  lr: 0.000063  loss: 2.9521 (3.1826)  time: 0.1269  data: 0.0043  max mem: 4873\n",
            "Epoch: [1]  [260/781]  eta: 0:01:12  lr: 0.000063  loss: 3.0173 (3.1780)  time: 0.1231  data: 0.0009  max mem: 4873\n",
            "Epoch: [1]  [270/781]  eta: 0:01:10  lr: 0.000063  loss: 2.9937 (3.1709)  time: 0.1248  data: 0.0026  max mem: 4873\n",
            "Epoch: [1]  [280/781]  eta: 0:01:09  lr: 0.000063  loss: 2.9486 (3.1641)  time: 0.1322  data: 0.0075  max mem: 4873\n",
            "Epoch: [1]  [290/781]  eta: 0:01:07  lr: 0.000063  loss: 2.9666 (3.1601)  time: 0.1373  data: 0.0113  max mem: 4873\n",
            "Epoch: [1]  [300/781]  eta: 0:01:06  lr: 0.000063  loss: 2.9666 (3.1568)  time: 0.1312  data: 0.0062  max mem: 4873\n",
            "Epoch: [1]  [310/781]  eta: 0:01:04  lr: 0.000063  loss: 2.9130 (3.1506)  time: 0.1272  data: 0.0040  max mem: 4873\n",
            "Epoch: [1]  [320/781]  eta: 0:01:03  lr: 0.000063  loss: 2.9466 (3.1537)  time: 0.1269  data: 0.0054  max mem: 4873\n",
            "Epoch: [1]  [330/781]  eta: 0:01:01  lr: 0.000063  loss: 2.9312 (3.1448)  time: 0.1254  data: 0.0041  max mem: 4873\n",
            "Epoch: [1]  [340/781]  eta: 0:01:00  lr: 0.000063  loss: 2.8789 (3.1445)  time: 0.1260  data: 0.0036  max mem: 4873\n",
            "Epoch: [1]  [350/781]  eta: 0:00:58  lr: 0.000063  loss: 2.9462 (3.1415)  time: 0.1269  data: 0.0037  max mem: 4873\n",
            "Epoch: [1]  [360/781]  eta: 0:00:57  lr: 0.000063  loss: 2.9318 (3.1386)  time: 0.1266  data: 0.0039  max mem: 4873\n",
            "Epoch: [1]  [370/781]  eta: 0:00:55  lr: 0.000063  loss: 2.9489 (3.1349)  time: 0.1278  data: 0.0051  max mem: 4873\n",
            "Epoch: [1]  [380/781]  eta: 0:00:54  lr: 0.000063  loss: 2.8269 (3.1262)  time: 0.1335  data: 0.0107  max mem: 4873\n",
            "Epoch: [1]  [390/781]  eta: 0:00:52  lr: 0.000063  loss: 2.8234 (3.1244)  time: 0.1338  data: 0.0112  max mem: 4873\n",
            "Epoch: [1]  [400/781]  eta: 0:00:51  lr: 0.000063  loss: 2.8305 (3.1204)  time: 0.1264  data: 0.0047  max mem: 4873\n",
            "Epoch: [1]  [410/781]  eta: 0:00:50  lr: 0.000063  loss: 2.8305 (3.1180)  time: 0.1251  data: 0.0041  max mem: 4873\n",
            "Epoch: [1]  [420/781]  eta: 0:00:48  lr: 0.000063  loss: 2.8748 (3.1162)  time: 0.1296  data: 0.0085  max mem: 4873\n",
            "Epoch: [1]  [430/781]  eta: 0:00:47  lr: 0.000063  loss: 2.8922 (3.1158)  time: 0.1305  data: 0.0093  max mem: 4873\n",
            "Epoch: [1]  [440/781]  eta: 0:00:45  lr: 0.000063  loss: 2.8922 (3.1102)  time: 0.1320  data: 0.0106  max mem: 4873\n",
            "Epoch: [1]  [450/781]  eta: 0:00:44  lr: 0.000063  loss: 2.9103 (3.1167)  time: 0.1300  data: 0.0076  max mem: 4873\n",
            "Epoch: [1]  [460/781]  eta: 0:00:43  lr: 0.000063  loss: 3.0672 (3.1204)  time: 0.1239  data: 0.0015  max mem: 4873\n",
            "Epoch: [1]  [470/781]  eta: 0:00:41  lr: 0.000063  loss: 2.8911 (3.1165)  time: 0.1249  data: 0.0031  max mem: 4873\n",
            "Epoch: [1]  [480/781]  eta: 0:00:40  lr: 0.000063  loss: 2.8506 (3.1159)  time: 0.1345  data: 0.0120  max mem: 4873\n",
            "Epoch: [1]  [490/781]  eta: 0:00:39  lr: 0.000063  loss: 2.9238 (3.1144)  time: 0.1393  data: 0.0174  max mem: 4873\n",
            "Epoch: [1]  [500/781]  eta: 0:00:37  lr: 0.000063  loss: 2.8919 (3.1150)  time: 0.1303  data: 0.0091  max mem: 4873\n",
            "Epoch: [1]  [510/781]  eta: 0:00:36  lr: 0.000063  loss: 2.8521 (3.1100)  time: 0.1257  data: 0.0031  max mem: 4873\n",
            "Epoch: [1]  [520/781]  eta: 0:00:34  lr: 0.000063  loss: 2.8513 (3.1061)  time: 0.1301  data: 0.0063  max mem: 4873\n",
            "Epoch: [1]  [530/781]  eta: 0:00:33  lr: 0.000063  loss: 2.8174 (3.1008)  time: 0.1283  data: 0.0043  max mem: 4873\n",
            "Epoch: [1]  [540/781]  eta: 0:00:32  lr: 0.000063  loss: 2.8332 (3.0979)  time: 0.1260  data: 0.0024  max mem: 4873\n",
            "Epoch: [1]  [550/781]  eta: 0:00:30  lr: 0.000063  loss: 2.8627 (3.0954)  time: 0.1293  data: 0.0062  max mem: 4873\n",
            "Epoch: [1]  [560/781]  eta: 0:00:29  lr: 0.000063  loss: 2.8042 (3.0951)  time: 0.1287  data: 0.0064  max mem: 4873\n",
            "Epoch: [1]  [570/781]  eta: 0:00:28  lr: 0.000063  loss: 2.7988 (3.0927)  time: 0.1263  data: 0.0042  max mem: 4873\n",
            "Epoch: [1]  [580/781]  eta: 0:00:26  lr: 0.000063  loss: 2.8623 (3.0913)  time: 0.1358  data: 0.0117  max mem: 4873\n",
            "Epoch: [1]  [590/781]  eta: 0:00:25  lr: 0.000063  loss: 2.9195 (3.0915)  time: 0.1358  data: 0.0113  max mem: 4873\n",
            "Epoch: [1]  [600/781]  eta: 0:00:24  lr: 0.000063  loss: 2.8716 (3.0887)  time: 0.1266  data: 0.0023  max mem: 4873\n",
            "Epoch: [1]  [610/781]  eta: 0:00:22  lr: 0.000063  loss: 2.8432 (3.0868)  time: 0.1272  data: 0.0035  max mem: 4873\n",
            "Epoch: [1]  [620/781]  eta: 0:00:21  lr: 0.000063  loss: 2.7823 (3.0856)  time: 0.1302  data: 0.0077  max mem: 4873\n",
            "Epoch: [1]  [630/781]  eta: 0:00:20  lr: 0.000063  loss: 2.7973 (3.0847)  time: 0.1311  data: 0.0087  max mem: 4873\n",
            "Epoch: [1]  [640/781]  eta: 0:00:18  lr: 0.000063  loss: 2.7494 (3.0797)  time: 0.1260  data: 0.0042  max mem: 4873\n",
            "Epoch: [1]  [650/781]  eta: 0:00:17  lr: 0.000063  loss: 2.7357 (3.0766)  time: 0.1244  data: 0.0024  max mem: 4873\n",
            "Epoch: [1]  [660/781]  eta: 0:00:16  lr: 0.000063  loss: 2.7635 (3.0756)  time: 0.1320  data: 0.0092  max mem: 4873\n",
            "Epoch: [1]  [670/781]  eta: 0:00:14  lr: 0.000063  loss: 2.8668 (3.0728)  time: 0.1331  data: 0.0101  max mem: 4873\n",
            "Epoch: [1]  [680/781]  eta: 0:00:13  lr: 0.000063  loss: 2.9057 (3.0716)  time: 0.1317  data: 0.0094  max mem: 4873\n",
            "Epoch: [1]  [690/781]  eta: 0:00:12  lr: 0.000063  loss: 2.7874 (3.0670)  time: 0.1302  data: 0.0090  max mem: 4873\n",
            "Epoch: [1]  [700/781]  eta: 0:00:10  lr: 0.000063  loss: 2.6959 (3.0629)  time: 0.1314  data: 0.0101  max mem: 4873\n",
            "Epoch: [1]  [710/781]  eta: 0:00:09  lr: 0.000063  loss: 2.7598 (3.0629)  time: 0.1324  data: 0.0106  max mem: 4873\n",
            "Epoch: [1]  [720/781]  eta: 0:00:08  lr: 0.000063  loss: 2.7693 (3.0624)  time: 0.1259  data: 0.0030  max mem: 4873\n",
            "Epoch: [1]  [730/781]  eta: 0:00:06  lr: 0.000063  loss: 2.7562 (3.0609)  time: 0.1230  data: 0.0003  max mem: 4873\n",
            "Epoch: [1]  [740/781]  eta: 0:00:05  lr: 0.000063  loss: 2.7474 (3.0643)  time: 0.1223  data: 0.0010  max mem: 4873\n",
            "Epoch: [1]  [750/781]  eta: 0:00:04  lr: 0.000063  loss: 2.7675 (3.0609)  time: 0.1242  data: 0.0023  max mem: 4873\n",
            "Epoch: [1]  [760/781]  eta: 0:00:02  lr: 0.000063  loss: 2.7578 (3.0588)  time: 0.1293  data: 0.0064  max mem: 4873\n",
            "Epoch: [1]  [770/781]  eta: 0:00:01  lr: 0.000063  loss: 2.6948 (3.0545)  time: 0.1330  data: 0.0094  max mem: 4873\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000063  loss: 2.6650 (3.0525)  time: 0.1335  data: 0.0105  max mem: 4873\n",
            "Epoch: [1] Total time: 0:01:43 (0.1325 s / it)\n",
            "Averaged stats: lr: 0.000063  loss: 2.6650 (3.0525)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.1306 (1.1306)  acc1: 76.0417 (76.0417)  acc5: 93.2292 (93.2292)  time: 0.8452  data: 0.8143  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.6021 (1.5833)  acc1: 60.4167 (62.9261)  acc5: 85.9375 (87.3106)  time: 0.1749  data: 0.1443  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.6771 (1.6923)  acc1: 60.4167 (61.8800)  acc5: 84.3750 (84.8958)  time: 0.1227  data: 0.0920  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.7853 (1.7092)  acc1: 60.9375 (61.8952)  acc5: 84.3750 (84.8454)  time: 0.1259  data: 0.0952  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.8416 (1.7929)  acc1: 57.8125 (59.8323)  acc5: 82.8125 (83.4731)  time: 0.1235  data: 0.0928  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.8457 (1.7908)  acc1: 55.7292 (59.7120)  acc5: 82.8125 (83.5478)  time: 0.1240  data: 0.0934  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.8457 (1.8032)  acc1: 55.7292 (59.6400)  acc5: 82.8125 (83.6200)  time: 0.1097  data: 0.0801  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1323 s / it)\n",
            "* Acc@1 59.640 Acc@5 83.620 loss 1.803\n",
            "Accuracy of the network on the 10000 test images: 59.6%\n",
            "Max accuracy: 59.64%\n",
            "Epoch: [2]  [  0/781]  eta: 0:11:31  lr: 0.000061  loss: 2.5586 (2.5586)  time: 0.8850  data: 0.7450  max mem: 4873\n",
            "Epoch: [2]  [ 10/781]  eta: 0:02:37  lr: 0.000061  loss: 2.6941 (2.6996)  time: 0.2038  data: 0.0808  max mem: 4873\n",
            "Epoch: [2]  [ 20/781]  eta: 0:02:14  lr: 0.000061  loss: 2.6138 (2.6649)  time: 0.1418  data: 0.0185  max mem: 4873\n",
            "Epoch: [2]  [ 30/781]  eta: 0:02:03  lr: 0.000061  loss: 2.6138 (2.7117)  time: 0.1426  data: 0.0189  max mem: 4873\n",
            "Epoch: [2]  [ 40/781]  eta: 0:01:54  lr: 0.000061  loss: 2.6678 (2.7312)  time: 0.1306  data: 0.0090  max mem: 4873\n",
            "Epoch: [2]  [ 50/781]  eta: 0:01:49  lr: 0.000061  loss: 2.7180 (2.7811)  time: 0.1270  data: 0.0059  max mem: 4873\n",
            "Epoch: [2]  [ 60/781]  eta: 0:01:45  lr: 0.000061  loss: 2.6927 (2.7721)  time: 0.1312  data: 0.0100  max mem: 4873\n",
            "Epoch: [2]  [ 70/781]  eta: 0:01:42  lr: 0.000061  loss: 2.6765 (2.7909)  time: 0.1291  data: 0.0079  max mem: 4873\n",
            "Epoch: [2]  [ 80/781]  eta: 0:01:39  lr: 0.000061  loss: 2.6717 (2.7761)  time: 0.1284  data: 0.0065  max mem: 4873\n",
            "Epoch: [2]  [ 90/781]  eta: 0:01:37  lr: 0.000061  loss: 2.6291 (2.7724)  time: 0.1283  data: 0.0063  max mem: 4873\n",
            "Epoch: [2]  [100/781]  eta: 0:01:35  lr: 0.000061  loss: 2.6291 (2.7689)  time: 0.1301  data: 0.0078  max mem: 4873\n",
            "Epoch: [2]  [110/781]  eta: 0:01:33  lr: 0.000061  loss: 2.6075 (2.7581)  time: 0.1361  data: 0.0128  max mem: 4873\n",
            "Epoch: [2]  [120/781]  eta: 0:01:31  lr: 0.000061  loss: 2.6019 (2.7767)  time: 0.1328  data: 0.0089  max mem: 4873\n",
            "Epoch: [2]  [130/781]  eta: 0:01:29  lr: 0.000061  loss: 2.7195 (2.7864)  time: 0.1278  data: 0.0041  max mem: 4873\n",
            "Epoch: [2]  [140/781]  eta: 0:01:27  lr: 0.000061  loss: 2.6805 (2.7945)  time: 0.1260  data: 0.0026  max mem: 4873\n",
            "Epoch: [2]  [150/781]  eta: 0:01:25  lr: 0.000061  loss: 2.6805 (2.7944)  time: 0.1247  data: 0.0004  max mem: 4873\n",
            "Epoch: [2]  [160/781]  eta: 0:01:24  lr: 0.000061  loss: 2.7160 (2.8097)  time: 0.1265  data: 0.0014  max mem: 4873\n",
            "Epoch: [2]  [170/781]  eta: 0:01:22  lr: 0.000061  loss: 2.6053 (2.8003)  time: 0.1270  data: 0.0024  max mem: 4873\n",
            "Epoch: [2]  [180/781]  eta: 0:01:20  lr: 0.000061  loss: 2.5754 (2.7898)  time: 0.1271  data: 0.0035  max mem: 4873\n",
            "Epoch: [2]  [190/781]  eta: 0:01:19  lr: 0.000061  loss: 2.6196 (2.7807)  time: 0.1317  data: 0.0083  max mem: 4873\n",
            "Epoch: [2]  [200/781]  eta: 0:01:17  lr: 0.000061  loss: 2.6276 (2.7771)  time: 0.1297  data: 0.0061  max mem: 4873\n",
            "Epoch: [2]  [210/781]  eta: 0:01:16  lr: 0.000061  loss: 2.6146 (2.7694)  time: 0.1291  data: 0.0043  max mem: 4873\n",
            "Epoch: [2]  [220/781]  eta: 0:01:15  lr: 0.000061  loss: 2.5831 (2.7604)  time: 0.1302  data: 0.0064  max mem: 4873\n",
            "Epoch: [2]  [230/781]  eta: 0:01:13  lr: 0.000061  loss: 2.6139 (2.7585)  time: 0.1291  data: 0.0062  max mem: 4873\n",
            "Epoch: [2]  [240/781]  eta: 0:01:12  lr: 0.000061  loss: 2.6288 (2.7597)  time: 0.1322  data: 0.0069  max mem: 4873\n",
            "Epoch: [2]  [250/781]  eta: 0:01:10  lr: 0.000061  loss: 2.5716 (2.7587)  time: 0.1327  data: 0.0089  max mem: 4873\n",
            "Epoch: [2]  [260/781]  eta: 0:01:09  lr: 0.000061  loss: 2.5984 (2.7562)  time: 0.1322  data: 0.0101  max mem: 4873\n",
            "Epoch: [2]  [270/781]  eta: 0:01:08  lr: 0.000061  loss: 2.5840 (2.7501)  time: 0.1340  data: 0.0118  max mem: 4873\n",
            "Epoch: [2]  [280/781]  eta: 0:01:06  lr: 0.000061  loss: 2.5799 (2.7525)  time: 0.1321  data: 0.0090  max mem: 4873\n",
            "Epoch: [2]  [290/781]  eta: 0:01:05  lr: 0.000061  loss: 2.5799 (2.7528)  time: 0.1315  data: 0.0082  max mem: 4873\n",
            "Epoch: [2]  [300/781]  eta: 0:01:04  lr: 0.000061  loss: 2.6194 (2.7475)  time: 0.1344  data: 0.0114  max mem: 4873\n",
            "Epoch: [2]  [310/781]  eta: 0:01:02  lr: 0.000061  loss: 2.6535 (2.7501)  time: 0.1357  data: 0.0129  max mem: 4873\n",
            "Epoch: [2]  [320/781]  eta: 0:01:01  lr: 0.000061  loss: 2.6276 (2.7475)  time: 0.1339  data: 0.0119  max mem: 4873\n",
            "Epoch: [2]  [330/781]  eta: 0:01:00  lr: 0.000061  loss: 2.5955 (2.7500)  time: 0.1293  data: 0.0068  max mem: 4873\n",
            "Epoch: [2]  [340/781]  eta: 0:00:58  lr: 0.000061  loss: 2.5236 (2.7439)  time: 0.1265  data: 0.0034  max mem: 4873\n",
            "Epoch: [2]  [350/781]  eta: 0:00:57  lr: 0.000061  loss: 2.5591 (2.7456)  time: 0.1254  data: 0.0019  max mem: 4873\n",
            "Epoch: [2]  [360/781]  eta: 0:00:55  lr: 0.000061  loss: 2.6077 (2.7462)  time: 0.1251  data: 0.0021  max mem: 4873\n",
            "Epoch: [2]  [370/781]  eta: 0:00:54  lr: 0.000061  loss: 2.5805 (2.7420)  time: 0.1274  data: 0.0055  max mem: 4873\n",
            "Epoch: [2]  [380/781]  eta: 0:00:53  lr: 0.000061  loss: 2.5917 (2.7440)  time: 0.1274  data: 0.0058  max mem: 4873\n",
            "Epoch: [2]  [390/781]  eta: 0:00:51  lr: 0.000061  loss: 2.5391 (2.7483)  time: 0.1292  data: 0.0072  max mem: 4873\n",
            "Epoch: [2]  [400/781]  eta: 0:00:50  lr: 0.000061  loss: 2.5786 (2.7537)  time: 0.1340  data: 0.0094  max mem: 4873\n",
            "Epoch: [2]  [410/781]  eta: 0:00:49  lr: 0.000061  loss: 2.5795 (2.7492)  time: 0.1348  data: 0.0099  max mem: 4873\n",
            "Epoch: [2]  [420/781]  eta: 0:00:47  lr: 0.000061  loss: 2.6030 (2.7538)  time: 0.1338  data: 0.0108  max mem: 4873\n",
            "Epoch: [2]  [430/781]  eta: 0:00:46  lr: 0.000061  loss: 2.7400 (2.7551)  time: 0.1291  data: 0.0060  max mem: 4873\n",
            "Epoch: [2]  [440/781]  eta: 0:00:45  lr: 0.000061  loss: 2.5417 (2.7527)  time: 0.1290  data: 0.0063  max mem: 4873\n",
            "Epoch: [2]  [450/781]  eta: 0:00:43  lr: 0.000061  loss: 2.5397 (2.7538)  time: 0.1325  data: 0.0105  max mem: 4873\n",
            "Epoch: [2]  [460/781]  eta: 0:00:42  lr: 0.000061  loss: 2.6247 (2.7560)  time: 0.1314  data: 0.0088  max mem: 4873\n",
            "Epoch: [2]  [470/781]  eta: 0:00:41  lr: 0.000061  loss: 2.6201 (2.7567)  time: 0.1288  data: 0.0060  max mem: 4873\n",
            "Epoch: [2]  [480/781]  eta: 0:00:39  lr: 0.000061  loss: 2.5492 (2.7549)  time: 0.1266  data: 0.0047  max mem: 4873\n",
            "Epoch: [2]  [490/781]  eta: 0:00:38  lr: 0.000061  loss: 2.5478 (2.7528)  time: 0.1323  data: 0.0090  max mem: 4873\n",
            "Epoch: [2]  [500/781]  eta: 0:00:37  lr: 0.000061  loss: 2.6253 (2.7529)  time: 0.1334  data: 0.0086  max mem: 4873\n",
            "Epoch: [2]  [510/781]  eta: 0:00:35  lr: 0.000061  loss: 2.6067 (2.7500)  time: 0.1311  data: 0.0081  max mem: 4873\n",
            "Epoch: [2]  [520/781]  eta: 0:00:34  lr: 0.000061  loss: 2.5450 (2.7465)  time: 0.1339  data: 0.0129  max mem: 4873\n",
            "Epoch: [2]  [530/781]  eta: 0:00:33  lr: 0.000061  loss: 2.5770 (2.7453)  time: 0.1312  data: 0.0100  max mem: 4873\n",
            "Epoch: [2]  [540/781]  eta: 0:00:31  lr: 0.000061  loss: 2.6019 (2.7447)  time: 0.1288  data: 0.0068  max mem: 4873\n",
            "Epoch: [2]  [550/781]  eta: 0:00:30  lr: 0.000061  loss: 2.5745 (2.7447)  time: 0.1252  data: 0.0029  max mem: 4873\n",
            "Epoch: [2]  [560/781]  eta: 0:00:29  lr: 0.000061  loss: 2.5549 (2.7438)  time: 0.1237  data: 0.0011  max mem: 4873\n",
            "Epoch: [2]  [570/781]  eta: 0:00:27  lr: 0.000061  loss: 2.5586 (2.7403)  time: 0.1250  data: 0.0015  max mem: 4873\n",
            "Epoch: [2]  [580/781]  eta: 0:00:26  lr: 0.000061  loss: 2.5152 (2.7371)  time: 0.1317  data: 0.0075  max mem: 4873\n",
            "Epoch: [2]  [590/781]  eta: 0:00:25  lr: 0.000061  loss: 2.5412 (2.7380)  time: 0.1393  data: 0.0147  max mem: 4873\n",
            "Epoch: [2]  [600/781]  eta: 0:00:23  lr: 0.000061  loss: 2.5481 (2.7348)  time: 0.1368  data: 0.0134  max mem: 4873\n",
            "Epoch: [2]  [610/781]  eta: 0:00:22  lr: 0.000061  loss: 2.5934 (2.7388)  time: 0.1289  data: 0.0069  max mem: 4873\n",
            "Epoch: [2]  [620/781]  eta: 0:00:21  lr: 0.000061  loss: 2.6558 (2.7391)  time: 0.1250  data: 0.0028  max mem: 4873\n",
            "Epoch: [2]  [630/781]  eta: 0:00:19  lr: 0.000061  loss: 2.6436 (2.7392)  time: 0.1263  data: 0.0040  max mem: 4873\n",
            "Epoch: [2]  [640/781]  eta: 0:00:18  lr: 0.000061  loss: 2.6076 (2.7406)  time: 0.1252  data: 0.0029  max mem: 4873\n",
            "Epoch: [2]  [650/781]  eta: 0:00:17  lr: 0.000061  loss: 2.5637 (2.7406)  time: 0.1285  data: 0.0063  max mem: 4873\n",
            "Epoch: [2]  [660/781]  eta: 0:00:15  lr: 0.000061  loss: 2.4707 (2.7359)  time: 0.1338  data: 0.0111  max mem: 4873\n",
            "Epoch: [2]  [670/781]  eta: 0:00:14  lr: 0.000061  loss: 2.4707 (2.7330)  time: 0.1303  data: 0.0081  max mem: 4873\n",
            "Epoch: [2]  [680/781]  eta: 0:00:13  lr: 0.000061  loss: 2.5236 (2.7332)  time: 0.1287  data: 0.0056  max mem: 4873\n",
            "Epoch: [2]  [690/781]  eta: 0:00:11  lr: 0.000061  loss: 2.5460 (2.7334)  time: 0.1358  data: 0.0109  max mem: 4873\n",
            "Epoch: [2]  [700/781]  eta: 0:00:10  lr: 0.000061  loss: 2.5510 (2.7325)  time: 0.1343  data: 0.0106  max mem: 4873\n",
            "Epoch: [2]  [710/781]  eta: 0:00:09  lr: 0.000061  loss: 2.5105 (2.7299)  time: 0.1277  data: 0.0058  max mem: 4873\n",
            "Epoch: [2]  [720/781]  eta: 0:00:08  lr: 0.000061  loss: 2.5249 (2.7280)  time: 0.1288  data: 0.0072  max mem: 4873\n",
            "Epoch: [2]  [730/781]  eta: 0:00:06  lr: 0.000061  loss: 2.5685 (2.7296)  time: 0.1298  data: 0.0082  max mem: 4873\n",
            "Epoch: [2]  [740/781]  eta: 0:00:05  lr: 0.000061  loss: 2.5342 (2.7270)  time: 0.1300  data: 0.0083  max mem: 4873\n",
            "Epoch: [2]  [750/781]  eta: 0:00:04  lr: 0.000061  loss: 2.4623 (2.7234)  time: 0.1279  data: 0.0057  max mem: 4873\n",
            "Epoch: [2]  [760/781]  eta: 0:00:02  lr: 0.000061  loss: 2.4628 (2.7228)  time: 0.1262  data: 0.0039  max mem: 4873\n",
            "Epoch: [2]  [770/781]  eta: 0:00:01  lr: 0.000061  loss: 2.5371 (2.7227)  time: 0.1259  data: 0.0023  max mem: 4873\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000061  loss: 2.4724 (2.7244)  time: 0.1262  data: 0.0027  max mem: 4873\n",
            "Epoch: [2] Total time: 0:01:42 (0.1313 s / it)\n",
            "Averaged stats: lr: 0.000061  loss: 2.4724 (2.7244)\n",
            "Test:  [ 0/53]  eta: 0:00:45  loss: 1.0133 (1.0133)  acc1: 76.5625 (76.5625)  acc5: 93.7500 (93.7500)  time: 0.8679  data: 0.8370  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.4917 (1.3679)  acc1: 68.2292 (67.7083)  acc5: 88.5417 (89.0152)  time: 0.1753  data: 0.1446  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.4917 (1.4368)  acc1: 64.5833 (67.2123)  acc5: 87.5000 (87.6736)  time: 0.1247  data: 0.0940  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.5292 (1.4731)  acc1: 64.0625 (66.6835)  acc5: 85.9375 (87.3320)  time: 0.1197  data: 0.0891  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.5598 (1.5238)  acc1: 63.0208 (65.4599)  acc5: 85.9375 (86.8648)  time: 0.1087  data: 0.0781  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4944 (1.5085)  acc1: 65.1042 (65.8292)  acc5: 87.5000 (87.0813)  time: 0.1143  data: 0.0837  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4870 (1.5018)  acc1: 65.6250 (65.8300)  acc5: 87.5000 (87.2000)  time: 0.0994  data: 0.0697  max mem: 4873\n",
            "Test: Total time: 0:00:06 (0.1265 s / it)\n",
            "* Acc@1 65.830 Acc@5 87.200 loss 1.502\n",
            "Accuracy of the network on the 10000 test images: 65.8%\n",
            "Max accuracy: 65.83%\n",
            "Epoch: [3]  [  0/781]  eta: 0:12:03  lr: 0.000057  loss: 2.4017 (2.4017)  time: 0.9259  data: 0.7892  max mem: 4873\n",
            "Epoch: [3]  [ 10/781]  eta: 0:02:31  lr: 0.000057  loss: 2.5759 (2.8069)  time: 0.1967  data: 0.0729  max mem: 4873\n",
            "Epoch: [3]  [ 20/781]  eta: 0:02:08  lr: 0.000057  loss: 2.4977 (2.7073)  time: 0.1310  data: 0.0075  max mem: 4873\n",
            "Epoch: [3]  [ 30/781]  eta: 0:02:01  lr: 0.000057  loss: 2.4641 (2.6972)  time: 0.1429  data: 0.0187  max mem: 4873\n",
            "Epoch: [3]  [ 40/781]  eta: 0:01:55  lr: 0.000057  loss: 2.4641 (2.6773)  time: 0.1423  data: 0.0187  max mem: 4873\n",
            "Epoch: [3]  [ 50/781]  eta: 0:01:51  lr: 0.000057  loss: 2.4873 (2.7007)  time: 0.1395  data: 0.0166  max mem: 4873\n",
            "Epoch: [3]  [ 60/781]  eta: 0:01:47  lr: 0.000057  loss: 2.4637 (2.6870)  time: 0.1371  data: 0.0153  max mem: 4873\n",
            "Epoch: [3]  [ 70/781]  eta: 0:01:45  lr: 0.000057  loss: 2.4410 (2.6708)  time: 0.1344  data: 0.0135  max mem: 4873\n",
            "Epoch: [3]  [ 80/781]  eta: 0:01:42  lr: 0.000057  loss: 2.5066 (2.6702)  time: 0.1353  data: 0.0144  max mem: 4873\n",
            "Epoch: [3]  [ 90/781]  eta: 0:01:41  lr: 0.000057  loss: 2.5106 (2.6647)  time: 0.1409  data: 0.0194  max mem: 4873\n",
            "Epoch: [3]  [100/781]  eta: 0:01:38  lr: 0.000057  loss: 2.4029 (2.6397)  time: 0.1407  data: 0.0187  max mem: 4873\n",
            "Epoch: [3]  [110/781]  eta: 0:01:38  lr: 0.000057  loss: 2.4290 (2.6261)  time: 0.1449  data: 0.0222  max mem: 4873\n",
            "Epoch: [3]  [120/781]  eta: 0:01:36  lr: 0.000057  loss: 2.4749 (2.6210)  time: 0.1477  data: 0.0242  max mem: 4873\n",
            "Epoch: [3]  [130/781]  eta: 0:01:35  lr: 0.000057  loss: 2.4586 (2.6159)  time: 0.1502  data: 0.0273  max mem: 4873\n",
            "Epoch: [3]  [140/781]  eta: 0:01:33  lr: 0.000057  loss: 2.4454 (2.6220)  time: 0.1515  data: 0.0289  max mem: 4873\n",
            "Epoch: [3]  [150/781]  eta: 0:01:32  lr: 0.000057  loss: 2.4454 (2.6164)  time: 0.1443  data: 0.0222  max mem: 4873\n",
            "Epoch: [3]  [160/781]  eta: 0:01:30  lr: 0.000057  loss: 2.5027 (2.6224)  time: 0.1424  data: 0.0206  max mem: 4873\n",
            "Epoch: [3]  [170/781]  eta: 0:01:28  lr: 0.000057  loss: 2.5289 (2.6228)  time: 0.1393  data: 0.0175  max mem: 4873\n",
            "Epoch: [3]  [180/781]  eta: 0:01:26  lr: 0.000057  loss: 2.5289 (2.6298)  time: 0.1355  data: 0.0138  max mem: 4873\n",
            "Epoch: [3]  [190/781]  eta: 0:01:25  lr: 0.000057  loss: 2.4779 (2.6271)  time: 0.1366  data: 0.0149  max mem: 4873\n",
            "Epoch: [3]  [200/781]  eta: 0:01:23  lr: 0.000057  loss: 2.3925 (2.6362)  time: 0.1402  data: 0.0166  max mem: 4873\n",
            "Epoch: [3]  [210/781]  eta: 0:01:22  lr: 0.000057  loss: 2.3925 (2.6324)  time: 0.1424  data: 0.0174  max mem: 4873\n",
            "Epoch: [3]  [220/781]  eta: 0:01:20  lr: 0.000057  loss: 2.4130 (2.6277)  time: 0.1409  data: 0.0185  max mem: 4873\n",
            "Epoch: [3]  [230/781]  eta: 0:01:19  lr: 0.000057  loss: 2.4798 (2.6340)  time: 0.1354  data: 0.0144  max mem: 4873\n",
            "Epoch: [3]  [240/781]  eta: 0:01:17  lr: 0.000057  loss: 2.4711 (2.6387)  time: 0.1374  data: 0.0164  max mem: 4873\n",
            "Epoch: [3]  [250/781]  eta: 0:01:16  lr: 0.000057  loss: 2.4407 (2.6396)  time: 0.1414  data: 0.0203  max mem: 4873\n",
            "Epoch: [3]  [260/781]  eta: 0:01:14  lr: 0.000057  loss: 2.4764 (2.6343)  time: 0.1396  data: 0.0185  max mem: 4873\n",
            "Epoch: [3]  [270/781]  eta: 0:01:13  lr: 0.000057  loss: 2.4541 (2.6295)  time: 0.1370  data: 0.0143  max mem: 4873\n",
            "Epoch: [3]  [280/781]  eta: 0:01:11  lr: 0.000057  loss: 2.4457 (2.6296)  time: 0.1372  data: 0.0127  max mem: 4873\n",
            "Epoch: [3]  [290/781]  eta: 0:01:10  lr: 0.000057  loss: 2.4243 (2.6261)  time: 0.1430  data: 0.0094  max mem: 4873\n",
            "Epoch: [3]  [300/781]  eta: 0:01:08  lr: 0.000057  loss: 2.3993 (2.6251)  time: 0.1508  data: 0.0176  max mem: 4873\n",
            "Epoch: [3]  [310/781]  eta: 0:01:07  lr: 0.000057  loss: 2.4150 (2.6197)  time: 0.1457  data: 0.0233  max mem: 4873\n",
            "Epoch: [3]  [320/781]  eta: 0:01:05  lr: 0.000057  loss: 2.4131 (2.6165)  time: 0.1391  data: 0.0184  max mem: 4873\n",
            "Epoch: [3]  [330/781]  eta: 0:01:04  lr: 0.000057  loss: 2.4132 (2.6145)  time: 0.1416  data: 0.0209  max mem: 4873\n",
            "Epoch: [3]  [340/781]  eta: 0:01:02  lr: 0.000057  loss: 2.3945 (2.6124)  time: 0.1396  data: 0.0180  max mem: 4873\n",
            "Epoch: [3]  [350/781]  eta: 0:01:01  lr: 0.000057  loss: 2.3728 (2.6124)  time: 0.1362  data: 0.0133  max mem: 4873\n",
            "Epoch: [3]  [360/781]  eta: 0:01:00  lr: 0.000057  loss: 2.3898 (2.6114)  time: 0.1393  data: 0.0165  max mem: 4873\n",
            "Epoch: [3]  [370/781]  eta: 0:00:58  lr: 0.000057  loss: 2.4538 (2.6162)  time: 0.1414  data: 0.0197  max mem: 4873\n",
            "Epoch: [3]  [380/781]  eta: 0:00:57  lr: 0.000057  loss: 2.4585 (2.6119)  time: 0.1392  data: 0.0176  max mem: 4873\n",
            "Epoch: [3]  [390/781]  eta: 0:00:55  lr: 0.000057  loss: 2.5172 (2.6203)  time: 0.1386  data: 0.0154  max mem: 4873\n",
            "Epoch: [3]  [400/781]  eta: 0:00:54  lr: 0.000057  loss: 2.5526 (2.6232)  time: 0.1402  data: 0.0170  max mem: 4873\n",
            "Epoch: [3]  [410/781]  eta: 0:00:52  lr: 0.000057  loss: 2.3953 (2.6166)  time: 0.1416  data: 0.0192  max mem: 4873\n",
            "Epoch: [3]  [420/781]  eta: 0:00:51  lr: 0.000057  loss: 2.3656 (2.6120)  time: 0.1419  data: 0.0198  max mem: 4873\n",
            "Epoch: [3]  [430/781]  eta: 0:00:49  lr: 0.000057  loss: 2.4134 (2.6100)  time: 0.1381  data: 0.0171  max mem: 4873\n",
            "Epoch: [3]  [440/781]  eta: 0:00:48  lr: 0.000057  loss: 2.4539 (2.6083)  time: 0.1388  data: 0.0175  max mem: 4873\n",
            "Epoch: [3]  [450/781]  eta: 0:00:47  lr: 0.000057  loss: 2.4269 (2.6060)  time: 0.1419  data: 0.0207  max mem: 4873\n",
            "Epoch: [3]  [460/781]  eta: 0:00:45  lr: 0.000057  loss: 2.4717 (2.6048)  time: 0.1390  data: 0.0177  max mem: 4873\n",
            "Epoch: [3]  [470/781]  eta: 0:00:44  lr: 0.000057  loss: 2.4236 (2.6062)  time: 0.1384  data: 0.0163  max mem: 4873\n",
            "Epoch: [3]  [480/781]  eta: 0:00:42  lr: 0.000057  loss: 2.4132 (2.6026)  time: 0.1416  data: 0.0194  max mem: 4873\n",
            "Epoch: [3]  [490/781]  eta: 0:00:41  lr: 0.000057  loss: 2.4163 (2.5999)  time: 0.1405  data: 0.0192  max mem: 4873\n",
            "Epoch: [3]  [500/781]  eta: 0:00:39  lr: 0.000057  loss: 2.4307 (2.6005)  time: 0.1401  data: 0.0184  max mem: 4873\n",
            "Epoch: [3]  [510/781]  eta: 0:00:38  lr: 0.000057  loss: 2.4360 (2.5984)  time: 0.1413  data: 0.0194  max mem: 4873\n",
            "Epoch: [3]  [520/781]  eta: 0:00:36  lr: 0.000057  loss: 2.3816 (2.5981)  time: 0.1378  data: 0.0164  max mem: 4873\n",
            "Epoch: [3]  [530/781]  eta: 0:00:35  lr: 0.000057  loss: 2.3850 (2.5946)  time: 0.1379  data: 0.0153  max mem: 4873\n",
            "Epoch: [3]  [540/781]  eta: 0:00:34  lr: 0.000057  loss: 2.4095 (2.5911)  time: 0.1388  data: 0.0155  max mem: 4873\n",
            "Epoch: [3]  [550/781]  eta: 0:00:32  lr: 0.000057  loss: 2.4440 (2.5976)  time: 0.1393  data: 0.0165  max mem: 4873\n",
            "Epoch: [3]  [560/781]  eta: 0:00:31  lr: 0.000057  loss: 2.4556 (2.5951)  time: 0.1450  data: 0.0223  max mem: 4873\n",
            "Epoch: [3]  [570/781]  eta: 0:00:29  lr: 0.000057  loss: 2.4556 (2.5942)  time: 0.1466  data: 0.0246  max mem: 4873\n",
            "Epoch: [3]  [580/781]  eta: 0:00:28  lr: 0.000057  loss: 2.4271 (2.5951)  time: 0.1409  data: 0.0192  max mem: 4873\n",
            "Epoch: [3]  [590/781]  eta: 0:00:27  lr: 0.000057  loss: 2.3763 (2.5938)  time: 0.1373  data: 0.0148  max mem: 4873\n",
            "Epoch: [3]  [600/781]  eta: 0:00:25  lr: 0.000057  loss: 2.3621 (2.5936)  time: 0.1348  data: 0.0121  max mem: 4873\n",
            "Epoch: [3]  [610/781]  eta: 0:00:24  lr: 0.000057  loss: 2.4081 (2.5956)  time: 0.1329  data: 0.0102  max mem: 4873\n",
            "Epoch: [3]  [620/781]  eta: 0:00:22  lr: 0.000057  loss: 2.4424 (2.5936)  time: 0.1377  data: 0.0156  max mem: 4873\n",
            "Epoch: [3]  [630/781]  eta: 0:00:21  lr: 0.000057  loss: 2.4470 (2.5932)  time: 0.1410  data: 0.0193  max mem: 4873\n",
            "Epoch: [3]  [640/781]  eta: 0:00:19  lr: 0.000057  loss: 2.4227 (2.5903)  time: 0.1387  data: 0.0153  max mem: 4873\n",
            "Epoch: [3]  [650/781]  eta: 0:00:18  lr: 0.000057  loss: 2.4750 (2.5929)  time: 0.1406  data: 0.0164  max mem: 4873\n",
            "Epoch: [3]  [660/781]  eta: 0:00:17  lr: 0.000057  loss: 2.4966 (2.5938)  time: 0.1426  data: 0.0190  max mem: 4873\n",
            "Epoch: [3]  [670/781]  eta: 0:00:15  lr: 0.000057  loss: 2.4323 (2.5955)  time: 0.1436  data: 0.0214  max mem: 4873\n",
            "Epoch: [3]  [680/781]  eta: 0:00:14  lr: 0.000057  loss: 2.4082 (2.5921)  time: 0.1401  data: 0.0190  max mem: 4873\n",
            "Epoch: [3]  [690/781]  eta: 0:00:12  lr: 0.000057  loss: 2.3611 (2.5898)  time: 0.1382  data: 0.0170  max mem: 4873\n",
            "Epoch: [3]  [700/781]  eta: 0:00:11  lr: 0.000057  loss: 2.3689 (2.5875)  time: 0.1375  data: 0.0158  max mem: 4873\n",
            "Epoch: [3]  [710/781]  eta: 0:00:10  lr: 0.000057  loss: 2.4589 (2.5901)  time: 0.1388  data: 0.0166  max mem: 4873\n",
            "Epoch: [3]  [720/781]  eta: 0:00:08  lr: 0.000057  loss: 2.4819 (2.5911)  time: 0.1429  data: 0.0212  max mem: 4873\n",
            "Epoch: [3]  [730/781]  eta: 0:00:07  lr: 0.000057  loss: 2.4037 (2.5878)  time: 0.1415  data: 0.0197  max mem: 4873\n",
            "Epoch: [3]  [740/781]  eta: 0:00:05  lr: 0.000057  loss: 2.3569 (2.5875)  time: 0.1478  data: 0.0251  max mem: 4873\n",
            "Epoch: [3]  [750/781]  eta: 0:00:04  lr: 0.000057  loss: 2.3569 (2.5865)  time: 0.1539  data: 0.0319  max mem: 4873\n",
            "Epoch: [3]  [760/781]  eta: 0:00:02  lr: 0.000057  loss: 2.3688 (2.5860)  time: 0.1455  data: 0.0248  max mem: 4873\n",
            "Epoch: [3]  [770/781]  eta: 0:00:01  lr: 0.000057  loss: 2.3818 (2.5841)  time: 0.1359  data: 0.0150  max mem: 4873\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000057  loss: 2.3773 (2.5831)  time: 0.1379  data: 0.0172  max mem: 4873\n",
            "Epoch: [3] Total time: 0:01:50 (0.1416 s / it)\n",
            "Averaged stats: lr: 0.000057  loss: 2.3773 (2.5831)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.9607 (0.9607)  acc1: 78.6458 (78.6458)  acc5: 94.2708 (94.2708)  time: 0.8311  data: 0.8002  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2653 (1.2251)  acc1: 70.8333 (72.0170)  acc5: 90.6250 (90.9091)  time: 0.1742  data: 0.1435  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.3203 (1.2782)  acc1: 68.7500 (71.2302)  acc5: 89.5833 (89.5833)  time: 0.1234  data: 0.0928  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3729 (1.3161)  acc1: 69.2708 (70.6485)  acc5: 89.0625 (89.1129)  time: 0.1244  data: 0.0937  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4380 (1.3747)  acc1: 66.6667 (68.9660)  acc5: 86.4583 (88.3638)  time: 0.1225  data: 0.0918  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4200 (1.3668)  acc1: 66.6667 (68.9338)  acc5: 86.4583 (88.2864)  time: 0.1154  data: 0.0847  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4380 (1.3720)  acc1: 65.6250 (68.7900)  acc5: 86.4583 (88.3700)  time: 0.0966  data: 0.0669  max mem: 4873\n",
            "Test: Total time: 0:00:06 (0.1285 s / it)\n",
            "* Acc@1 68.790 Acc@5 88.370 loss 1.372\n",
            "Accuracy of the network on the 10000 test images: 68.8%\n",
            "Max accuracy: 68.79%\n",
            "Epoch: [4]  [  0/781]  eta: 0:11:40  lr: 0.000052  loss: 2.3182 (2.3182)  time: 0.8966  data: 0.7695  max mem: 4873\n",
            "Epoch: [4]  [ 10/781]  eta: 0:02:28  lr: 0.000052  loss: 2.3612 (2.4557)  time: 0.1931  data: 0.0711  max mem: 4873\n",
            "Epoch: [4]  [ 20/781]  eta: 0:02:02  lr: 0.000052  loss: 2.4307 (2.5243)  time: 0.1243  data: 0.0020  max mem: 4873\n",
            "Epoch: [4]  [ 30/781]  eta: 0:01:51  lr: 0.000052  loss: 2.4492 (2.5174)  time: 0.1244  data: 0.0017  max mem: 4873\n",
            "Epoch: [4]  [ 40/781]  eta: 0:01:46  lr: 0.000052  loss: 2.3925 (2.4826)  time: 0.1246  data: 0.0023  max mem: 4873\n",
            "Epoch: [4]  [ 50/781]  eta: 0:01:42  lr: 0.000052  loss: 2.3998 (2.4852)  time: 0.1269  data: 0.0051  max mem: 4873\n",
            "Epoch: [4]  [ 60/781]  eta: 0:01:39  lr: 0.000052  loss: 2.4572 (2.5109)  time: 0.1277  data: 0.0063  max mem: 4873\n",
            "Epoch: [4]  [ 70/781]  eta: 0:01:37  lr: 0.000052  loss: 2.4061 (2.5154)  time: 0.1279  data: 0.0062  max mem: 4873\n",
            "Epoch: [4]  [ 80/781]  eta: 0:01:36  lr: 0.000052  loss: 2.3637 (2.5139)  time: 0.1346  data: 0.0109  max mem: 4873\n",
            "Epoch: [4]  [ 90/781]  eta: 0:01:34  lr: 0.000052  loss: 2.3495 (2.5062)  time: 0.1369  data: 0.0122  max mem: 4873\n",
            "Epoch: [4]  [100/781]  eta: 0:01:32  lr: 0.000052  loss: 2.2881 (2.4812)  time: 0.1278  data: 0.0053  max mem: 4873\n",
            "Epoch: [4]  [110/781]  eta: 0:01:31  lr: 0.000052  loss: 2.2881 (2.4829)  time: 0.1335  data: 0.0118  max mem: 4873\n",
            "Epoch: [4]  [120/781]  eta: 0:01:29  lr: 0.000052  loss: 2.3765 (2.4748)  time: 0.1359  data: 0.0134  max mem: 4873\n",
            "Epoch: [4]  [130/781]  eta: 0:01:27  lr: 0.000052  loss: 2.3405 (2.4641)  time: 0.1270  data: 0.0048  max mem: 4873\n",
            "Epoch: [4]  [140/781]  eta: 0:01:26  lr: 0.000052  loss: 2.3554 (2.4662)  time: 0.1292  data: 0.0071  max mem: 4873\n",
            "Epoch: [4]  [150/781]  eta: 0:01:24  lr: 0.000052  loss: 2.3925 (2.4767)  time: 0.1329  data: 0.0113  max mem: 4873\n",
            "Epoch: [4]  [160/781]  eta: 0:01:23  lr: 0.000052  loss: 2.3362 (2.4679)  time: 0.1329  data: 0.0104  max mem: 4873\n",
            "Epoch: [4]  [170/781]  eta: 0:01:22  lr: 0.000052  loss: 2.3362 (2.4715)  time: 0.1317  data: 0.0084  max mem: 4873\n",
            "Epoch: [4]  [180/781]  eta: 0:01:20  lr: 0.000052  loss: 2.3052 (2.4646)  time: 0.1333  data: 0.0101  max mem: 4873\n",
            "Epoch: [4]  [190/781]  eta: 0:01:19  lr: 0.000052  loss: 2.3010 (2.4557)  time: 0.1425  data: 0.0198  max mem: 4873\n",
            "Epoch: [4]  [200/781]  eta: 0:01:18  lr: 0.000052  loss: 2.2953 (2.4587)  time: 0.1408  data: 0.0184  max mem: 4873\n",
            "Epoch: [4]  [210/781]  eta: 0:01:16  lr: 0.000052  loss: 2.3924 (2.4646)  time: 0.1300  data: 0.0079  max mem: 4873\n",
            "Epoch: [4]  [220/781]  eta: 0:01:15  lr: 0.000052  loss: 2.3924 (2.4766)  time: 0.1277  data: 0.0064  max mem: 4873\n",
            "Epoch: [4]  [230/781]  eta: 0:01:13  lr: 0.000052  loss: 2.4375 (2.4882)  time: 0.1251  data: 0.0040  max mem: 4873\n",
            "Epoch: [4]  [240/781]  eta: 0:01:12  lr: 0.000052  loss: 2.4375 (2.4863)  time: 0.1220  data: 0.0011  max mem: 4873\n",
            "Epoch: [4]  [250/781]  eta: 0:01:10  lr: 0.000052  loss: 2.3264 (2.4830)  time: 0.1215  data: 0.0007  max mem: 4873\n",
            "Epoch: [4]  [260/781]  eta: 0:01:09  lr: 0.000052  loss: 2.3872 (2.4846)  time: 0.1247  data: 0.0037  max mem: 4873\n",
            "Epoch: [4]  [270/781]  eta: 0:01:07  lr: 0.000052  loss: 2.3991 (2.4794)  time: 0.1326  data: 0.0109  max mem: 4873\n",
            "Epoch: [4]  [280/781]  eta: 0:01:06  lr: 0.000052  loss: 2.3991 (2.4768)  time: 0.1332  data: 0.0114  max mem: 4873\n",
            "Epoch: [4]  [290/781]  eta: 0:01:05  lr: 0.000052  loss: 2.3810 (2.4767)  time: 0.1356  data: 0.0134  max mem: 4873\n",
            "Epoch: [4]  [300/781]  eta: 0:01:03  lr: 0.000052  loss: 2.2657 (2.4745)  time: 0.1366  data: 0.0144  max mem: 4873\n",
            "Epoch: [4]  [310/781]  eta: 0:01:02  lr: 0.000052  loss: 2.3292 (2.4756)  time: 0.1278  data: 0.0055  max mem: 4873\n",
            "Epoch: [4]  [320/781]  eta: 0:01:01  lr: 0.000052  loss: 2.3564 (2.4790)  time: 0.1235  data: 0.0011  max mem: 4873\n",
            "Epoch: [4]  [330/781]  eta: 0:00:59  lr: 0.000052  loss: 2.3529 (2.4767)  time: 0.1236  data: 0.0015  max mem: 4873\n",
            "Epoch: [4]  [340/781]  eta: 0:00:58  lr: 0.000052  loss: 2.3529 (2.4773)  time: 0.1294  data: 0.0072  max mem: 4873\n",
            "Epoch: [4]  [350/781]  eta: 0:00:56  lr: 0.000052  loss: 2.3330 (2.4734)  time: 0.1304  data: 0.0082  max mem: 4873\n",
            "Epoch: [4]  [360/781]  eta: 0:00:55  lr: 0.000052  loss: 2.3217 (2.4737)  time: 0.1296  data: 0.0057  max mem: 4873\n",
            "Epoch: [4]  [370/781]  eta: 0:00:54  lr: 0.000052  loss: 2.3685 (2.4745)  time: 0.1293  data: 0.0052  max mem: 4873\n",
            "Epoch: [4]  [380/781]  eta: 0:00:52  lr: 0.000052  loss: 2.3902 (2.4846)  time: 0.1256  data: 0.0031  max mem: 4873\n",
            "Epoch: [4]  [390/781]  eta: 0:00:51  lr: 0.000052  loss: 2.4073 (2.4822)  time: 0.1274  data: 0.0051  max mem: 4873\n",
            "Epoch: [4]  [400/781]  eta: 0:00:50  lr: 0.000052  loss: 2.3531 (2.4817)  time: 0.1293  data: 0.0068  max mem: 4873\n",
            "Epoch: [4]  [410/781]  eta: 0:00:48  lr: 0.000052  loss: 2.3353 (2.4828)  time: 0.1277  data: 0.0049  max mem: 4873\n",
            "Epoch: [4]  [420/781]  eta: 0:00:47  lr: 0.000052  loss: 2.3981 (2.4887)  time: 0.1264  data: 0.0044  max mem: 4873\n",
            "Epoch: [4]  [430/781]  eta: 0:00:46  lr: 0.000052  loss: 2.4535 (2.4863)  time: 0.1254  data: 0.0040  max mem: 4873\n",
            "Epoch: [4]  [440/781]  eta: 0:00:44  lr: 0.000052  loss: 2.3222 (2.4862)  time: 0.1262  data: 0.0046  max mem: 4873\n",
            "Epoch: [4]  [450/781]  eta: 0:00:43  lr: 0.000052  loss: 2.3502 (2.4937)  time: 0.1298  data: 0.0078  max mem: 4873\n",
            "Epoch: [4]  [460/781]  eta: 0:00:42  lr: 0.000052  loss: 2.3697 (2.4925)  time: 0.1329  data: 0.0102  max mem: 4873\n",
            "Epoch: [4]  [470/781]  eta: 0:00:40  lr: 0.000052  loss: 2.3351 (2.4917)  time: 0.1307  data: 0.0076  max mem: 4873\n",
            "Epoch: [4]  [480/781]  eta: 0:00:39  lr: 0.000052  loss: 2.3458 (2.4906)  time: 0.1265  data: 0.0041  max mem: 4873\n",
            "Epoch: [4]  [490/781]  eta: 0:00:38  lr: 0.000052  loss: 2.3662 (2.4894)  time: 0.1250  data: 0.0037  max mem: 4873\n",
            "Epoch: [4]  [500/781]  eta: 0:00:36  lr: 0.000052  loss: 2.3795 (2.4868)  time: 0.1262  data: 0.0052  max mem: 4873\n",
            "Epoch: [4]  [510/781]  eta: 0:00:35  lr: 0.000052  loss: 2.3795 (2.4894)  time: 0.1288  data: 0.0072  max mem: 4873\n",
            "Epoch: [4]  [520/781]  eta: 0:00:34  lr: 0.000052  loss: 2.2772 (2.4855)  time: 0.1270  data: 0.0036  max mem: 4873\n",
            "Epoch: [4]  [530/781]  eta: 0:00:32  lr: 0.000052  loss: 2.2772 (2.4865)  time: 0.1230  data: 0.0003  max mem: 4873\n",
            "Epoch: [4]  [540/781]  eta: 0:00:31  lr: 0.000052  loss: 2.3319 (2.4847)  time: 0.1226  data: 0.0015  max mem: 4873\n",
            "Epoch: [4]  [550/781]  eta: 0:00:30  lr: 0.000052  loss: 2.3295 (2.4821)  time: 0.1255  data: 0.0039  max mem: 4873\n",
            "Epoch: [4]  [560/781]  eta: 0:00:28  lr: 0.000052  loss: 2.3310 (2.4855)  time: 0.1314  data: 0.0091  max mem: 4873\n",
            "Epoch: [4]  [570/781]  eta: 0:00:27  lr: 0.000052  loss: 2.3353 (2.4844)  time: 0.1320  data: 0.0094  max mem: 4873\n",
            "Epoch: [4]  [580/781]  eta: 0:00:26  lr: 0.000052  loss: 2.3079 (2.4845)  time: 0.1263  data: 0.0040  max mem: 4873\n",
            "Epoch: [4]  [590/781]  eta: 0:00:24  lr: 0.000052  loss: 2.3079 (2.4884)  time: 0.1264  data: 0.0029  max mem: 4873\n",
            "Epoch: [4]  [600/781]  eta: 0:00:23  lr: 0.000052  loss: 2.3359 (2.4877)  time: 0.1260  data: 0.0021  max mem: 4873\n",
            "Epoch: [4]  [610/781]  eta: 0:00:22  lr: 0.000052  loss: 2.3359 (2.4850)  time: 0.1259  data: 0.0034  max mem: 4873\n",
            "Epoch: [4]  [620/781]  eta: 0:00:20  lr: 0.000052  loss: 2.3549 (2.4836)  time: 0.1255  data: 0.0033  max mem: 4873\n",
            "Epoch: [4]  [630/781]  eta: 0:00:19  lr: 0.000052  loss: 2.3549 (2.4868)  time: 0.1247  data: 0.0020  max mem: 4873\n",
            "Epoch: [4]  [640/781]  eta: 0:00:18  lr: 0.000052  loss: 2.2885 (2.4885)  time: 0.1273  data: 0.0045  max mem: 4873\n",
            "Epoch: [4]  [650/781]  eta: 0:00:17  lr: 0.000052  loss: 2.2977 (2.4884)  time: 0.1291  data: 0.0054  max mem: 4873\n",
            "Epoch: [4]  [660/781]  eta: 0:00:15  lr: 0.000052  loss: 2.3133 (2.4876)  time: 0.1294  data: 0.0029  max mem: 4873\n",
            "Epoch: [4]  [670/781]  eta: 0:00:14  lr: 0.000052  loss: 2.3133 (2.4870)  time: 0.1289  data: 0.0021  max mem: 4873\n",
            "Epoch: [4]  [680/781]  eta: 0:00:13  lr: 0.000052  loss: 2.3812 (2.4878)  time: 0.1261  data: 0.0027  max mem: 4873\n",
            "Epoch: [4]  [690/781]  eta: 0:00:11  lr: 0.000052  loss: 2.3778 (2.4862)  time: 0.1255  data: 0.0036  max mem: 4873\n",
            "Epoch: [4]  [700/781]  eta: 0:00:10  lr: 0.000052  loss: 2.3047 (2.4880)  time: 0.1294  data: 0.0069  max mem: 4873\n",
            "Epoch: [4]  [710/781]  eta: 0:00:09  lr: 0.000052  loss: 2.3120 (2.4869)  time: 0.1320  data: 0.0079  max mem: 4873\n",
            "Epoch: [4]  [720/781]  eta: 0:00:07  lr: 0.000052  loss: 2.2751 (2.4849)  time: 0.1296  data: 0.0055  max mem: 4873\n",
            "Epoch: [4]  [730/781]  eta: 0:00:06  lr: 0.000052  loss: 2.2602 (2.4832)  time: 0.1245  data: 0.0021  max mem: 4873\n",
            "Epoch: [4]  [740/781]  eta: 0:00:05  lr: 0.000052  loss: 2.3051 (2.4851)  time: 0.1228  data: 0.0009  max mem: 4873\n",
            "Epoch: [4]  [750/781]  eta: 0:00:04  lr: 0.000052  loss: 2.3051 (2.4853)  time: 0.1306  data: 0.0076  max mem: 4873\n",
            "Epoch: [4]  [760/781]  eta: 0:00:02  lr: 0.000052  loss: 2.4080 (2.4879)  time: 0.1409  data: 0.0166  max mem: 4873\n",
            "Epoch: [4]  [770/781]  eta: 0:00:01  lr: 0.000052  loss: 2.3984 (2.4867)  time: 0.1414  data: 0.0181  max mem: 4873\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000052  loss: 2.3984 (2.4935)  time: 0.1361  data: 0.0146  max mem: 4873\n",
            "Epoch: [4] Total time: 0:01:41 (0.1301 s / it)\n",
            "Averaged stats: lr: 0.000052  loss: 2.3984 (2.4935)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8933 (0.8933)  acc1: 78.1250 (78.1250)  acc5: 94.2708 (94.2708)  time: 0.8281  data: 0.7972  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2172 (1.1439)  acc1: 71.8750 (74.0530)  acc5: 92.1875 (91.8561)  time: 0.1694  data: 0.1388  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.2664 (1.2243)  acc1: 70.8333 (72.2718)  acc5: 90.6250 (90.3522)  time: 0.1068  data: 0.0762  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3495 (1.2646)  acc1: 68.7500 (71.4550)  acc5: 89.5833 (89.9866)  time: 0.1313  data: 0.1006  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4168 (1.3002)  acc1: 68.7500 (70.5920)  acc5: 89.0625 (89.6214)  time: 0.1320  data: 0.1013  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3360 (1.2918)  acc1: 70.3125 (70.5576)  acc5: 89.0625 (89.6957)  time: 0.1319  data: 0.1012  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3232 (1.2889)  acc1: 70.3125 (70.4900)  acc5: 89.0625 (89.8000)  time: 0.1309  data: 0.1012  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1364 s / it)\n",
            "* Acc@1 70.490 Acc@5 89.800 loss 1.289\n",
            "Accuracy of the network on the 10000 test images: 70.5%\n",
            "Max accuracy: 70.49%\n",
            "Epoch: [5]  [  0/781]  eta: 0:11:42  lr: 0.000044  loss: 2.3887 (2.3887)  time: 0.8989  data: 0.7504  max mem: 4873\n",
            "Epoch: [5]  [ 10/781]  eta: 0:02:42  lr: 0.000044  loss: 2.3887 (2.6310)  time: 0.2110  data: 0.0853  max mem: 4873\n",
            "Epoch: [5]  [ 20/781]  eta: 0:02:13  lr: 0.000044  loss: 2.3604 (2.5261)  time: 0.1387  data: 0.0151  max mem: 4873\n",
            "Epoch: [5]  [ 30/781]  eta: 0:02:01  lr: 0.000044  loss: 2.3532 (2.4695)  time: 0.1347  data: 0.0114  max mem: 4873\n",
            "Epoch: [5]  [ 40/781]  eta: 0:01:54  lr: 0.000044  loss: 2.1792 (2.4084)  time: 0.1323  data: 0.0100  max mem: 4873\n",
            "Epoch: [5]  [ 50/781]  eta: 0:01:50  lr: 0.000044  loss: 2.1792 (2.4202)  time: 0.1335  data: 0.0120  max mem: 4873\n",
            "Epoch: [5]  [ 60/781]  eta: 0:01:46  lr: 0.000044  loss: 2.3461 (2.4593)  time: 0.1352  data: 0.0134  max mem: 4873\n",
            "Epoch: [5]  [ 70/781]  eta: 0:01:44  lr: 0.000044  loss: 2.3820 (2.5210)  time: 0.1351  data: 0.0128  max mem: 4873\n",
            "Epoch: [5]  [ 80/781]  eta: 0:01:42  lr: 0.000044  loss: 2.3811 (2.5014)  time: 0.1389  data: 0.0166  max mem: 4873\n",
            "Epoch: [5]  [ 90/781]  eta: 0:01:40  lr: 0.000044  loss: 2.3282 (2.5224)  time: 0.1446  data: 0.0220  max mem: 4873\n",
            "Epoch: [5]  [100/781]  eta: 0:01:38  lr: 0.000044  loss: 2.3143 (2.5021)  time: 0.1438  data: 0.0203  max mem: 4873\n",
            "Epoch: [5]  [110/781]  eta: 0:01:37  lr: 0.000044  loss: 2.2676 (2.5137)  time: 0.1429  data: 0.0196  max mem: 4873\n",
            "Epoch: [5]  [120/781]  eta: 0:01:35  lr: 0.000044  loss: 2.3268 (2.5088)  time: 0.1389  data: 0.0161  max mem: 4873\n",
            "Epoch: [5]  [130/781]  eta: 0:01:33  lr: 0.000044  loss: 2.3330 (2.5260)  time: 0.1337  data: 0.0105  max mem: 4873\n",
            "Epoch: [5]  [140/781]  eta: 0:01:31  lr: 0.000044  loss: 2.3494 (2.5269)  time: 0.1331  data: 0.0081  max mem: 4873\n",
            "Epoch: [5]  [150/781]  eta: 0:01:29  lr: 0.000044  loss: 2.3551 (2.5354)  time: 0.1353  data: 0.0108  max mem: 4873\n",
            "Epoch: [5]  [160/781]  eta: 0:01:28  lr: 0.000044  loss: 2.2705 (2.5187)  time: 0.1369  data: 0.0151  max mem: 4873\n",
            "Epoch: [5]  [170/781]  eta: 0:01:26  lr: 0.000044  loss: 2.2729 (2.5130)  time: 0.1321  data: 0.0108  max mem: 4873\n",
            "Epoch: [5]  [180/781]  eta: 0:01:24  lr: 0.000044  loss: 2.3261 (2.5119)  time: 0.1317  data: 0.0094  max mem: 4873\n",
            "Epoch: [5]  [190/781]  eta: 0:01:23  lr: 0.000044  loss: 2.2964 (2.5123)  time: 0.1373  data: 0.0142  max mem: 4873\n",
            "Epoch: [5]  [200/781]  eta: 0:01:21  lr: 0.000044  loss: 2.2959 (2.5076)  time: 0.1384  data: 0.0157  max mem: 4873\n",
            "Epoch: [5]  [210/781]  eta: 0:01:20  lr: 0.000044  loss: 2.2840 (2.5101)  time: 0.1350  data: 0.0125  max mem: 4873\n",
            "Epoch: [5]  [220/781]  eta: 0:01:18  lr: 0.000044  loss: 2.2933 (2.5038)  time: 0.1318  data: 0.0089  max mem: 4873\n",
            "Epoch: [5]  [230/781]  eta: 0:01:16  lr: 0.000044  loss: 2.3073 (2.5063)  time: 0.1321  data: 0.0082  max mem: 4873\n",
            "Epoch: [5]  [240/781]  eta: 0:01:15  lr: 0.000044  loss: 2.2994 (2.5033)  time: 0.1307  data: 0.0069  max mem: 4873\n",
            "Epoch: [5]  [250/781]  eta: 0:01:13  lr: 0.000044  loss: 2.2321 (2.4911)  time: 0.1299  data: 0.0075  max mem: 4873\n",
            "Epoch: [5]  [260/781]  eta: 0:01:12  lr: 0.000044  loss: 2.2337 (2.4943)  time: 0.1295  data: 0.0072  max mem: 4873\n",
            "Epoch: [5]  [270/781]  eta: 0:01:10  lr: 0.000044  loss: 2.2904 (2.4911)  time: 0.1260  data: 0.0029  max mem: 4873\n",
            "Epoch: [5]  [280/781]  eta: 0:01:09  lr: 0.000044  loss: 2.3094 (2.4865)  time: 0.1348  data: 0.0111  max mem: 4873\n",
            "Epoch: [5]  [290/781]  eta: 0:01:07  lr: 0.000044  loss: 2.2892 (2.4876)  time: 0.1413  data: 0.0185  max mem: 4873\n",
            "Epoch: [5]  [300/781]  eta: 0:01:06  lr: 0.000044  loss: 2.2347 (2.4880)  time: 0.1386  data: 0.0161  max mem: 4873\n",
            "Epoch: [5]  [310/781]  eta: 0:01:04  lr: 0.000044  loss: 2.2726 (2.4867)  time: 0.1338  data: 0.0111  max mem: 4873\n",
            "Epoch: [5]  [320/781]  eta: 0:01:03  lr: 0.000044  loss: 2.3108 (2.4880)  time: 0.1359  data: 0.0132  max mem: 4873\n",
            "Epoch: [5]  [330/781]  eta: 0:01:02  lr: 0.000044  loss: 2.2706 (2.4844)  time: 0.1402  data: 0.0173  max mem: 4873\n",
            "Epoch: [5]  [340/781]  eta: 0:01:00  lr: 0.000044  loss: 2.2682 (2.4906)  time: 0.1423  data: 0.0201  max mem: 4873\n",
            "Epoch: [5]  [350/781]  eta: 0:00:59  lr: 0.000044  loss: 2.2660 (2.4837)  time: 0.1411  data: 0.0195  max mem: 4873\n",
            "Epoch: [5]  [360/781]  eta: 0:00:58  lr: 0.000044  loss: 2.2660 (2.4831)  time: 0.1386  data: 0.0162  max mem: 4873\n",
            "Epoch: [5]  [370/781]  eta: 0:00:56  lr: 0.000044  loss: 2.3006 (2.4825)  time: 0.1423  data: 0.0194  max mem: 4873\n",
            "Epoch: [5]  [380/781]  eta: 0:00:55  lr: 0.000044  loss: 2.2922 (2.4776)  time: 0.1448  data: 0.0227  max mem: 4873\n",
            "Epoch: [5]  [390/781]  eta: 0:00:54  lr: 0.000044  loss: 2.2697 (2.4822)  time: 0.1405  data: 0.0166  max mem: 4873\n",
            "Epoch: [5]  [400/781]  eta: 0:00:52  lr: 0.000044  loss: 2.3297 (2.4782)  time: 0.1383  data: 0.0143  max mem: 4873\n",
            "Epoch: [5]  [410/781]  eta: 0:00:51  lr: 0.000044  loss: 2.2774 (2.4756)  time: 0.1336  data: 0.0110  max mem: 4873\n",
            "Epoch: [5]  [420/781]  eta: 0:00:49  lr: 0.000044  loss: 2.2283 (2.4740)  time: 0.1301  data: 0.0073  max mem: 4873\n",
            "Epoch: [5]  [430/781]  eta: 0:00:48  lr: 0.000044  loss: 2.2321 (2.4717)  time: 0.1284  data: 0.0061  max mem: 4873\n",
            "Epoch: [5]  [440/781]  eta: 0:00:46  lr: 0.000044  loss: 2.2120 (2.4735)  time: 0.1247  data: 0.0023  max mem: 4873\n",
            "Epoch: [5]  [450/781]  eta: 0:00:45  lr: 0.000044  loss: 2.2537 (2.4689)  time: 0.1279  data: 0.0052  max mem: 4873\n",
            "Epoch: [5]  [460/781]  eta: 0:00:44  lr: 0.000044  loss: 2.2544 (2.4664)  time: 0.1351  data: 0.0117  max mem: 4873\n",
            "Epoch: [5]  [470/781]  eta: 0:00:42  lr: 0.000044  loss: 2.2713 (2.4673)  time: 0.1367  data: 0.0127  max mem: 4873\n",
            "Epoch: [5]  [480/781]  eta: 0:00:41  lr: 0.000044  loss: 2.2483 (2.4671)  time: 0.1301  data: 0.0067  max mem: 4873\n",
            "Epoch: [5]  [490/781]  eta: 0:00:39  lr: 0.000044  loss: 2.2483 (2.4676)  time: 0.1286  data: 0.0056  max mem: 4873\n",
            "Epoch: [5]  [500/781]  eta: 0:00:38  lr: 0.000044  loss: 2.2813 (2.4641)  time: 0.1274  data: 0.0035  max mem: 4873\n",
            "Epoch: [5]  [510/781]  eta: 0:00:37  lr: 0.000044  loss: 2.2404 (2.4602)  time: 0.1260  data: 0.0023  max mem: 4873\n",
            "Epoch: [5]  [520/781]  eta: 0:00:35  lr: 0.000044  loss: 2.2582 (2.4581)  time: 0.1273  data: 0.0038  max mem: 4873\n",
            "Epoch: [5]  [530/781]  eta: 0:00:34  lr: 0.000044  loss: 2.3107 (2.4595)  time: 0.1258  data: 0.0028  max mem: 4873\n",
            "Epoch: [5]  [540/781]  eta: 0:00:32  lr: 0.000044  loss: 2.3068 (2.4589)  time: 0.1287  data: 0.0070  max mem: 4873\n",
            "Epoch: [5]  [550/781]  eta: 0:00:31  lr: 0.000044  loss: 2.2629 (2.4586)  time: 0.1330  data: 0.0114  max mem: 4873\n",
            "Epoch: [5]  [560/781]  eta: 0:00:30  lr: 0.000044  loss: 2.2629 (2.4577)  time: 0.1343  data: 0.0123  max mem: 4873\n",
            "Epoch: [5]  [570/781]  eta: 0:00:28  lr: 0.000044  loss: 2.3184 (2.4560)  time: 0.1352  data: 0.0131  max mem: 4873\n",
            "Epoch: [5]  [580/781]  eta: 0:00:27  lr: 0.000044  loss: 2.3244 (2.4576)  time: 0.1338  data: 0.0115  max mem: 4873\n",
            "Epoch: [5]  [590/781]  eta: 0:00:25  lr: 0.000044  loss: 2.3162 (2.4578)  time: 0.1382  data: 0.0067  max mem: 4873\n",
            "Epoch: [5]  [600/781]  eta: 0:00:24  lr: 0.000044  loss: 2.3326 (2.4594)  time: 0.1325  data: 0.0012  max mem: 4873\n",
            "Epoch: [5]  [610/781]  eta: 0:00:23  lr: 0.000044  loss: 2.2599 (2.4598)  time: 0.1218  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [620/781]  eta: 0:00:21  lr: 0.000044  loss: 2.2838 (2.4618)  time: 0.1225  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [630/781]  eta: 0:00:20  lr: 0.000044  loss: 2.2951 (2.4594)  time: 0.1230  data: 0.0004  max mem: 4873\n",
            "Epoch: [5]  [640/781]  eta: 0:00:19  lr: 0.000044  loss: 2.2144 (2.4567)  time: 0.1251  data: 0.0034  max mem: 4873\n",
            "Epoch: [5]  [650/781]  eta: 0:00:17  lr: 0.000044  loss: 2.2144 (2.4606)  time: 0.1301  data: 0.0080  max mem: 4873\n",
            "Epoch: [5]  [660/781]  eta: 0:00:16  lr: 0.000044  loss: 2.2165 (2.4570)  time: 0.1320  data: 0.0091  max mem: 4873\n",
            "Epoch: [5]  [670/781]  eta: 0:00:14  lr: 0.000044  loss: 2.2375 (2.4543)  time: 0.1286  data: 0.0060  max mem: 4873\n",
            "Epoch: [5]  [680/781]  eta: 0:00:13  lr: 0.000044  loss: 2.2536 (2.4522)  time: 0.1290  data: 0.0071  max mem: 4873\n",
            "Epoch: [5]  [690/781]  eta: 0:00:12  lr: 0.000044  loss: 2.2228 (2.4532)  time: 0.1328  data: 0.0112  max mem: 4873\n",
            "Epoch: [5]  [700/781]  eta: 0:00:10  lr: 0.000044  loss: 2.2298 (2.4518)  time: 0.1310  data: 0.0097  max mem: 4873\n",
            "Epoch: [5]  [710/781]  eta: 0:00:09  lr: 0.000044  loss: 2.2787 (2.4514)  time: 0.1261  data: 0.0044  max mem: 4873\n",
            "Epoch: [5]  [720/781]  eta: 0:00:08  lr: 0.000044  loss: 2.3239 (2.4530)  time: 0.1229  data: 0.0006  max mem: 4873\n",
            "Epoch: [5]  [730/781]  eta: 0:00:06  lr: 0.000044  loss: 2.2862 (2.4545)  time: 0.1245  data: 0.0020  max mem: 4873\n",
            "Epoch: [5]  [740/781]  eta: 0:00:05  lr: 0.000044  loss: 2.2941 (2.4553)  time: 0.1305  data: 0.0073  max mem: 4873\n",
            "Epoch: [5]  [750/781]  eta: 0:00:04  lr: 0.000044  loss: 2.3083 (2.4571)  time: 0.1332  data: 0.0097  max mem: 4873\n",
            "Epoch: [5]  [760/781]  eta: 0:00:02  lr: 0.000044  loss: 2.2846 (2.4575)  time: 0.1281  data: 0.0055  max mem: 4873\n",
            "Epoch: [5]  [770/781]  eta: 0:00:01  lr: 0.000044  loss: 2.2765 (2.4607)  time: 0.1269  data: 0.0054  max mem: 4873\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000044  loss: 2.3028 (2.4597)  time: 0.1266  data: 0.0057  max mem: 4873\n",
            "Epoch: [5] Total time: 0:01:44 (0.1341 s / it)\n",
            "Averaged stats: lr: 0.000044  loss: 2.3028 (2.4597)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.9114 (0.9114)  acc1: 77.6042 (77.6042)  acc5: 94.2708 (94.2708)  time: 0.8268  data: 0.7959  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1719 (1.1071)  acc1: 71.8750 (74.4318)  acc5: 92.7083 (91.7140)  time: 0.1693  data: 0.1386  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.1792 (1.1540)  acc1: 71.3542 (73.6607)  acc5: 90.6250 (90.7490)  time: 0.1075  data: 0.0768  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2766 (1.1957)  acc1: 70.8333 (72.9335)  acc5: 90.1042 (90.3394)  time: 0.1327  data: 0.1020  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3580 (1.2568)  acc1: 70.8333 (71.6972)  acc5: 90.1042 (89.8247)  time: 0.1341  data: 0.1035  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2778 (1.2457)  acc1: 70.3125 (71.8852)  acc5: 90.6250 (90.0837)  time: 0.1348  data: 0.1041  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2778 (1.2498)  acc1: 69.7917 (71.7400)  acc5: 90.6250 (90.1600)  time: 0.1338  data: 0.1041  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1378 s / it)\n",
            "* Acc@1 71.740 Acc@5 90.160 loss 1.250\n",
            "Accuracy of the network on the 10000 test images: 71.7%\n",
            "Max accuracy: 71.74%\n",
            "Epoch: [6]  [  0/781]  eta: 0:12:18  lr: 0.000036  loss: 2.3166 (2.3166)  time: 0.9460  data: 0.7996  max mem: 4873\n",
            "Epoch: [6]  [ 10/781]  eta: 0:02:33  lr: 0.000036  loss: 2.1440 (2.1499)  time: 0.1984  data: 0.0739  max mem: 4873\n",
            "Epoch: [6]  [ 20/781]  eta: 0:02:08  lr: 0.000036  loss: 2.1786 (2.2667)  time: 0.1301  data: 0.0077  max mem: 4873\n",
            "Epoch: [6]  [ 30/781]  eta: 0:02:02  lr: 0.000036  loss: 2.2959 (2.2865)  time: 0.1440  data: 0.0214  max mem: 4873\n",
            "Epoch: [6]  [ 40/781]  eta: 0:01:57  lr: 0.000036  loss: 2.3217 (2.4244)  time: 0.1481  data: 0.0256  max mem: 4873\n",
            "Epoch: [6]  [ 50/781]  eta: 0:01:56  lr: 0.000036  loss: 2.3269 (2.4379)  time: 0.1543  data: 0.0324  max mem: 4873\n",
            "Epoch: [6]  [ 60/781]  eta: 0:01:52  lr: 0.000036  loss: 2.2377 (2.4134)  time: 0.1514  data: 0.0297  max mem: 4873\n",
            "Epoch: [6]  [ 70/781]  eta: 0:01:51  lr: 0.000036  loss: 2.2377 (2.4177)  time: 0.1476  data: 0.0259  max mem: 4873\n",
            "Epoch: [6]  [ 80/781]  eta: 0:01:48  lr: 0.000036  loss: 2.2964 (2.4126)  time: 0.1476  data: 0.0255  max mem: 4873\n",
            "Epoch: [6]  [ 90/781]  eta: 0:01:44  lr: 0.000036  loss: 2.2246 (2.4225)  time: 0.1352  data: 0.0119  max mem: 4873\n",
            "Epoch: [6]  [100/781]  eta: 0:01:41  lr: 0.000036  loss: 2.2028 (2.4199)  time: 0.1276  data: 0.0039  max mem: 4873\n",
            "Epoch: [6]  [110/781]  eta: 0:01:38  lr: 0.000036  loss: 2.2153 (2.4136)  time: 0.1264  data: 0.0038  max mem: 4873\n",
            "Epoch: [6]  [120/781]  eta: 0:01:36  lr: 0.000036  loss: 2.2788 (2.4062)  time: 0.1294  data: 0.0071  max mem: 4873\n",
            "Epoch: [6]  [130/781]  eta: 0:01:34  lr: 0.000036  loss: 2.2693 (2.3943)  time: 0.1329  data: 0.0103  max mem: 4873\n",
            "Epoch: [6]  [140/781]  eta: 0:01:32  lr: 0.000036  loss: 2.2239 (2.3977)  time: 0.1332  data: 0.0110  max mem: 4873\n",
            "Epoch: [6]  [150/781]  eta: 0:01:30  lr: 0.000036  loss: 2.3128 (2.4082)  time: 0.1367  data: 0.0146  max mem: 4873\n",
            "Epoch: [6]  [160/781]  eta: 0:01:28  lr: 0.000036  loss: 2.2830 (2.4084)  time: 0.1336  data: 0.0119  max mem: 4873\n",
            "Epoch: [6]  [170/781]  eta: 0:01:27  lr: 0.000036  loss: 2.2751 (2.3967)  time: 0.1326  data: 0.0097  max mem: 4873\n",
            "Epoch: [6]  [180/781]  eta: 0:01:25  lr: 0.000036  loss: 2.2068 (2.4033)  time: 0.1401  data: 0.0162  max mem: 4873\n",
            "Epoch: [6]  [190/781]  eta: 0:01:24  lr: 0.000036  loss: 2.2591 (2.4022)  time: 0.1389  data: 0.0154  max mem: 4873\n",
            "Epoch: [6]  [200/781]  eta: 0:01:22  lr: 0.000036  loss: 2.2951 (2.4139)  time: 0.1333  data: 0.0105  max mem: 4873\n",
            "Epoch: [6]  [210/781]  eta: 0:01:20  lr: 0.000036  loss: 2.3144 (2.4141)  time: 0.1351  data: 0.0132  max mem: 4873\n",
            "Epoch: [6]  [220/781]  eta: 0:01:18  lr: 0.000036  loss: 2.2623 (2.4206)  time: 0.1336  data: 0.0111  max mem: 4873\n",
            "Epoch: [6]  [230/781]  eta: 0:01:17  lr: 0.000036  loss: 2.1941 (2.4182)  time: 0.1238  data: 0.0012  max mem: 4873\n",
            "Epoch: [6]  [240/781]  eta: 0:01:15  lr: 0.000036  loss: 2.2740 (2.4188)  time: 0.1232  data: 0.0009  max mem: 4873\n",
            "Epoch: [6]  [250/781]  eta: 0:01:13  lr: 0.000036  loss: 2.2041 (2.4135)  time: 0.1276  data: 0.0051  max mem: 4873\n",
            "Epoch: [6]  [260/781]  eta: 0:01:12  lr: 0.000036  loss: 2.2041 (2.4210)  time: 0.1293  data: 0.0069  max mem: 4873\n",
            "Epoch: [6]  [270/781]  eta: 0:01:10  lr: 0.000036  loss: 2.2169 (2.4150)  time: 0.1328  data: 0.0099  max mem: 4873\n",
            "Epoch: [6]  [280/781]  eta: 0:01:09  lr: 0.000036  loss: 2.2038 (2.4074)  time: 0.1389  data: 0.0155  max mem: 4873\n",
            "Epoch: [6]  [290/781]  eta: 0:01:08  lr: 0.000036  loss: 2.2343 (2.4139)  time: 0.1386  data: 0.0157  max mem: 4873\n",
            "Epoch: [6]  [300/781]  eta: 0:01:06  lr: 0.000036  loss: 2.3376 (2.4190)  time: 0.1360  data: 0.0132  max mem: 4873\n",
            "Epoch: [6]  [310/781]  eta: 0:01:04  lr: 0.000036  loss: 2.1875 (2.4162)  time: 0.1286  data: 0.0055  max mem: 4873\n",
            "Epoch: [6]  [320/781]  eta: 0:01:03  lr: 0.000036  loss: 2.1875 (2.4147)  time: 0.1230  data: 0.0003  max mem: 4873\n",
            "Epoch: [6]  [330/781]  eta: 0:01:01  lr: 0.000036  loss: 2.2862 (2.4130)  time: 0.1244  data: 0.0019  max mem: 4873\n",
            "Epoch: [6]  [340/781]  eta: 0:01:00  lr: 0.000036  loss: 2.3172 (2.4126)  time: 0.1256  data: 0.0032  max mem: 4873\n",
            "Epoch: [6]  [350/781]  eta: 0:00:58  lr: 0.000036  loss: 2.3172 (2.4137)  time: 0.1282  data: 0.0055  max mem: 4873\n",
            "Epoch: [6]  [360/781]  eta: 0:00:57  lr: 0.000036  loss: 2.2573 (2.4163)  time: 0.1369  data: 0.0134  max mem: 4873\n",
            "Epoch: [6]  [370/781]  eta: 0:00:56  lr: 0.000036  loss: 2.2389 (2.4149)  time: 0.1383  data: 0.0143  max mem: 4873\n",
            "Epoch: [6]  [380/781]  eta: 0:00:54  lr: 0.000036  loss: 2.2158 (2.4101)  time: 0.1356  data: 0.0126  max mem: 4873\n",
            "Epoch: [6]  [390/781]  eta: 0:00:53  lr: 0.000036  loss: 2.2754 (2.4121)  time: 0.1314  data: 0.0093  max mem: 4873\n",
            "Epoch: [6]  [400/781]  eta: 0:00:51  lr: 0.000036  loss: 2.3086 (2.4134)  time: 0.1239  data: 0.0018  max mem: 4873\n",
            "Epoch: [6]  [410/781]  eta: 0:00:50  lr: 0.000036  loss: 2.2627 (2.4114)  time: 0.1236  data: 0.0003  max mem: 4873\n",
            "Epoch: [6]  [420/781]  eta: 0:00:48  lr: 0.000036  loss: 2.2627 (2.4163)  time: 0.1257  data: 0.0005  max mem: 4873\n",
            "Epoch: [6]  [430/781]  eta: 0:00:47  lr: 0.000036  loss: 2.3362 (2.4211)  time: 0.1287  data: 0.0046  max mem: 4873\n",
            "Epoch: [6]  [440/781]  eta: 0:00:46  lr: 0.000036  loss: 2.3269 (2.4231)  time: 0.1300  data: 0.0073  max mem: 4873\n",
            "Epoch: [6]  [450/781]  eta: 0:00:44  lr: 0.000036  loss: 2.2009 (2.4190)  time: 0.1302  data: 0.0066  max mem: 4873\n",
            "Epoch: [6]  [460/781]  eta: 0:00:43  lr: 0.000036  loss: 2.1587 (2.4157)  time: 0.1321  data: 0.0060  max mem: 4873\n",
            "Epoch: [6]  [470/781]  eta: 0:00:42  lr: 0.000036  loss: 2.2152 (2.4193)  time: 0.1352  data: 0.0089  max mem: 4873\n",
            "Epoch: [6]  [480/781]  eta: 0:00:40  lr: 0.000036  loss: 2.2774 (2.4213)  time: 0.1358  data: 0.0126  max mem: 4873\n",
            "Epoch: [6]  [490/781]  eta: 0:00:39  lr: 0.000036  loss: 2.3324 (2.4233)  time: 0.1305  data: 0.0090  max mem: 4873\n",
            "Epoch: [6]  [500/781]  eta: 0:00:37  lr: 0.000036  loss: 2.2040 (2.4199)  time: 0.1272  data: 0.0058  max mem: 4873\n",
            "Epoch: [6]  [510/781]  eta: 0:00:36  lr: 0.000036  loss: 2.1248 (2.4167)  time: 0.1264  data: 0.0047  max mem: 4873\n",
            "Epoch: [6]  [520/781]  eta: 0:00:35  lr: 0.000036  loss: 2.2368 (2.4227)  time: 0.1264  data: 0.0043  max mem: 4873\n",
            "Epoch: [6]  [530/781]  eta: 0:00:33  lr: 0.000036  loss: 2.2483 (2.4225)  time: 0.1287  data: 0.0054  max mem: 4873\n",
            "Epoch: [6]  [540/781]  eta: 0:00:32  lr: 0.000036  loss: 2.2100 (2.4199)  time: 0.1312  data: 0.0078  max mem: 4873\n",
            "Epoch: [6]  [550/781]  eta: 0:00:31  lr: 0.000036  loss: 2.2208 (2.4165)  time: 0.1347  data: 0.0108  max mem: 4873\n",
            "Epoch: [6]  [560/781]  eta: 0:00:29  lr: 0.000036  loss: 2.2574 (2.4140)  time: 0.1390  data: 0.0146  max mem: 4873\n",
            "Epoch: [6]  [570/781]  eta: 0:00:28  lr: 0.000036  loss: 2.2617 (2.4146)  time: 0.1311  data: 0.0088  max mem: 4873\n",
            "Epoch: [6]  [580/781]  eta: 0:00:26  lr: 0.000036  loss: 2.2274 (2.4157)  time: 0.1215  data: 0.0006  max mem: 4873\n",
            "Epoch: [6]  [590/781]  eta: 0:00:25  lr: 0.000036  loss: 2.2555 (2.4227)  time: 0.1270  data: 0.0060  max mem: 4873\n",
            "Epoch: [6]  [600/781]  eta: 0:00:24  lr: 0.000036  loss: 2.2381 (2.4197)  time: 0.1336  data: 0.0126  max mem: 4873\n",
            "Epoch: [6]  [610/781]  eta: 0:00:22  lr: 0.000036  loss: 2.2381 (2.4243)  time: 0.1313  data: 0.0104  max mem: 4873\n",
            "Epoch: [6]  [620/781]  eta: 0:00:21  lr: 0.000036  loss: 2.1494 (2.4200)  time: 0.1335  data: 0.0125  max mem: 4873\n",
            "Epoch: [6]  [630/781]  eta: 0:00:20  lr: 0.000036  loss: 2.1961 (2.4212)  time: 0.1319  data: 0.0109  max mem: 4873\n",
            "Epoch: [6]  [640/781]  eta: 0:00:18  lr: 0.000036  loss: 2.1929 (2.4178)  time: 0.1321  data: 0.0110  max mem: 4873\n",
            "Epoch: [6]  [650/781]  eta: 0:00:17  lr: 0.000036  loss: 2.1736 (2.4204)  time: 0.1381  data: 0.0164  max mem: 4873\n",
            "Epoch: [6]  [660/781]  eta: 0:00:16  lr: 0.000036  loss: 2.1038 (2.4154)  time: 0.1340  data: 0.0113  max mem: 4873\n",
            "Epoch: [6]  [670/781]  eta: 0:00:14  lr: 0.000036  loss: 2.1268 (2.4147)  time: 0.1302  data: 0.0075  max mem: 4873\n",
            "Epoch: [6]  [680/781]  eta: 0:00:13  lr: 0.000036  loss: 2.2207 (2.4135)  time: 0.1268  data: 0.0042  max mem: 4873\n",
            "Epoch: [6]  [690/781]  eta: 0:00:12  lr: 0.000036  loss: 2.2297 (2.4159)  time: 0.1261  data: 0.0039  max mem: 4873\n",
            "Epoch: [6]  [700/781]  eta: 0:00:10  lr: 0.000036  loss: 2.2516 (2.4158)  time: 0.1262  data: 0.0042  max mem: 4873\n",
            "Epoch: [6]  [710/781]  eta: 0:00:09  lr: 0.000036  loss: 2.2211 (2.4147)  time: 0.1270  data: 0.0045  max mem: 4873\n",
            "Epoch: [6]  [720/781]  eta: 0:00:08  lr: 0.000036  loss: 2.2211 (2.4121)  time: 0.1281  data: 0.0063  max mem: 4873\n",
            "Epoch: [6]  [730/781]  eta: 0:00:06  lr: 0.000036  loss: 2.1457 (2.4116)  time: 0.1278  data: 0.0050  max mem: 4873\n",
            "Epoch: [6]  [740/781]  eta: 0:00:05  lr: 0.000036  loss: 2.2712 (2.4119)  time: 0.1282  data: 0.0028  max mem: 4873\n",
            "Epoch: [6]  [750/781]  eta: 0:00:04  lr: 0.000036  loss: 2.3218 (2.4130)  time: 0.1303  data: 0.0050  max mem: 4873\n",
            "Epoch: [6]  [760/781]  eta: 0:00:02  lr: 0.000036  loss: 2.2136 (2.4112)  time: 0.1310  data: 0.0073  max mem: 4873\n",
            "Epoch: [6]  [770/781]  eta: 0:00:01  lr: 0.000036  loss: 2.1743 (2.4086)  time: 0.1277  data: 0.0047  max mem: 4873\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000036  loss: 2.1668 (2.4064)  time: 0.1262  data: 0.0040  max mem: 4873\n",
            "Epoch: [6] Total time: 0:01:44 (0.1332 s / it)\n",
            "Averaged stats: lr: 0.000036  loss: 2.1668 (2.4064)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.7784 (0.7784)  acc1: 81.7708 (81.7708)  acc5: 94.7917 (94.7917)  time: 0.8410  data: 0.8102  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0736 (1.0021)  acc1: 75.0000 (76.0417)  acc5: 92.7083 (93.0871)  time: 0.1742  data: 0.1436  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1605 (1.0803)  acc1: 70.3125 (74.7768)  acc5: 92.1875 (91.8899)  time: 0.1237  data: 0.0931  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2254 (1.1270)  acc1: 70.3125 (74.0759)  acc5: 90.1042 (91.3643)  time: 0.1238  data: 0.0931  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2648 (1.1773)  acc1: 72.3958 (73.1326)  acc5: 89.5833 (90.8410)  time: 0.1278  data: 0.0971  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2354 (1.1740)  acc1: 72.3958 (72.8452)  acc5: 90.1042 (91.0641)  time: 0.1292  data: 0.0985  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2648 (1.1801)  acc1: 68.7500 (72.7500)  acc5: 90.6250 (91.0900)  time: 0.1098  data: 0.0801  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1333 s / it)\n",
            "* Acc@1 72.750 Acc@5 91.090 loss 1.180\n",
            "Accuracy of the network on the 10000 test images: 72.8%\n",
            "Max accuracy: 72.75%\n",
            "Epoch: [7]  [  0/781]  eta: 0:12:05  lr: 0.000028  loss: 2.0608 (2.0608)  time: 0.9293  data: 0.7900  max mem: 4873\n",
            "Epoch: [7]  [ 10/781]  eta: 0:02:40  lr: 0.000028  loss: 2.1725 (2.3092)  time: 0.2078  data: 0.0850  max mem: 4873\n",
            "Epoch: [7]  [ 20/781]  eta: 0:02:12  lr: 0.000028  loss: 2.1981 (2.3557)  time: 0.1368  data: 0.0156  max mem: 4873\n",
            "Epoch: [7]  [ 30/781]  eta: 0:02:00  lr: 0.000028  loss: 2.2776 (2.3890)  time: 0.1338  data: 0.0124  max mem: 4873\n",
            "Epoch: [7]  [ 40/781]  eta: 0:01:55  lr: 0.000028  loss: 2.2805 (2.3872)  time: 0.1352  data: 0.0136  max mem: 4873\n",
            "Epoch: [7]  [ 50/781]  eta: 0:01:50  lr: 0.000028  loss: 2.2665 (2.4221)  time: 0.1361  data: 0.0142  max mem: 4873\n",
            "Epoch: [7]  [ 60/781]  eta: 0:01:47  lr: 0.000028  loss: 2.2549 (2.4306)  time: 0.1368  data: 0.0152  max mem: 4873\n",
            "Epoch: [7]  [ 70/781]  eta: 0:01:44  lr: 0.000028  loss: 2.1879 (2.4082)  time: 0.1381  data: 0.0165  max mem: 4873\n",
            "Epoch: [7]  [ 80/781]  eta: 0:01:43  lr: 0.000028  loss: 2.1661 (2.4292)  time: 0.1427  data: 0.0200  max mem: 4873\n",
            "Epoch: [7]  [ 90/781]  eta: 0:01:41  lr: 0.000028  loss: 2.2621 (2.4436)  time: 0.1465  data: 0.0221  max mem: 4873\n",
            "Epoch: [7]  [100/781]  eta: 0:01:39  lr: 0.000028  loss: 2.1516 (2.4352)  time: 0.1428  data: 0.0198  max mem: 4873\n",
            "Epoch: [7]  [110/781]  eta: 0:01:38  lr: 0.000028  loss: 2.1995 (2.4393)  time: 0.1419  data: 0.0205  max mem: 4873\n",
            "Epoch: [7]  [120/781]  eta: 0:01:36  lr: 0.000028  loss: 2.1995 (2.4239)  time: 0.1420  data: 0.0206  max mem: 4873\n",
            "Epoch: [7]  [130/781]  eta: 0:01:34  lr: 0.000028  loss: 2.1240 (2.4100)  time: 0.1388  data: 0.0176  max mem: 4873\n",
            "Epoch: [7]  [140/781]  eta: 0:01:33  lr: 0.000028  loss: 2.1336 (2.3950)  time: 0.1407  data: 0.0186  max mem: 4873\n",
            "Epoch: [7]  [150/781]  eta: 0:01:31  lr: 0.000028  loss: 2.1381 (2.3913)  time: 0.1420  data: 0.0201  max mem: 4873\n",
            "Epoch: [7]  [160/781]  eta: 0:01:29  lr: 0.000028  loss: 2.1652 (2.4008)  time: 0.1400  data: 0.0187  max mem: 4873\n",
            "Epoch: [7]  [170/781]  eta: 0:01:27  lr: 0.000028  loss: 2.1797 (2.4102)  time: 0.1398  data: 0.0173  max mem: 4873\n",
            "Epoch: [7]  [180/781]  eta: 0:01:26  lr: 0.000028  loss: 2.2169 (2.4140)  time: 0.1422  data: 0.0169  max mem: 4873\n",
            "Epoch: [7]  [190/781]  eta: 0:01:25  lr: 0.000028  loss: 2.1687 (2.4050)  time: 0.1435  data: 0.0189  max mem: 4873\n",
            "Epoch: [7]  [200/781]  eta: 0:01:23  lr: 0.000028  loss: 2.1426 (2.3996)  time: 0.1410  data: 0.0186  max mem: 4873\n",
            "Epoch: [7]  [210/781]  eta: 0:01:21  lr: 0.000028  loss: 2.1223 (2.3864)  time: 0.1371  data: 0.0139  max mem: 4873\n",
            "Epoch: [7]  [220/781]  eta: 0:01:20  lr: 0.000028  loss: 2.1127 (2.3747)  time: 0.1331  data: 0.0103  max mem: 4873\n",
            "Epoch: [7]  [230/781]  eta: 0:01:19  lr: 0.000028  loss: 2.1381 (2.3730)  time: 0.1445  data: 0.0226  max mem: 4873\n",
            "Epoch: [7]  [240/781]  eta: 0:01:17  lr: 0.000028  loss: 2.1455 (2.3645)  time: 0.1483  data: 0.0266  max mem: 4873\n",
            "Epoch: [7]  [250/781]  eta: 0:01:16  lr: 0.000028  loss: 2.1489 (2.3582)  time: 0.1452  data: 0.0235  max mem: 4873\n",
            "Epoch: [7]  [260/781]  eta: 0:01:14  lr: 0.000028  loss: 2.1639 (2.3513)  time: 0.1400  data: 0.0177  max mem: 4873\n",
            "Epoch: [7]  [270/781]  eta: 0:01:13  lr: 0.000028  loss: 2.1909 (2.3522)  time: 0.1399  data: 0.0166  max mem: 4873\n",
            "Epoch: [7]  [280/781]  eta: 0:01:11  lr: 0.000028  loss: 2.2045 (2.3599)  time: 0.1419  data: 0.0189  max mem: 4873\n",
            "Epoch: [7]  [290/781]  eta: 0:01:10  lr: 0.000028  loss: 2.2155 (2.3569)  time: 0.1389  data: 0.0172  max mem: 4873\n",
            "Epoch: [7]  [300/781]  eta: 0:01:08  lr: 0.000028  loss: 2.1486 (2.3544)  time: 0.1389  data: 0.0175  max mem: 4873\n",
            "Epoch: [7]  [310/781]  eta: 0:01:07  lr: 0.000028  loss: 2.1464 (2.3523)  time: 0.1434  data: 0.0224  max mem: 4873\n",
            "Epoch: [7]  [320/781]  eta: 0:01:05  lr: 0.000028  loss: 2.1967 (2.3485)  time: 0.1481  data: 0.0271  max mem: 4873\n",
            "Epoch: [7]  [330/781]  eta: 0:01:04  lr: 0.000028  loss: 2.2075 (2.3489)  time: 0.1480  data: 0.0268  max mem: 4873\n",
            "Epoch: [7]  [340/781]  eta: 0:01:03  lr: 0.000028  loss: 2.1559 (2.3448)  time: 0.1434  data: 0.0221  max mem: 4873\n",
            "Epoch: [7]  [350/781]  eta: 0:01:01  lr: 0.000028  loss: 2.1253 (2.3376)  time: 0.1492  data: 0.0267  max mem: 4873\n",
            "Epoch: [7]  [360/781]  eta: 0:01:00  lr: 0.000028  loss: 2.1253 (2.3323)  time: 0.1504  data: 0.0275  max mem: 4873\n",
            "Epoch: [7]  [370/781]  eta: 0:00:59  lr: 0.000028  loss: 2.1782 (2.3292)  time: 0.1428  data: 0.0204  max mem: 4873\n",
            "Epoch: [7]  [380/781]  eta: 0:00:57  lr: 0.000028  loss: 2.1966 (2.3308)  time: 0.1418  data: 0.0193  max mem: 4873\n",
            "Epoch: [7]  [390/781]  eta: 0:00:56  lr: 0.000028  loss: 2.1859 (2.3271)  time: 0.1376  data: 0.0154  max mem: 4873\n",
            "Epoch: [7]  [400/781]  eta: 0:00:54  lr: 0.000028  loss: 2.2142 (2.3283)  time: 0.1361  data: 0.0136  max mem: 4873\n",
            "Epoch: [7]  [410/781]  eta: 0:00:53  lr: 0.000028  loss: 2.2131 (2.3309)  time: 0.1393  data: 0.0166  max mem: 4873\n",
            "Epoch: [7]  [420/781]  eta: 0:00:51  lr: 0.000028  loss: 2.1852 (2.3305)  time: 0.1424  data: 0.0205  max mem: 4873\n",
            "Epoch: [7]  [430/781]  eta: 0:00:50  lr: 0.000028  loss: 2.1852 (2.3304)  time: 0.1432  data: 0.0210  max mem: 4873\n",
            "Epoch: [7]  [440/781]  eta: 0:00:48  lr: 0.000028  loss: 2.1887 (2.3349)  time: 0.1390  data: 0.0159  max mem: 4873\n",
            "Epoch: [7]  [450/781]  eta: 0:00:47  lr: 0.000028  loss: 2.2362 (2.3373)  time: 0.1383  data: 0.0162  max mem: 4873\n",
            "Epoch: [7]  [460/781]  eta: 0:00:45  lr: 0.000028  loss: 2.1890 (2.3390)  time: 0.1379  data: 0.0169  max mem: 4873\n",
            "Epoch: [7]  [470/781]  eta: 0:00:44  lr: 0.000028  loss: 2.2260 (2.3430)  time: 0.1382  data: 0.0173  max mem: 4873\n",
            "Epoch: [7]  [480/781]  eta: 0:00:42  lr: 0.000028  loss: 2.1804 (2.3394)  time: 0.1394  data: 0.0181  max mem: 4873\n",
            "Epoch: [7]  [490/781]  eta: 0:00:41  lr: 0.000028  loss: 2.2363 (2.3418)  time: 0.1449  data: 0.0235  max mem: 4873\n",
            "Epoch: [7]  [500/781]  eta: 0:00:40  lr: 0.000028  loss: 2.3366 (2.3448)  time: 0.1417  data: 0.0200  max mem: 4873\n",
            "Epoch: [7]  [510/781]  eta: 0:00:38  lr: 0.000028  loss: 2.3150 (2.3441)  time: 0.1330  data: 0.0107  max mem: 4873\n",
            "Epoch: [7]  [520/781]  eta: 0:00:37  lr: 0.000028  loss: 2.0902 (2.3395)  time: 0.1381  data: 0.0151  max mem: 4873\n",
            "Epoch: [7]  [530/781]  eta: 0:00:35  lr: 0.000028  loss: 2.1503 (2.3425)  time: 0.1380  data: 0.0149  max mem: 4873\n",
            "Epoch: [7]  [540/781]  eta: 0:00:34  lr: 0.000028  loss: 2.2701 (2.3440)  time: 0.1358  data: 0.0139  max mem: 4873\n",
            "Epoch: [7]  [550/781]  eta: 0:00:32  lr: 0.000028  loss: 2.1483 (2.3398)  time: 0.1402  data: 0.0193  max mem: 4873\n",
            "Epoch: [7]  [560/781]  eta: 0:00:31  lr: 0.000028  loss: 2.1483 (2.3421)  time: 0.1408  data: 0.0200  max mem: 4873\n",
            "Epoch: [7]  [570/781]  eta: 0:00:29  lr: 0.000028  loss: 2.2091 (2.3422)  time: 0.1352  data: 0.0142  max mem: 4873\n",
            "Epoch: [7]  [580/781]  eta: 0:00:28  lr: 0.000028  loss: 2.2118 (2.3463)  time: 0.1344  data: 0.0122  max mem: 4873\n",
            "Epoch: [7]  [590/781]  eta: 0:00:27  lr: 0.000028  loss: 2.2127 (2.3479)  time: 0.1382  data: 0.0156  max mem: 4873\n",
            "Epoch: [7]  [600/781]  eta: 0:00:25  lr: 0.000028  loss: 2.1679 (2.3459)  time: 0.1395  data: 0.0180  max mem: 4873\n",
            "Epoch: [7]  [610/781]  eta: 0:00:24  lr: 0.000028  loss: 2.1502 (2.3476)  time: 0.1403  data: 0.0180  max mem: 4873\n",
            "Epoch: [7]  [620/781]  eta: 0:00:22  lr: 0.000028  loss: 2.2019 (2.3458)  time: 0.1423  data: 0.0192  max mem: 4873\n",
            "Epoch: [7]  [630/781]  eta: 0:00:21  lr: 0.000028  loss: 2.1529 (2.3445)  time: 0.1451  data: 0.0227  max mem: 4873\n",
            "Epoch: [7]  [640/781]  eta: 0:00:19  lr: 0.000028  loss: 2.0949 (2.3431)  time: 0.1442  data: 0.0219  max mem: 4873\n",
            "Epoch: [7]  [650/781]  eta: 0:00:18  lr: 0.000028  loss: 2.1450 (2.3405)  time: 0.1391  data: 0.0171  max mem: 4873\n",
            "Epoch: [7]  [660/781]  eta: 0:00:17  lr: 0.000028  loss: 2.1698 (2.3394)  time: 0.1374  data: 0.0155  max mem: 4873\n",
            "Epoch: [7]  [670/781]  eta: 0:00:15  lr: 0.000028  loss: 2.1739 (2.3389)  time: 0.1396  data: 0.0180  max mem: 4873\n",
            "Epoch: [7]  [680/781]  eta: 0:00:14  lr: 0.000028  loss: 2.1918 (2.3388)  time: 0.1388  data: 0.0176  max mem: 4873\n",
            "Epoch: [7]  [690/781]  eta: 0:00:12  lr: 0.000028  loss: 2.1804 (2.3359)  time: 0.1373  data: 0.0159  max mem: 4873\n",
            "Epoch: [7]  [700/781]  eta: 0:00:11  lr: 0.000028  loss: 2.1758 (2.3339)  time: 0.1358  data: 0.0125  max mem: 4873\n",
            "Epoch: [7]  [710/781]  eta: 0:00:10  lr: 0.000028  loss: 2.1984 (2.3366)  time: 0.1363  data: 0.0111  max mem: 4873\n",
            "Epoch: [7]  [720/781]  eta: 0:00:08  lr: 0.000028  loss: 2.1445 (2.3362)  time: 0.1378  data: 0.0135  max mem: 4873\n",
            "Epoch: [7]  [730/781]  eta: 0:00:07  lr: 0.000028  loss: 2.1182 (2.3339)  time: 0.1378  data: 0.0153  max mem: 4873\n",
            "Epoch: [7]  [740/781]  eta: 0:00:05  lr: 0.000028  loss: 2.1636 (2.3321)  time: 0.1380  data: 0.0162  max mem: 4873\n",
            "Epoch: [7]  [750/781]  eta: 0:00:04  lr: 0.000028  loss: 2.1636 (2.3319)  time: 0.1366  data: 0.0151  max mem: 4873\n",
            "Epoch: [7]  [760/781]  eta: 0:00:02  lr: 0.000028  loss: 2.1587 (2.3310)  time: 0.1382  data: 0.0170  max mem: 4873\n",
            "Epoch: [7]  [770/781]  eta: 0:00:01  lr: 0.000028  loss: 2.1859 (2.3307)  time: 0.1371  data: 0.0153  max mem: 4873\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000028  loss: 2.1632 (2.3300)  time: 0.1339  data: 0.0125  max mem: 4873\n",
            "Epoch: [7] Total time: 0:01:50 (0.1411 s / it)\n",
            "Averaged stats: lr: 0.000028  loss: 2.1632 (2.3300)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.9310 (0.9310)  acc1: 77.6042 (77.6042)  acc5: 92.7083 (92.7083)  time: 0.8482  data: 0.8173  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1811 (1.0522)  acc1: 72.9167 (75.7102)  acc5: 92.7083 (92.2822)  time: 0.1805  data: 0.1488  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1811 (1.0899)  acc1: 72.9167 (75.0992)  acc5: 92.1875 (91.8403)  time: 0.1281  data: 0.0967  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1850 (1.1368)  acc1: 72.9167 (74.2440)  acc5: 91.1458 (91.3306)  time: 0.1253  data: 0.0944  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2194 (1.1742)  acc1: 72.9167 (73.2470)  acc5: 90.1042 (90.8537)  time: 0.1258  data: 0.0951  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1367 (1.1647)  acc1: 72.9167 (73.3967)  acc5: 90.6250 (90.9722)  time: 0.1242  data: 0.0936  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1685 (1.1673)  acc1: 72.9167 (73.2900)  acc5: 91.1458 (91.0100)  time: 0.1047  data: 0.0750  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1335 s / it)\n",
            "* Acc@1 73.290 Acc@5 91.010 loss 1.167\n",
            "Accuracy of the network on the 10000 test images: 73.3%\n",
            "Max accuracy: 73.29%\n",
            "Epoch: [8]  [  0/781]  eta: 0:15:48  lr: 0.000021  loss: 1.9869 (1.9869)  time: 1.2143  data: 0.9089  max mem: 4873\n",
            "Epoch: [8]  [ 10/781]  eta: 0:02:50  lr: 0.000021  loss: 2.2659 (2.4632)  time: 0.2208  data: 0.0829  max mem: 4873\n",
            "Epoch: [8]  [ 20/781]  eta: 0:02:15  lr: 0.000021  loss: 2.2279 (2.4304)  time: 0.1266  data: 0.0042  max mem: 4873\n",
            "Epoch: [8]  [ 30/781]  eta: 0:02:02  lr: 0.000021  loss: 2.1706 (2.3537)  time: 0.1317  data: 0.0086  max mem: 4873\n",
            "Epoch: [8]  [ 40/781]  eta: 0:01:57  lr: 0.000021  loss: 2.0635 (2.3268)  time: 0.1377  data: 0.0135  max mem: 4873\n",
            "Epoch: [8]  [ 50/781]  eta: 0:01:55  lr: 0.000021  loss: 2.0813 (2.3225)  time: 0.1482  data: 0.0238  max mem: 4873\n",
            "Epoch: [8]  [ 60/781]  eta: 0:01:50  lr: 0.000021  loss: 2.2004 (2.3760)  time: 0.1444  data: 0.0222  max mem: 4873\n",
            "Epoch: [8]  [ 70/781]  eta: 0:01:47  lr: 0.000021  loss: 2.2399 (2.3998)  time: 0.1334  data: 0.0121  max mem: 4873\n",
            "Epoch: [8]  [ 80/781]  eta: 0:01:44  lr: 0.000021  loss: 2.2281 (2.4176)  time: 0.1326  data: 0.0112  max mem: 4873\n",
            "Epoch: [8]  [ 90/781]  eta: 0:01:41  lr: 0.000021  loss: 2.2375 (2.4147)  time: 0.1366  data: 0.0153  max mem: 4873\n",
            "Epoch: [8]  [100/781]  eta: 0:01:39  lr: 0.000021  loss: 2.2279 (2.3956)  time: 0.1366  data: 0.0156  max mem: 4873\n",
            "Epoch: [8]  [110/781]  eta: 0:01:37  lr: 0.000021  loss: 2.0805 (2.3764)  time: 0.1358  data: 0.0145  max mem: 4873\n",
            "Epoch: [8]  [120/781]  eta: 0:01:35  lr: 0.000021  loss: 2.0789 (2.3639)  time: 0.1366  data: 0.0143  max mem: 4873\n",
            "Epoch: [8]  [130/781]  eta: 0:01:34  lr: 0.000021  loss: 2.1561 (2.3487)  time: 0.1397  data: 0.0164  max mem: 4873\n",
            "Epoch: [8]  [140/781]  eta: 0:01:32  lr: 0.000021  loss: 2.1609 (2.3470)  time: 0.1439  data: 0.0202  max mem: 4873\n",
            "Epoch: [8]  [150/781]  eta: 0:01:31  lr: 0.000021  loss: 2.1288 (2.3507)  time: 0.1460  data: 0.0230  max mem: 4873\n",
            "Epoch: [8]  [160/781]  eta: 0:01:29  lr: 0.000021  loss: 2.1183 (2.3408)  time: 0.1418  data: 0.0195  max mem: 4873\n",
            "Epoch: [8]  [170/781]  eta: 0:01:27  lr: 0.000021  loss: 2.1345 (2.3428)  time: 0.1374  data: 0.0149  max mem: 4873\n",
            "Epoch: [8]  [180/781]  eta: 0:01:26  lr: 0.000021  loss: 2.1336 (2.3282)  time: 0.1348  data: 0.0126  max mem: 4873\n",
            "Epoch: [8]  [190/781]  eta: 0:01:24  lr: 0.000021  loss: 2.1217 (2.3285)  time: 0.1337  data: 0.0120  max mem: 4873\n",
            "Epoch: [8]  [200/781]  eta: 0:01:22  lr: 0.000021  loss: 2.1312 (2.3261)  time: 0.1364  data: 0.0148  max mem: 4873\n",
            "Epoch: [8]  [210/781]  eta: 0:01:21  lr: 0.000021  loss: 2.0974 (2.3205)  time: 0.1333  data: 0.0108  max mem: 4873\n",
            "Epoch: [8]  [220/781]  eta: 0:01:19  lr: 0.000021  loss: 2.0905 (2.3108)  time: 0.1332  data: 0.0095  max mem: 4873\n",
            "Epoch: [8]  [230/781]  eta: 0:01:17  lr: 0.000021  loss: 2.0930 (2.3090)  time: 0.1377  data: 0.0149  max mem: 4873\n",
            "Epoch: [8]  [240/781]  eta: 0:01:16  lr: 0.000021  loss: 2.0957 (2.3053)  time: 0.1368  data: 0.0154  max mem: 4873\n",
            "Epoch: [8]  [250/781]  eta: 0:01:14  lr: 0.000021  loss: 2.0957 (2.3079)  time: 0.1334  data: 0.0120  max mem: 4873\n",
            "Epoch: [8]  [260/781]  eta: 0:01:13  lr: 0.000021  loss: 2.1081 (2.3112)  time: 0.1303  data: 0.0081  max mem: 4873\n",
            "Epoch: [8]  [270/781]  eta: 0:01:11  lr: 0.000021  loss: 2.0909 (2.3102)  time: 0.1290  data: 0.0064  max mem: 4873\n",
            "Epoch: [8]  [280/781]  eta: 0:01:10  lr: 0.000021  loss: 2.1836 (2.3218)  time: 0.1317  data: 0.0094  max mem: 4873\n",
            "Epoch: [8]  [290/781]  eta: 0:01:08  lr: 0.000021  loss: 2.1864 (2.3151)  time: 0.1315  data: 0.0095  max mem: 4873\n",
            "Epoch: [8]  [300/781]  eta: 0:01:06  lr: 0.000021  loss: 2.1305 (2.3145)  time: 0.1290  data: 0.0069  max mem: 4873\n",
            "Epoch: [8]  [310/781]  eta: 0:01:05  lr: 0.000021  loss: 2.1764 (2.3218)  time: 0.1353  data: 0.0123  max mem: 4873\n",
            "Epoch: [8]  [320/781]  eta: 0:01:04  lr: 0.000021  loss: 2.1575 (2.3178)  time: 0.1415  data: 0.0180  max mem: 4873\n",
            "Epoch: [8]  [330/781]  eta: 0:01:02  lr: 0.000021  loss: 2.0987 (2.3142)  time: 0.1347  data: 0.0118  max mem: 4873\n",
            "Epoch: [8]  [340/781]  eta: 0:01:01  lr: 0.000021  loss: 2.0944 (2.3132)  time: 0.1278  data: 0.0054  max mem: 4873\n",
            "Epoch: [8]  [350/781]  eta: 0:00:59  lr: 0.000021  loss: 2.0675 (2.3073)  time: 0.1304  data: 0.0085  max mem: 4873\n",
            "Epoch: [8]  [360/781]  eta: 0:00:58  lr: 0.000021  loss: 2.1143 (2.3074)  time: 0.1372  data: 0.0150  max mem: 4873\n",
            "Epoch: [8]  [370/781]  eta: 0:00:56  lr: 0.000021  loss: 2.1350 (2.3027)  time: 0.1355  data: 0.0124  max mem: 4873\n",
            "Epoch: [8]  [380/781]  eta: 0:00:55  lr: 0.000021  loss: 2.0903 (2.3032)  time: 0.1334  data: 0.0103  max mem: 4873\n",
            "Epoch: [8]  [390/781]  eta: 0:00:53  lr: 0.000021  loss: 2.2602 (2.3183)  time: 0.1349  data: 0.0116  max mem: 4873\n",
            "Epoch: [8]  [400/781]  eta: 0:00:52  lr: 0.000021  loss: 2.1847 (2.3139)  time: 0.1334  data: 0.0104  max mem: 4873\n",
            "Epoch: [8]  [410/781]  eta: 0:00:51  lr: 0.000021  loss: 2.1847 (2.3198)  time: 0.1419  data: 0.0187  max mem: 4873\n",
            "Epoch: [8]  [420/781]  eta: 0:00:49  lr: 0.000021  loss: 2.1797 (2.3167)  time: 0.1412  data: 0.0182  max mem: 4873\n",
            "Epoch: [8]  [430/781]  eta: 0:00:48  lr: 0.000021  loss: 2.0955 (2.3131)  time: 0.1333  data: 0.0114  max mem: 4873\n",
            "Epoch: [8]  [440/781]  eta: 0:00:47  lr: 0.000021  loss: 2.1883 (2.3111)  time: 0.1316  data: 0.0096  max mem: 4873\n",
            "Epoch: [8]  [450/781]  eta: 0:00:45  lr: 0.000021  loss: 2.1224 (2.3099)  time: 0.1383  data: 0.0163  max mem: 4873\n",
            "Epoch: [8]  [460/781]  eta: 0:00:44  lr: 0.000021  loss: 2.1267 (2.3064)  time: 0.1398  data: 0.0173  max mem: 4873\n",
            "Epoch: [8]  [470/781]  eta: 0:00:42  lr: 0.000021  loss: 2.1316 (2.3108)  time: 0.1295  data: 0.0063  max mem: 4873\n",
            "Epoch: [8]  [480/781]  eta: 0:00:41  lr: 0.000021  loss: 2.1365 (2.3172)  time: 0.1271  data: 0.0047  max mem: 4873\n",
            "Epoch: [8]  [490/781]  eta: 0:00:39  lr: 0.000021  loss: 2.1155 (2.3156)  time: 0.1285  data: 0.0045  max mem: 4873\n",
            "Epoch: [8]  [500/781]  eta: 0:00:38  lr: 0.000021  loss: 2.0863 (2.3161)  time: 0.1322  data: 0.0077  max mem: 4873\n",
            "Epoch: [8]  [510/781]  eta: 0:00:37  lr: 0.000021  loss: 2.1446 (2.3174)  time: 0.1365  data: 0.0147  max mem: 4873\n",
            "Epoch: [8]  [520/781]  eta: 0:00:35  lr: 0.000021  loss: 2.1594 (2.3176)  time: 0.1344  data: 0.0126  max mem: 4873\n",
            "Epoch: [8]  [530/781]  eta: 0:00:34  lr: 0.000021  loss: 2.1245 (2.3170)  time: 0.1321  data: 0.0095  max mem: 4873\n",
            "Epoch: [8]  [540/781]  eta: 0:00:32  lr: 0.000021  loss: 2.1490 (2.3149)  time: 0.1295  data: 0.0074  max mem: 4873\n",
            "Epoch: [8]  [550/781]  eta: 0:00:31  lr: 0.000021  loss: 2.1358 (2.3121)  time: 0.1277  data: 0.0062  max mem: 4873\n",
            "Epoch: [8]  [560/781]  eta: 0:00:30  lr: 0.000021  loss: 2.1263 (2.3156)  time: 0.1304  data: 0.0090  max mem: 4873\n",
            "Epoch: [8]  [570/781]  eta: 0:00:28  lr: 0.000021  loss: 2.1658 (2.3162)  time: 0.1322  data: 0.0109  max mem: 4873\n",
            "Epoch: [8]  [580/781]  eta: 0:00:27  lr: 0.000021  loss: 2.0696 (2.3131)  time: 0.1316  data: 0.0097  max mem: 4873\n",
            "Epoch: [8]  [590/781]  eta: 0:00:26  lr: 0.000021  loss: 2.1484 (2.3192)  time: 0.1332  data: 0.0102  max mem: 4873\n",
            "Epoch: [8]  [600/781]  eta: 0:00:24  lr: 0.000021  loss: 2.2851 (2.3172)  time: 0.1403  data: 0.0170  max mem: 4873\n",
            "Epoch: [8]  [610/781]  eta: 0:00:23  lr: 0.000021  loss: 2.1631 (2.3147)  time: 0.1455  data: 0.0230  max mem: 4873\n",
            "Epoch: [8]  [620/781]  eta: 0:00:21  lr: 0.000021  loss: 2.1893 (2.3181)  time: 0.1377  data: 0.0155  max mem: 4873\n",
            "Epoch: [8]  [630/781]  eta: 0:00:20  lr: 0.000021  loss: 2.1803 (2.3155)  time: 0.1301  data: 0.0078  max mem: 4873\n",
            "Epoch: [8]  [640/781]  eta: 0:00:19  lr: 0.000021  loss: 2.1122 (2.3159)  time: 0.1313  data: 0.0095  max mem: 4873\n",
            "Epoch: [8]  [650/781]  eta: 0:00:17  lr: 0.000021  loss: 2.1110 (2.3152)  time: 0.1328  data: 0.0111  max mem: 4873\n",
            "Epoch: [8]  [660/781]  eta: 0:00:16  lr: 0.000021  loss: 2.1959 (2.3148)  time: 0.1323  data: 0.0106  max mem: 4873\n",
            "Epoch: [8]  [670/781]  eta: 0:00:15  lr: 0.000021  loss: 2.1453 (2.3151)  time: 0.1314  data: 0.0094  max mem: 4873\n",
            "Epoch: [8]  [680/781]  eta: 0:00:13  lr: 0.000021  loss: 2.1453 (2.3196)  time: 0.1336  data: 0.0114  max mem: 4873\n",
            "Epoch: [8]  [690/781]  eta: 0:00:12  lr: 0.000021  loss: 2.1954 (2.3173)  time: 0.1367  data: 0.0134  max mem: 4873\n",
            "Epoch: [8]  [700/781]  eta: 0:00:11  lr: 0.000021  loss: 2.1530 (2.3179)  time: 0.1368  data: 0.0128  max mem: 4873\n",
            "Epoch: [8]  [710/781]  eta: 0:00:09  lr: 0.000021  loss: 2.1535 (2.3174)  time: 0.1307  data: 0.0076  max mem: 4873\n",
            "Epoch: [8]  [720/781]  eta: 0:00:08  lr: 0.000021  loss: 2.1047 (2.3148)  time: 0.1299  data: 0.0073  max mem: 4873\n",
            "Epoch: [8]  [730/781]  eta: 0:00:06  lr: 0.000021  loss: 2.1069 (2.3152)  time: 0.1339  data: 0.0113  max mem: 4873\n",
            "Epoch: [8]  [740/781]  eta: 0:00:05  lr: 0.000021  loss: 2.1708 (2.3152)  time: 0.1355  data: 0.0130  max mem: 4873\n",
            "Epoch: [8]  [750/781]  eta: 0:00:04  lr: 0.000021  loss: 2.1899 (2.3189)  time: 0.1366  data: 0.0140  max mem: 4873\n",
            "Epoch: [8]  [760/781]  eta: 0:00:02  lr: 0.000021  loss: 2.1491 (2.3170)  time: 0.1367  data: 0.0146  max mem: 4873\n",
            "Epoch: [8]  [770/781]  eta: 0:00:01  lr: 0.000021  loss: 2.1011 (2.3194)  time: 0.1356  data: 0.0139  max mem: 4873\n",
            "Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000021  loss: 2.1279 (2.3164)  time: 0.1359  data: 0.0145  max mem: 4873\n",
            "Epoch: [8] Total time: 0:01:46 (0.1362 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 2.1279 (2.3164)\n",
            "Test:  [ 0/53]  eta: 0:00:45  loss: 0.8001 (0.8001)  acc1: 80.7292 (80.7292)  acc5: 95.8333 (95.8333)  time: 0.8547  data: 0.8239  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0405 (0.9825)  acc1: 76.5625 (77.7462)  acc5: 93.2292 (93.7500)  time: 0.1750  data: 0.1443  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1187 (1.0513)  acc1: 74.4792 (76.9345)  acc5: 92.7083 (92.4851)  time: 0.1235  data: 0.0929  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1741 (1.0966)  acc1: 73.4375 (75.8569)  acc5: 91.1458 (91.6835)  time: 0.1241  data: 0.0934  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1885 (1.1378)  acc1: 72.9167 (74.6824)  acc5: 89.0625 (91.2602)  time: 0.1194  data: 0.0888  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1805 (1.1380)  acc1: 72.9167 (74.3975)  acc5: 92.1875 (91.3909)  time: 0.1215  data: 0.0908  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1833 (1.1466)  acc1: 72.3958 (74.2500)  acc5: 92.1875 (91.4100)  time: 0.1027  data: 0.0730  max mem: 4873\n",
            "Test: Total time: 0:00:06 (0.1313 s / it)\n",
            "* Acc@1 74.250 Acc@5 91.410 loss 1.147\n",
            "Accuracy of the network on the 10000 test images: 74.3%\n",
            "Max accuracy: 74.25%\n",
            "Epoch: [9]  [  0/781]  eta: 0:11:55  lr: 0.000015  loss: 2.1758 (2.1758)  time: 0.9166  data: 0.7910  max mem: 4873\n",
            "Epoch: [9]  [ 10/781]  eta: 0:02:31  lr: 0.000015  loss: 2.0956 (2.1168)  time: 0.1965  data: 0.0743  max mem: 4873\n",
            "Epoch: [9]  [ 20/781]  eta: 0:02:08  lr: 0.000015  loss: 2.0956 (2.1742)  time: 0.1315  data: 0.0076  max mem: 4873\n",
            "Epoch: [9]  [ 30/781]  eta: 0:02:03  lr: 0.000015  loss: 2.1112 (2.2152)  time: 0.1467  data: 0.0218  max mem: 4873\n",
            "Epoch: [9]  [ 40/781]  eta: 0:01:55  lr: 0.000015  loss: 2.1200 (2.2802)  time: 0.1436  data: 0.0211  max mem: 4873\n",
            "Epoch: [9]  [ 50/781]  eta: 0:01:50  lr: 0.000015  loss: 2.1338 (2.2951)  time: 0.1319  data: 0.0108  max mem: 4873\n",
            "Epoch: [9]  [ 60/781]  eta: 0:01:46  lr: 0.000015  loss: 2.1338 (2.3221)  time: 0.1311  data: 0.0077  max mem: 4873\n",
            "Epoch: [9]  [ 70/781]  eta: 0:01:43  lr: 0.000015  loss: 2.2210 (2.3706)  time: 0.1282  data: 0.0045  max mem: 4873\n",
            "Epoch: [9]  [ 80/781]  eta: 0:01:40  lr: 0.000015  loss: 2.1807 (2.3865)  time: 0.1266  data: 0.0046  max mem: 4873\n",
            "Epoch: [9]  [ 90/781]  eta: 0:01:37  lr: 0.000015  loss: 2.3396 (2.4192)  time: 0.1260  data: 0.0037  max mem: 4873\n",
            "Epoch: [9]  [100/781]  eta: 0:01:34  lr: 0.000015  loss: 2.1367 (2.3823)  time: 0.1236  data: 0.0020  max mem: 4873\n",
            "Epoch: [9]  [110/781]  eta: 0:01:33  lr: 0.000015  loss: 2.1070 (2.3950)  time: 0.1293  data: 0.0067  max mem: 4873\n",
            "Epoch: [9]  [120/781]  eta: 0:01:31  lr: 0.000015  loss: 2.2061 (2.3943)  time: 0.1357  data: 0.0116  max mem: 4873\n",
            "Epoch: [9]  [130/781]  eta: 0:01:30  lr: 0.000015  loss: 2.1469 (2.3832)  time: 0.1377  data: 0.0140  max mem: 4873\n",
            "Epoch: [9]  [140/781]  eta: 0:01:28  lr: 0.000015  loss: 2.1234 (2.3957)  time: 0.1315  data: 0.0086  max mem: 4873\n",
            "Epoch: [9]  [150/781]  eta: 0:01:26  lr: 0.000015  loss: 2.1818 (2.3915)  time: 0.1251  data: 0.0019  max mem: 4873\n",
            "Epoch: [9]  [160/781]  eta: 0:01:24  lr: 0.000015  loss: 2.1761 (2.3823)  time: 0.1246  data: 0.0017  max mem: 4873\n",
            "Epoch: [9]  [170/781]  eta: 0:01:22  lr: 0.000015  loss: 2.1761 (2.3836)  time: 0.1226  data: 0.0003  max mem: 4873\n",
            "Epoch: [9]  [180/781]  eta: 0:01:20  lr: 0.000015  loss: 2.1740 (2.3758)  time: 0.1233  data: 0.0007  max mem: 4873\n",
            "Epoch: [9]  [190/781]  eta: 0:01:19  lr: 0.000015  loss: 2.1201 (2.3809)  time: 0.1254  data: 0.0029  max mem: 4873\n",
            "Epoch: [9]  [200/781]  eta: 0:01:17  lr: 0.000015  loss: 2.1401 (2.3699)  time: 0.1282  data: 0.0058  max mem: 4873\n",
            "Epoch: [9]  [210/781]  eta: 0:01:16  lr: 0.000015  loss: 2.1401 (2.3687)  time: 0.1317  data: 0.0094  max mem: 4873\n",
            "Epoch: [9]  [220/781]  eta: 0:01:15  lr: 0.000015  loss: 2.1462 (2.3672)  time: 0.1350  data: 0.0117  max mem: 4873\n",
            "Epoch: [9]  [230/781]  eta: 0:01:13  lr: 0.000015  loss: 2.1462 (2.3602)  time: 0.1315  data: 0.0083  max mem: 4873\n",
            "Epoch: [9]  [240/781]  eta: 0:01:12  lr: 0.000015  loss: 2.1322 (2.3502)  time: 0.1259  data: 0.0036  max mem: 4873\n",
            "Epoch: [9]  [250/781]  eta: 0:01:10  lr: 0.000015  loss: 2.1203 (2.3468)  time: 0.1276  data: 0.0039  max mem: 4873\n",
            "Epoch: [9]  [260/781]  eta: 0:01:09  lr: 0.000015  loss: 2.1135 (2.3506)  time: 0.1300  data: 0.0058  max mem: 4873\n",
            "Epoch: [9]  [270/781]  eta: 0:01:08  lr: 0.000015  loss: 2.1559 (2.3498)  time: 0.1350  data: 0.0115  max mem: 4873\n",
            "Epoch: [9]  [280/781]  eta: 0:01:06  lr: 0.000015  loss: 2.1405 (2.3438)  time: 0.1364  data: 0.0138  max mem: 4873\n",
            "Epoch: [9]  [290/781]  eta: 0:01:05  lr: 0.000015  loss: 2.1243 (2.3359)  time: 0.1299  data: 0.0066  max mem: 4873\n",
            "Epoch: [9]  [300/781]  eta: 0:01:03  lr: 0.000015  loss: 2.1243 (2.3357)  time: 0.1262  data: 0.0019  max mem: 4873\n",
            "Epoch: [9]  [310/781]  eta: 0:01:02  lr: 0.000015  loss: 2.1455 (2.3331)  time: 0.1320  data: 0.0080  max mem: 4873\n",
            "Epoch: [9]  [320/781]  eta: 0:01:01  lr: 0.000015  loss: 2.1767 (2.3312)  time: 0.1341  data: 0.0110  max mem: 4873\n",
            "Epoch: [9]  [330/781]  eta: 0:00:59  lr: 0.000015  loss: 2.1322 (2.3237)  time: 0.1273  data: 0.0042  max mem: 4873\n",
            "Epoch: [9]  [340/781]  eta: 0:00:58  lr: 0.000015  loss: 2.1202 (2.3216)  time: 0.1247  data: 0.0008  max mem: 4873\n",
            "Epoch: [9]  [350/781]  eta: 0:00:57  lr: 0.000015  loss: 2.0723 (2.3180)  time: 0.1279  data: 0.0044  max mem: 4873\n",
            "Epoch: [9]  [360/781]  eta: 0:00:55  lr: 0.000015  loss: 2.0666 (2.3213)  time: 0.1269  data: 0.0044  max mem: 4873\n",
            "Epoch: [9]  [370/781]  eta: 0:00:54  lr: 0.000015  loss: 2.1302 (2.3246)  time: 0.1236  data: 0.0023  max mem: 4873\n",
            "Epoch: [9]  [380/781]  eta: 0:00:52  lr: 0.000015  loss: 2.1302 (2.3240)  time: 0.1261  data: 0.0050  max mem: 4873\n",
            "Epoch: [9]  [390/781]  eta: 0:00:51  lr: 0.000015  loss: 2.0670 (2.3222)  time: 0.1283  data: 0.0066  max mem: 4873\n",
            "Epoch: [9]  [400/781]  eta: 0:00:50  lr: 0.000015  loss: 2.0320 (2.3173)  time: 0.1324  data: 0.0089  max mem: 4873\n",
            "Epoch: [9]  [410/781]  eta: 0:00:48  lr: 0.000015  loss: 2.0482 (2.3151)  time: 0.1303  data: 0.0054  max mem: 4873\n",
            "Epoch: [9]  [420/781]  eta: 0:00:47  lr: 0.000015  loss: 2.1822 (2.3231)  time: 0.1251  data: 0.0003  max mem: 4873\n",
            "Epoch: [9]  [430/781]  eta: 0:00:46  lr: 0.000015  loss: 2.1822 (2.3200)  time: 0.1268  data: 0.0035  max mem: 4873\n",
            "Epoch: [9]  [440/781]  eta: 0:00:44  lr: 0.000015  loss: 2.0590 (2.3169)  time: 0.1270  data: 0.0037  max mem: 4873\n",
            "Epoch: [9]  [450/781]  eta: 0:00:43  lr: 0.000015  loss: 2.1503 (2.3152)  time: 0.1233  data: 0.0005  max mem: 4873\n",
            "Epoch: [9]  [460/781]  eta: 0:00:42  lr: 0.000015  loss: 2.0960 (2.3140)  time: 0.1213  data: 0.0003  max mem: 4873\n",
            "Epoch: [9]  [470/781]  eta: 0:00:40  lr: 0.000015  loss: 2.0538 (2.3117)  time: 0.1231  data: 0.0015  max mem: 4873\n",
            "Epoch: [9]  [480/781]  eta: 0:00:39  lr: 0.000015  loss: 2.0649 (2.3102)  time: 0.1263  data: 0.0037  max mem: 4873\n",
            "Epoch: [9]  [490/781]  eta: 0:00:37  lr: 0.000015  loss: 2.0605 (2.3080)  time: 0.1260  data: 0.0036  max mem: 4873\n",
            "Epoch: [9]  [500/781]  eta: 0:00:36  lr: 0.000015  loss: 2.0269 (2.3091)  time: 0.1290  data: 0.0055  max mem: 4873\n",
            "Epoch: [9]  [510/781]  eta: 0:00:35  lr: 0.000015  loss: 2.0840 (2.3114)  time: 0.1310  data: 0.0064  max mem: 4873\n",
            "Epoch: [9]  [520/781]  eta: 0:00:34  lr: 0.000015  loss: 2.1407 (2.3123)  time: 0.1313  data: 0.0075  max mem: 4873\n",
            "Epoch: [9]  [530/781]  eta: 0:00:32  lr: 0.000015  loss: 2.2220 (2.3109)  time: 0.1286  data: 0.0054  max mem: 4873\n",
            "Epoch: [9]  [540/781]  eta: 0:00:31  lr: 0.000015  loss: 2.1791 (2.3107)  time: 0.1228  data: 0.0010  max mem: 4873\n",
            "Epoch: [9]  [550/781]  eta: 0:00:30  lr: 0.000015  loss: 2.1182 (2.3104)  time: 0.1259  data: 0.0046  max mem: 4873\n",
            "Epoch: [9]  [560/781]  eta: 0:00:28  lr: 0.000015  loss: 2.2300 (2.3166)  time: 0.1266  data: 0.0039  max mem: 4873\n",
            "Epoch: [9]  [570/781]  eta: 0:00:27  lr: 0.000015  loss: 2.2557 (2.3204)  time: 0.1266  data: 0.0033  max mem: 4873\n",
            "Epoch: [9]  [580/781]  eta: 0:00:26  lr: 0.000015  loss: 2.1502 (2.3220)  time: 0.1281  data: 0.0044  max mem: 4873\n",
            "Epoch: [9]  [590/781]  eta: 0:00:24  lr: 0.000015  loss: 2.1502 (2.3216)  time: 0.1274  data: 0.0032  max mem: 4873\n",
            "Epoch: [9]  [600/781]  eta: 0:00:23  lr: 0.000015  loss: 2.0835 (2.3190)  time: 0.1282  data: 0.0039  max mem: 4873\n",
            "Epoch: [9]  [610/781]  eta: 0:00:22  lr: 0.000015  loss: 2.1171 (2.3157)  time: 0.1321  data: 0.0074  max mem: 4873\n",
            "Epoch: [9]  [620/781]  eta: 0:00:20  lr: 0.000015  loss: 2.1171 (2.3136)  time: 0.1332  data: 0.0101  max mem: 4873\n",
            "Epoch: [9]  [630/781]  eta: 0:00:19  lr: 0.000015  loss: 2.1074 (2.3128)  time: 0.1294  data: 0.0075  max mem: 4873\n",
            "Epoch: [9]  [640/781]  eta: 0:00:18  lr: 0.000015  loss: 2.0418 (2.3095)  time: 0.1301  data: 0.0079  max mem: 4873\n",
            "Epoch: [9]  [650/781]  eta: 0:00:17  lr: 0.000015  loss: 2.0287 (2.3091)  time: 0.1288  data: 0.0061  max mem: 4873\n",
            "Epoch: [9]  [660/781]  eta: 0:00:15  lr: 0.000015  loss: 2.1319 (2.3085)  time: 0.1247  data: 0.0013  max mem: 4873\n",
            "Epoch: [9]  [670/781]  eta: 0:00:14  lr: 0.000015  loss: 2.1081 (2.3063)  time: 0.1256  data: 0.0007  max mem: 4873\n",
            "Epoch: [9]  [680/781]  eta: 0:00:13  lr: 0.000015  loss: 2.0377 (2.3050)  time: 0.1255  data: 0.0005  max mem: 4873\n",
            "Epoch: [9]  [690/781]  eta: 0:00:11  lr: 0.000015  loss: 2.0377 (2.3020)  time: 0.1266  data: 0.0028  max mem: 4873\n",
            "Epoch: [9]  [700/781]  eta: 0:00:10  lr: 0.000015  loss: 2.1225 (2.3045)  time: 0.1313  data: 0.0078  max mem: 4873\n",
            "Epoch: [9]  [710/781]  eta: 0:00:09  lr: 0.000015  loss: 2.1672 (2.3050)  time: 0.1314  data: 0.0081  max mem: 4873\n",
            "Epoch: [9]  [720/781]  eta: 0:00:07  lr: 0.000015  loss: 2.2228 (2.3070)  time: 0.1268  data: 0.0047  max mem: 4873\n",
            "Epoch: [9]  [730/781]  eta: 0:00:06  lr: 0.000015  loss: 2.1321 (2.3055)  time: 0.1231  data: 0.0022  max mem: 4873\n",
            "Epoch: [9]  [740/781]  eta: 0:00:05  lr: 0.000015  loss: 2.0636 (2.3020)  time: 0.1270  data: 0.0061  max mem: 4873\n",
            "Epoch: [9]  [750/781]  eta: 0:00:04  lr: 0.000015  loss: 2.0589 (2.3005)  time: 0.1296  data: 0.0087  max mem: 4873\n",
            "Epoch: [9]  [760/781]  eta: 0:00:02  lr: 0.000015  loss: 2.0746 (2.2989)  time: 0.1280  data: 0.0070  max mem: 4873\n",
            "Epoch: [9]  [770/781]  eta: 0:00:01  lr: 0.000015  loss: 2.1199 (2.2983)  time: 0.1279  data: 0.0069  max mem: 4873\n",
            "Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000015  loss: 2.0988 (2.2977)  time: 0.1290  data: 0.0085  max mem: 4873\n",
            "Epoch: [9] Total time: 0:01:41 (0.1298 s / it)\n",
            "Averaged stats: lr: 0.000015  loss: 2.0988 (2.2977)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.8107 (0.8107)  acc1: 79.1667 (79.1667)  acc5: 94.7917 (94.7917)  time: 0.8359  data: 0.8050  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0198 (0.9555)  acc1: 77.0833 (78.0303)  acc5: 94.7917 (93.9867)  time: 0.1758  data: 0.1451  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0534 (1.0319)  acc1: 73.4375 (76.5873)  acc5: 92.7083 (92.5347)  time: 0.1261  data: 0.0954  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1399 (1.0694)  acc1: 72.9167 (75.8065)  acc5: 90.6250 (91.9523)  time: 0.1378  data: 0.1071  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1945 (1.1093)  acc1: 71.8750 (74.6951)  acc5: 90.6250 (91.5523)  time: 0.1244  data: 0.0937  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1978 (1.1164)  acc1: 71.8750 (74.3873)  acc5: 91.1458 (91.5339)  time: 0.1343  data: 0.1036  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1992 (1.1260)  acc1: 71.8750 (74.2600)  acc5: 91.1458 (91.5500)  time: 0.1280  data: 0.0983  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1410 s / it)\n",
            "* Acc@1 74.260 Acc@5 91.550 loss 1.126\n",
            "Accuracy of the network on the 10000 test images: 74.3%\n",
            "Max accuracy: 74.26%\n",
            "Training time 0:18:56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}