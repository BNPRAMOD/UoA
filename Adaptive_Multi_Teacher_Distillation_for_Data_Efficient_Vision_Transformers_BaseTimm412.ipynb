{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "ed74202e-f7e0-4d95-8cc2-3c7992a35b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "eb1bb1d9-a092-4f52-dea2-7dedf0cbc885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'deit' already exists and is not an empty directory.\n",
            "/content/deit\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "a81dfb11-d99f-4f43-c2c6-ccd30bf00ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "a242ee4d-f680-455c-dd05-d7cfdd8a09e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "7a3730af-8c44-424d-ca0c-86a6e121bdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jedi<0.19,>=0.16 in /usr/local/lib/python3.12/dist-packages (0.18.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "# !pip -q install timm==0.6.13 submitit\n",
        "!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q uninstall -y timm\n",
        "#!pip -q install -U pip setuptools wheel\n",
        "#!pip -q install -U \"timm>=1.0.0\""
      ],
      "metadata": {
        "id": "q0Mim13um2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\"\n",
        "#0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "05037149-3c64-4af5-cd91-8c9031113ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "836a14bf-5a6f-4ed0-a389-a93b42b69eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runtime → Restart runtime"
      ],
      "metadata": {
        "id": "M0ZDDe3uvU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install timm submitit"
      ],
      "metadata": {
        "id": "H3T5zLnQukuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "d9441edc-2bb3-4a5a-e1c6-6e9df1623c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "00b84688-da0f-4da8-d3a8-ae2355b4d9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.4.12\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "1cf3118e-3ee2-4272-9db7-b1591e9e9a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "968987c1-fab6-45d4-cd13-9e171ab8c124",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "d5a82984-2213-4cc0-e74d-aaaa98c81356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "f868c4f4-cb12-485b-d51f-ac365aa6cd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n03891332\n",
            "/content/tiny-imagenet-200/val/n04275548\n",
            "/content/tiny-imagenet-200/val/n07695742\n",
            "/content/tiny-imagenet-200/val/n01950731\n",
            "/content/tiny-imagenet-200/val/n01770393\n",
            "/content/tiny-imagenet-200/val/n03393912\n",
            "/content/tiny-imagenet-200/val/n03179701\n",
            "/content/tiny-imagenet-200/val/n09256479\n",
            "/content/tiny-imagenet-200/val/n04070727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "172a6607-8c52-4b38-b9dc-4f9752363cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Jan 25 13:15 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Jan 25 13:14 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Jan 25 13:15 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "a6e6be6f-ac8d-4ef3-ebde-6c8cee391f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "ℹ️ Expected import line not found; augment.py may already be patched or different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!sed -n '1,200p' models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cm_gVMz1-_",
        "outputId": "96ea4bae-b16d-49e8-f71a-4c7cdeb87ef3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "# Copyright (c) 2015-present, Facebook, Inc.\n",
            "# All rights reserved.\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from functools import partial\n",
            "\n",
            "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
            "from timm.models.registry import register_model\n",
            "from timm.models.layers import trunc_normal_\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
            "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
            "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
            "    'deit_base_distilled_patch16_384',\n",
            "]\n",
            "\n",
            "\n",
            "class DistilledVisionTransformer(VisionTransformer):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
            "        num_patches = self.patch_embed.num_patches\n",
            "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
            "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
            "\n",
            "        trunc_normal_(self.dist_token, std=.02)\n",
            "        trunc_normal_(self.pos_embed, std=.02)\n",
            "        self.head_dist.apply(self._init_weights)\n",
            "\n",
            "    def forward_features(self, x):\n",
            "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
            "        # with slight modifications to add the dist_token\n",
            "        B = x.shape[0]\n",
            "        x = self.patch_embed(x)\n",
            "\n",
            "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
            "        dist_token = self.dist_token.expand(B, -1, -1)\n",
            "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
            "\n",
            "        x = x + self.pos_embed\n",
            "        x = self.pos_drop(x)\n",
            "\n",
            "        for blk in self.blocks:\n",
            "            x = blk(x)\n",
            "\n",
            "        x = self.norm(x)\n",
            "        return x[:, 0], x[:, 1]\n",
            "\n",
            "    def forward(self, x):\n",
            "        x, x_dist = self.forward_features(x)\n",
            "        x = self.head(x)\n",
            "        x_dist = self.head_dist(x_dist)\n",
            "        if self.training:\n",
            "            return x, x_dist\n",
            "        else:\n",
            "            # during inference, return the average of both classifier predictions\n",
            "            return (x + x_dist) / 2\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "    kwargs.pop('pretrained_cfg_priority', None)\n",
            "    # Drop timm-injected kwargs not supported by DeiT\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('cache_dir', None)\n",
            "    kwargs.pop('hf_hub_id', None)\n",
            "    kwargs.pop('hf_hub_filename', None)\n",
            "    kwargs.pop('hf_hub_revision', None)\n",
            "    kwargs.pop('pretrained_cfg', None)\n",
            "    kwargs.pop('pretrained_cfg_overlay', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "id": "RizknqA6MBXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(teacher_logits: Dict[str, torch.Tensor], teacher_order: List[str], lambdas: torch.Tensor) -> torch.Tensor:\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)\n",
        "    lambdas = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lambdas.dim() == 1:\n",
        "        lambdas = lambdas.unsqueeze(0).expand(stacked.size(0), -1)\n",
        "    return (stacked * lambdas.unsqueeze(-1)).sum(dim=1)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    T = float(T)\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    return F.cross_entropy(student_logits, teacher_logits.argmax(dim=-1))\n",
        "\n",
        "\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleDict({\n",
        "            name: timm.create_model(name, pretrained=True, num_classes=1000).eval().to(device)\n",
        "            for name in teacher_names\n",
        "        })\n",
        "        for m in self.models.values():\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "class TeacherLogitAdapter(nn.Module):\n",
        "    def __init__(self, teacher_keys: List[str], student_num_classes: int):\n",
        "        super().__init__()\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            k: nn.Linear(1000, student_num_classes, bias=False) for k in teacher_keys\n",
        "        })\n",
        "\n",
        "    def forward(self, teacher_logits):\n",
        "        return {k: self.adapters[k](v) for k, v in teacher_logits.items()}\n",
        "\n",
        "\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    def __init__(self, temp=1.0):\n",
        "        super().__init__()\n",
        "        self.temp = temp\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, student_logits, teacher_logits, teacher_order, targets):\n",
        "        stacked = torch.stack([teacher_logits[k] for k in teacher_order], dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Case 1: hard labels (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Case 2: soft labels from mixup/cutmix (B,C)\n",
        "        # Confidence = expected probability under the soft label distribution\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)     # (B,C)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)                 # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion,\n",
        "        student_num_classes,\n",
        "        teacher_names,\n",
        "        distillation_type=\"soft\",\n",
        "        alpha=0.5,\n",
        "        tau=2.0,\n",
        "        device=None,\n",
        "        use_adapter=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = distillation_type\n",
        "        self.alpha = alpha\n",
        "        self.tau = tau\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, self.device)\n",
        "        self.adapter = TeacherLogitAdapter(self.teachers.teacher_order, student_num_classes).to(self.device) if use_adapter else None\n",
        "        self.hdtse = HDTSEConfidence()\n",
        "\n",
        "    def forward(self, inputs, outputs, targets):\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)\n",
        "        if self.adapter:\n",
        "            t_logits = self.adapter(t_logits)\n",
        "\n",
        "        order = self.teachers.teacher_order\n",
        "        lambdas = self.hdtse(outputs, t_logits, order, targets)\n",
        "        fused = fuse_logits(t_logits, order, lambdas)\n",
        "\n",
        "        kd = kd_soft(outputs, fused, self.tau) if self.distillation_type == \"soft\" else kd_hard(outputs, fused)\n",
        "        return (1 - self.alpha) * base_loss + self.alpha * kd\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "id": "_k4jzkzbMHD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - <<'PY'\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"main.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "# 1) Import our new loss\n",
        "if \"from multiteacher_loss import MultiTeacherDistillationLoss\" not in txt:\n",
        "    txt = txt.replace(\n",
        "        \"from losses import DistillationLoss\",\n",
        "        \"from losses import DistillationLoss\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\"\n",
        "    )\n",
        "\n",
        "# 2) Add CLI arg for multiple teachers\n",
        "if \"--teacher-models\" not in txt:\n",
        "    txt = txt.replace(\n",
        "        \"parser.add_argument('--teacher-path', type=str, default='')\",\n",
        "        \"parser.add_argument('--teacher-path', type=str, default='')\\n\"\n",
        "        \"    parser.add_argument('--teacher-models', type=str, default='',\\n\"\n",
        "        \"                        help='Comma-separated timm model names for multi-teacher distillation')\"\n",
        "    )\n",
        "\n",
        "# 3) Replace criterion construction block\n",
        "# We search for the block that instantiates DistillationLoss and replace it with a multi-teacher switch.\n",
        "pattern = r\"(criterion\\s*=\\s*DistillationLoss\\([\\s\\S]*?\\)\\n)\"\n",
        "m = re.search(pattern, txt)\n",
        "if m:\n",
        "    block = m.group(1)\n",
        "    repl = (\n",
        "        \"    # --- Multi-teacher distillation (Layer 2) ---\\n\"\n",
        "        \"    if args.teacher_models:\\n\"\n",
        "        \"        teacher_names = [t.strip() for t in args.teacher_models.split(',') if t.strip()]\\n\"\n",
        "        \"        criterion = MultiTeacherDistillationLoss(\\n\"\n",
        "        \"            base_criterion=criterion,\\n\"\n",
        "        \"            student_num_classes=args.nb_classes,\\n\"\n",
        "        \"            teacher_names=teacher_names,\\n\"\n",
        "        \"            distillation_type=args.distillation_type,\\n\"\n",
        "        \"            alpha=args.distillation_alpha,\\n\"\n",
        "        \"            tau=args.distillation_tau,\\n\"\n",
        "        \"            device=device,\\n\"\n",
        "        \"            use_adapter=True,\\n\"\n",
        "        \"        )\\n\"\n",
        "        \"    else:\\n\"\n",
        "        + block\n",
        "    )\n",
        "    txt = txt[:m.start()] + repl + txt[m.end():]\n",
        "else:\n",
        "    print(\"WARNING: Could not auto-find DistillationLoss block. You'll need to patch main.py manually by searching for 'DistillationLoss('.\")\n",
        "\n",
        "p.write_text(txt)\n",
        "print(\"✅ Patched main.py for multi-teacher loss.\")"
      ],
      "metadata": {
        "id": "zUdJZ4F-NoE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "\n",
        "# 1) Backup\n",
        "backup = MAIN.with_name(f\"main.py.bak_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "shutil.copy2(MAIN, backup)\n",
        "print(\"✅ Backup created:\", backup)\n",
        "\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# 2) Canonical distillation block (single-teacher OR multi-teacher, no duplicates)\n",
        "replacement_block = r\"\"\"\n",
        "    teacher_model = None\n",
        "    if args.distillation_type != 'none':\n",
        "        assert (args.teacher_path or args.teacher_models), 'need to specify teacher-path OR teacher-models when using distillation'\n",
        "\n",
        "        if args.teacher_models:\n",
        "            teacher_names = [t.strip() for t in args.teacher_models.split(',') if t.strip()]\n",
        "            teacher_model = None\n",
        "            criterion = MultiTeacherDistillationLoss(\n",
        "                base_criterion=criterion,\n",
        "                student_num_classes=model_without_ddp.head.out_features,\n",
        "                teacher_names=teacher_names,\n",
        "                distillation_type=args.distillation_type,\n",
        "                alpha=args.distillation_alpha,\n",
        "                tau=args.distillation_tau,\n",
        "                device=device,\n",
        "                use_adapter=True,\n",
        "            )\n",
        "        else:\n",
        "            # Single-teacher DeiT distillation path (expects a checkpoint at --teacher-path)\n",
        "            print(f\"Creating teacher model: {args.teacher_model}\")\n",
        "            teacher_model = create_model(\n",
        "                args.teacher_model,\n",
        "                pretrained=False,\n",
        "                num_classes=args.nb_classes,\n",
        "                global_pool='avg',\n",
        "            )\n",
        "            if args.teacher_path.startswith('https'):\n",
        "                checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                    args.teacher_path, map_location='cpu', check_hash=True)\n",
        "            else:\n",
        "                checkpoint = torch.load(args.teacher_path, map_location='cpu')\n",
        "            teacher_model.load_state_dict(checkpoint['model'])\n",
        "            teacher_model.to(device)\n",
        "            teacher_model.eval()\n",
        "            for p in teacher_model.parameters():\n",
        "                p.requires_grad_(False)\n",
        "\n",
        "            criterion = DistillationLoss(\n",
        "                criterion,\n",
        "                teacher_model,\n",
        "                args.distillation_type,\n",
        "                args.distillation_alpha,\n",
        "                args.distillation_tau,\n",
        "            )\n",
        "\"\"\"\n",
        "\n",
        "# 3) Replace the entire problematic region:\n",
        "#    from: \"teacher_model = None\"\n",
        "#    to:   just before \"output_dir = Path(args.output_dir)\"\n",
        "pattern = re.compile(\n",
        "    r\"\\n\\s*teacher_model\\s*=\\s*None\\s*\\n.*?\\n\\s*output_dir\\s*=\\s*Path\\(args\\.output_dir\\)\\s*\\n\",\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "new_txt, n = pattern.subn(\"\\n\" + replacement_block + \"\\n\\n    output_dir = Path(args.output_dir)\\n\", txt)\n",
        "\n",
        "if n != 1:\n",
        "    raise RuntimeError(f\"Expected to replace exactly 1 distillation block, but replaced {n}. \"\n",
        "                       f\"Open {backup} and inspect if needed.\")\n",
        "\n",
        "MAIN.write_text(new_txt)\n",
        "print(\"✅ main.py patched successfully (distillation block rewritten cleanly).\")\n",
        "\n",
        "# 4) Quick sanity check: ensure there are NO duplicate 'Layer 2' blocks left\n",
        "print(\"Occurrences of 'Multi-teacher distillation (Layer 2)' after patch:\",\n",
        "      new_txt.count(\"Multi-teacher distillation (Layer 2)\"))\n",
        "print(\"Occurrences of 'if args.teacher_models' after patch:\",\n",
        "      new_txt.count(\"if args.teacher_models\"))"
      ],
      "metadata": {
        "id": "JZ0ssb9DOarH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "f75ef233-7c10-46b0-ed12-d4f8d99b521f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"pretrained_cfg\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxOmdCb90Ymg",
        "outputId": "38dfb57b-289a-4a65-e6a1-e85ec2aa4dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('pretrained_cfg', None)\n",
            "66:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "67:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "73:    kwargs.pop('pretrained_cfg', None)\n",
            "74:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "75:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "85:    kwargs.pop('pretrained_cfg', None)\n",
            "86:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "87:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "101:    kwargs.pop('pretrained_cfg', None)\n",
            "102:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "103:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "120:    kwargs.pop('pretrained_cfg', None)\n",
            "121:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "122:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "128:    kwargs.pop('pretrained_cfg', None)\n",
            "129:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "130:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "140:    kwargs.pop('pretrained_cfg', None)\n",
            "141:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "142:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "156:    kwargs.pop('pretrained_cfg', None)\n",
            "157:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "158:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "175:    kwargs.pop('pretrained_cfg', None)\n",
            "176:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "177:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "183:    kwargs.pop('pretrained_cfg', None)\n",
            "184:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "185:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "195:    kwargs.pop('pretrained_cfg', None)\n",
            "196:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "197:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "211:    kwargs.pop('pretrained_cfg', None)\n",
            "212:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "213:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "230:    kwargs.pop('pretrained_cfg', None)\n",
            "231:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "232:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "238:    kwargs.pop('pretrained_cfg', None)\n",
            "239:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "240:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "250:    kwargs.pop('pretrained_cfg', None)\n",
            "251:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "252:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "266:    kwargs.pop('pretrained_cfg', None)\n",
            "267:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "268:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "285:    kwargs.pop('pretrained_cfg', None)\n",
            "286:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "287:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "293:    kwargs.pop('pretrained_cfg', None)\n",
            "294:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "295:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "305:    kwargs.pop('pretrained_cfg', None)\n",
            "306:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "307:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "321:    kwargs.pop('pretrained_cfg', None)\n",
            "322:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "323:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "340:    kwargs.pop('pretrained_cfg', None)\n",
            "341:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "342:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "348:    kwargs.pop('pretrained_cfg', None)\n",
            "349:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "350:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "360:    kwargs.pop('pretrained_cfg', None)\n",
            "361:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "362:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "376:    kwargs.pop('pretrained_cfg', None)\n",
            "377:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "378:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "395:    kwargs.pop('pretrained_cfg', None)\n",
            "396:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "397:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "403:    kwargs.pop('pretrained_cfg', None)\n",
            "404:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "405:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "415:    kwargs.pop('pretrained_cfg', None)\n",
            "416:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "417:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "431:    kwargs.pop('pretrained_cfg', None)\n",
            "432:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "433:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "450:    kwargs.pop('pretrained_cfg', None)\n",
            "451:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "452:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "458:    kwargs.pop('pretrained_cfg', None)\n",
            "459:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "460:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "470:    kwargs.pop('pretrained_cfg', None)\n",
            "471:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "472:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "486:    kwargs.pop('pretrained_cfg', None)\n",
            "487:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "488:    kwargs.pop('pretrained_cfg_priority', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "2e91f2e1-3350-4bc7-daa7-e13718f33ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "V409XjDO1cdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"cache_dir\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFoOP5c1dbu",
        "outputId": "c852425a-2b62-4b33-a934-4c77fd76f5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('cache_dir', None)\n",
            "73:    kwargs.pop('cache_dir', None)\n",
            "77:    kwargs.pop('cache_dir', None)\n",
            "85:    kwargs.pop('cache_dir', None)\n",
            "89:    kwargs.pop('cache_dir', None)\n",
            "93:    kwargs.pop('cache_dir', None)\n",
            "101:    kwargs.pop('cache_dir', None)\n",
            "105:    kwargs.pop('cache_dir', None)\n",
            "109:    kwargs.pop('cache_dir', None)\n",
            "113:    kwargs.pop('cache_dir', None)\n",
            "136:    kwargs.pop('cache_dir', None)\n",
            "144:    kwargs.pop('cache_dir', None)\n",
            "148:    kwargs.pop('cache_dir', None)\n",
            "156:    kwargs.pop('cache_dir', None)\n",
            "160:    kwargs.pop('cache_dir', None)\n",
            "164:    kwargs.pop('cache_dir', None)\n",
            "172:    kwargs.pop('cache_dir', None)\n",
            "176:    kwargs.pop('cache_dir', None)\n",
            "180:    kwargs.pop('cache_dir', None)\n",
            "184:    kwargs.pop('cache_dir', None)\n",
            "207:    kwargs.pop('cache_dir', None)\n",
            "215:    kwargs.pop('cache_dir', None)\n",
            "219:    kwargs.pop('cache_dir', None)\n",
            "227:    kwargs.pop('cache_dir', None)\n",
            "231:    kwargs.pop('cache_dir', None)\n",
            "235:    kwargs.pop('cache_dir', None)\n",
            "243:    kwargs.pop('cache_dir', None)\n",
            "247:    kwargs.pop('cache_dir', None)\n",
            "251:    kwargs.pop('cache_dir', None)\n",
            "255:    kwargs.pop('cache_dir', None)\n",
            "278:    kwargs.pop('cache_dir', None)\n",
            "286:    kwargs.pop('cache_dir', None)\n",
            "290:    kwargs.pop('cache_dir', None)\n",
            "298:    kwargs.pop('cache_dir', None)\n",
            "302:    kwargs.pop('cache_dir', None)\n",
            "306:    kwargs.pop('cache_dir', None)\n",
            "314:    kwargs.pop('cache_dir', None)\n",
            "318:    kwargs.pop('cache_dir', None)\n",
            "322:    kwargs.pop('cache_dir', None)\n",
            "326:    kwargs.pop('cache_dir', None)\n",
            "349:    kwargs.pop('cache_dir', None)\n",
            "357:    kwargs.pop('cache_dir', None)\n",
            "361:    kwargs.pop('cache_dir', None)\n",
            "369:    kwargs.pop('cache_dir', None)\n",
            "373:    kwargs.pop('cache_dir', None)\n",
            "377:    kwargs.pop('cache_dir', None)\n",
            "385:    kwargs.pop('cache_dir', None)\n",
            "389:    kwargs.pop('cache_dir', None)\n",
            "393:    kwargs.pop('cache_dir', None)\n",
            "397:    kwargs.pop('cache_dir', None)\n",
            "420:    kwargs.pop('cache_dir', None)\n",
            "428:    kwargs.pop('cache_dir', None)\n",
            "432:    kwargs.pop('cache_dir', None)\n",
            "440:    kwargs.pop('cache_dir', None)\n",
            "444:    kwargs.pop('cache_dir', None)\n",
            "448:    kwargs.pop('cache_dir', None)\n",
            "456:    kwargs.pop('cache_dir', None)\n",
            "460:    kwargs.pop('cache_dir', None)\n",
            "464:    kwargs.pop('cache_dir', None)\n",
            "468:    kwargs.pop('cache_dir', None)\n",
            "491:    kwargs.pop('cache_dir', None)\n",
            "499:    kwargs.pop('cache_dir', None)\n",
            "503:    kwargs.pop('cache_dir', None)\n",
            "511:    kwargs.pop('cache_dir', None)\n",
            "515:    kwargs.pop('cache_dir', None)\n",
            "519:    kwargs.pop('cache_dir', None)\n",
            "527:    kwargs.pop('cache_dir', None)\n",
            "531:    kwargs.pop('cache_dir', None)\n",
            "535:    kwargs.pop('cache_dir', None)\n",
            "539:    kwargs.pop('cache_dir', None)\n",
            "562:    kwargs.pop('cache_dir', None)\n",
            "570:    kwargs.pop('cache_dir', None)\n",
            "574:    kwargs.pop('cache_dir', None)\n",
            "582:    kwargs.pop('cache_dir', None)\n",
            "586:    kwargs.pop('cache_dir', None)\n",
            "590:    kwargs.pop('cache_dir', None)\n",
            "598:    kwargs.pop('cache_dir', None)\n",
            "602:    kwargs.pop('cache_dir', None)\n",
            "606:    kwargs.pop('cache_dir', None)\n",
            "610:    kwargs.pop('cache_dir', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 10 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 2 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.1 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b0,tf_efficientnet_b1,mobilenetv3_large_100\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "5fcfef6c-1048-4451-d82e-fd3cf15f0339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=10, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.00025, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=2, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m6-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.1, remode='pixel', recount=1, resplit=False, mixup=0.2, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet_5ep', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "number of params: 5717416\n",
            "/usr/local/lib/python3.12/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "Start training for 10 epochs\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch: [0]  [  0/781]  eta: 0:26:36  lr: 0.000001  loss: 8.6634 (8.6634)  time: 2.0446  data: 0.9924  max mem: 3311\n",
            "Epoch: [0]  [ 10/781]  eta: 0:03:19  lr: 0.000001  loss: 8.5700 (8.5611)  time: 0.2589  data: 0.1011  max mem: 3355\n",
            "Epoch: [0]  [ 20/781]  eta: 0:02:32  lr: 0.000001  loss: 8.5445 (8.5534)  time: 0.1077  data: 0.0397  max mem: 3355\n",
            "Epoch: [0]  [ 30/781]  eta: 0:02:19  lr: 0.000001  loss: 8.4699 (8.4819)  time: 0.1448  data: 0.0774  max mem: 3355\n",
            "Epoch: [0]  [ 40/781]  eta: 0:02:08  lr: 0.000001  loss: 8.3143 (8.4298)  time: 0.1447  data: 0.0759  max mem: 3355\n",
            "Epoch: [0]  [ 50/781]  eta: 0:02:03  lr: 0.000001  loss: 8.2161 (8.3814)  time: 0.1420  data: 0.0731  max mem: 3355\n",
            "Epoch: [0]  [ 60/781]  eta: 0:01:57  lr: 0.000001  loss: 8.1469 (8.3311)  time: 0.1410  data: 0.0732  max mem: 3355\n",
            "Epoch: [0]  [ 70/781]  eta: 0:01:52  lr: 0.000001  loss: 7.9728 (8.2683)  time: 0.1332  data: 0.0637  max mem: 3355\n",
            "Epoch: [0]  [ 80/781]  eta: 0:01:51  lr: 0.000001  loss: 7.8368 (8.2051)  time: 0.1484  data: 0.0796  max mem: 3355\n",
            "Epoch: [0]  [ 90/781]  eta: 0:01:47  lr: 0.000001  loss: 7.6834 (8.1415)  time: 0.1471  data: 0.0804  max mem: 3355\n",
            "Epoch: [0]  [100/781]  eta: 0:01:45  lr: 0.000001  loss: 7.5539 (8.0782)  time: 0.1409  data: 0.0728  max mem: 3355\n",
            "Epoch: [0]  [110/781]  eta: 0:01:42  lr: 0.000001  loss: 7.4331 (8.0145)  time: 0.1420  data: 0.0723  max mem: 3355\n",
            "Epoch: [0]  [120/781]  eta: 0:01:41  lr: 0.000001  loss: 7.3320 (7.9592)  time: 0.1443  data: 0.0715  max mem: 3355\n",
            "Epoch: [0]  [130/781]  eta: 0:01:39  lr: 0.000001  loss: 7.2770 (7.9032)  time: 0.1455  data: 0.0746  max mem: 3355\n",
            "Epoch: [0]  [140/781]  eta: 0:01:37  lr: 0.000001  loss: 7.1800 (7.8488)  time: 0.1458  data: 0.0794  max mem: 3355\n",
            "Epoch: [0]  [150/781]  eta: 0:01:35  lr: 0.000001  loss: 7.1176 (7.7993)  time: 0.1425  data: 0.0758  max mem: 3355\n",
            "Epoch: [0]  [160/781]  eta: 0:01:34  lr: 0.000001  loss: 7.0449 (7.7489)  time: 0.1467  data: 0.0774  max mem: 3355\n",
            "Epoch: [0]  [170/781]  eta: 0:01:32  lr: 0.000001  loss: 6.9598 (7.7013)  time: 0.1490  data: 0.0772  max mem: 3355\n",
            "Epoch: [0]  [180/781]  eta: 0:01:31  lr: 0.000001  loss: 6.8861 (7.6548)  time: 0.1488  data: 0.0801  max mem: 3355\n",
            "Epoch: [0]  [190/781]  eta: 0:01:28  lr: 0.000001  loss: 6.8553 (7.6123)  time: 0.1463  data: 0.0788  max mem: 3355\n",
            "Epoch: [0]  [200/781]  eta: 0:01:27  lr: 0.000001  loss: 6.7591 (7.5691)  time: 0.1469  data: 0.0760  max mem: 3355\n",
            "Epoch: [0]  [210/781]  eta: 0:01:25  lr: 0.000001  loss: 6.7397 (7.5306)  time: 0.1443  data: 0.0743  max mem: 3355\n",
            "Epoch: [0]  [220/781]  eta: 0:01:24  lr: 0.000001  loss: 6.7130 (7.4920)  time: 0.1440  data: 0.0748  max mem: 3355\n",
            "Epoch: [0]  [230/781]  eta: 0:01:22  lr: 0.000001  loss: 6.7052 (7.4572)  time: 0.1464  data: 0.0768  max mem: 3355\n",
            "Epoch: [0]  [240/781]  eta: 0:01:21  lr: 0.000001  loss: 6.6846 (7.4229)  time: 0.1448  data: 0.0769  max mem: 3355\n",
            "Epoch: [0]  [250/781]  eta: 0:01:19  lr: 0.000001  loss: 6.5883 (7.3891)  time: 0.1457  data: 0.0773  max mem: 3355\n",
            "Epoch: [0]  [260/781]  eta: 0:01:17  lr: 0.000001  loss: 6.5862 (7.3581)  time: 0.1385  data: 0.0705  max mem: 3355\n",
            "Epoch: [0]  [270/781]  eta: 0:01:15  lr: 0.000001  loss: 6.5703 (7.3298)  time: 0.1342  data: 0.0651  max mem: 3355\n",
            "Epoch: [0]  [280/781]  eta: 0:01:13  lr: 0.000001  loss: 6.5637 (7.3029)  time: 0.1290  data: 0.0593  max mem: 3355\n",
            "Epoch: [0]  [290/781]  eta: 0:01:11  lr: 0.000001  loss: 6.5424 (7.2762)  time: 0.1258  data: 0.0584  max mem: 3355\n",
            "Epoch: [0]  [300/781]  eta: 0:01:10  lr: 0.000001  loss: 6.5029 (7.2501)  time: 0.1254  data: 0.0597  max mem: 3355\n",
            "Epoch: [0]  [310/781]  eta: 0:01:08  lr: 0.000001  loss: 6.4564 (7.2242)  time: 0.1269  data: 0.0621  max mem: 3355\n",
            "Epoch: [0]  [320/781]  eta: 0:01:06  lr: 0.000001  loss: 6.4469 (7.2008)  time: 0.1323  data: 0.0659  max mem: 3355\n",
            "Epoch: [0]  [330/781]  eta: 0:01:05  lr: 0.000001  loss: 6.4422 (7.1783)  time: 0.1339  data: 0.0654  max mem: 3355\n",
            "Epoch: [0]  [340/781]  eta: 0:01:03  lr: 0.000001  loss: 6.4422 (7.1569)  time: 0.1313  data: 0.0627  max mem: 3355\n",
            "Epoch: [0]  [350/781]  eta: 0:01:02  lr: 0.000001  loss: 6.4154 (7.1359)  time: 0.1328  data: 0.0649  max mem: 3355\n",
            "Epoch: [0]  [360/781]  eta: 0:01:00  lr: 0.000001  loss: 6.4081 (7.1158)  time: 0.1360  data: 0.0691  max mem: 3355\n",
            "Epoch: [0]  [370/781]  eta: 0:00:59  lr: 0.000001  loss: 6.3827 (7.0958)  time: 0.1431  data: 0.0767  max mem: 3355\n",
            "Epoch: [0]  [380/781]  eta: 0:00:57  lr: 0.000001  loss: 6.3782 (7.0766)  time: 0.1402  data: 0.0713  max mem: 3355\n",
            "Epoch: [0]  [390/781]  eta: 0:00:55  lr: 0.000001  loss: 6.3736 (7.0585)  time: 0.1291  data: 0.0610  max mem: 3355\n",
            "Epoch: [0]  [400/781]  eta: 0:00:54  lr: 0.000001  loss: 6.3523 (7.0406)  time: 0.1234  data: 0.0582  max mem: 3355\n",
            "Epoch: [0]  [410/781]  eta: 0:00:52  lr: 0.000001  loss: 6.3604 (7.0247)  time: 0.1334  data: 0.0681  max mem: 3355\n",
            "Epoch: [0]  [420/781]  eta: 0:00:51  lr: 0.000001  loss: 6.3659 (7.0087)  time: 0.1301  data: 0.0645  max mem: 3355\n",
            "Epoch: [0]  [430/781]  eta: 0:00:49  lr: 0.000001  loss: 6.3285 (6.9926)  time: 0.1285  data: 0.0619  max mem: 3355\n",
            "Epoch: [0]  [440/781]  eta: 0:00:48  lr: 0.000001  loss: 6.3180 (6.9773)  time: 0.1275  data: 0.0611  max mem: 3355\n",
            "Epoch: [0]  [450/781]  eta: 0:00:46  lr: 0.000001  loss: 6.3180 (6.9627)  time: 0.1307  data: 0.0651  max mem: 3355\n",
            "Epoch: [0]  [460/781]  eta: 0:00:45  lr: 0.000001  loss: 6.2888 (6.9478)  time: 0.1309  data: 0.0630  max mem: 3355\n",
            "Epoch: [0]  [470/781]  eta: 0:00:43  lr: 0.000001  loss: 6.2775 (6.9336)  time: 0.1389  data: 0.0679  max mem: 3355\n",
            "Epoch: [0]  [480/781]  eta: 0:00:42  lr: 0.000001  loss: 6.2703 (6.9195)  time: 0.1429  data: 0.0734  max mem: 3355\n",
            "Epoch: [0]  [490/781]  eta: 0:00:41  lr: 0.000001  loss: 6.2703 (6.9061)  time: 0.1354  data: 0.0682  max mem: 3355\n",
            "Epoch: [0]  [500/781]  eta: 0:00:39  lr: 0.000001  loss: 6.2584 (6.8931)  time: 0.1274  data: 0.0596  max mem: 3355\n",
            "Epoch: [0]  [510/781]  eta: 0:00:38  lr: 0.000001  loss: 6.2410 (6.8806)  time: 0.1285  data: 0.0618  max mem: 3355\n",
            "Epoch: [0]  [520/781]  eta: 0:00:36  lr: 0.000001  loss: 6.2575 (6.8692)  time: 0.1336  data: 0.0676  max mem: 3355\n",
            "Epoch: [0]  [530/781]  eta: 0:00:35  lr: 0.000001  loss: 6.2575 (6.8575)  time: 0.1398  data: 0.0731  max mem: 3355\n",
            "Epoch: [0]  [540/781]  eta: 0:00:33  lr: 0.000001  loss: 6.2455 (6.8461)  time: 0.1359  data: 0.0698  max mem: 3355\n",
            "Epoch: [0]  [550/781]  eta: 0:00:32  lr: 0.000001  loss: 6.2323 (6.8349)  time: 0.1326  data: 0.0655  max mem: 3355\n",
            "Epoch: [0]  [560/781]  eta: 0:00:30  lr: 0.000001  loss: 6.2082 (6.8237)  time: 0.1360  data: 0.0674  max mem: 3355\n",
            "Epoch: [0]  [570/781]  eta: 0:00:29  lr: 0.000001  loss: 6.2054 (6.8129)  time: 0.1378  data: 0.0677  max mem: 3355\n",
            "Epoch: [0]  [580/781]  eta: 0:00:28  lr: 0.000001  loss: 6.1873 (6.8022)  time: 0.1328  data: 0.0639  max mem: 3355\n",
            "Epoch: [0]  [590/781]  eta: 0:00:26  lr: 0.000001  loss: 6.1892 (6.7920)  time: 0.1295  data: 0.0630  max mem: 3355\n",
            "Epoch: [0]  [600/781]  eta: 0:00:25  lr: 0.000001  loss: 6.2120 (6.7824)  time: 0.1339  data: 0.0668  max mem: 3355\n",
            "Epoch: [0]  [610/781]  eta: 0:00:23  lr: 0.000001  loss: 6.1999 (6.7724)  time: 0.1289  data: 0.0624  max mem: 3355\n",
            "Epoch: [0]  [620/781]  eta: 0:00:22  lr: 0.000001  loss: 6.1722 (6.7628)  time: 0.1287  data: 0.0607  max mem: 3355\n",
            "Epoch: [0]  [630/781]  eta: 0:00:21  lr: 0.000001  loss: 6.1722 (6.7533)  time: 0.1283  data: 0.0603  max mem: 3355\n",
            "Epoch: [0]  [640/781]  eta: 0:00:19  lr: 0.000001  loss: 6.1693 (6.7443)  time: 0.1264  data: 0.0605  max mem: 3355\n",
            "Epoch: [0]  [650/781]  eta: 0:00:18  lr: 0.000001  loss: 6.1842 (6.7357)  time: 0.1331  data: 0.0647  max mem: 3355\n",
            "Epoch: [0]  [660/781]  eta: 0:00:16  lr: 0.000001  loss: 6.1496 (6.7267)  time: 0.1359  data: 0.0657  max mem: 3355\n",
            "Epoch: [0]  [670/781]  eta: 0:00:15  lr: 0.000001  loss: 6.1359 (6.7181)  time: 0.1352  data: 0.0660  max mem: 3355\n",
            "Epoch: [0]  [680/781]  eta: 0:00:13  lr: 0.000001  loss: 6.1521 (6.7100)  time: 0.1329  data: 0.0656  max mem: 3355\n",
            "Epoch: [0]  [690/781]  eta: 0:00:12  lr: 0.000001  loss: 6.1248 (6.7014)  time: 0.1336  data: 0.0677  max mem: 3355\n",
            "Epoch: [0]  [700/781]  eta: 0:00:11  lr: 0.000001  loss: 6.1368 (6.6937)  time: 0.1342  data: 0.0679  max mem: 3355\n",
            "Epoch: [0]  [710/781]  eta: 0:00:09  lr: 0.000001  loss: 6.1434 (6.6857)  time: 0.1328  data: 0.0656  max mem: 3355\n",
            "Epoch: [0]  [720/781]  eta: 0:00:08  lr: 0.000001  loss: 6.1158 (6.6777)  time: 0.1322  data: 0.0643  max mem: 3355\n",
            "Epoch: [0]  [730/781]  eta: 0:00:07  lr: 0.000001  loss: 6.1158 (6.6706)  time: 0.1226  data: 0.0559  max mem: 3355\n",
            "Epoch: [0]  [740/781]  eta: 0:00:05  lr: 0.000001  loss: 6.1592 (6.6634)  time: 0.1270  data: 0.0589  max mem: 3355\n",
            "Epoch: [0]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 6.1118 (6.6560)  time: 0.1320  data: 0.0618  max mem: 3355\n",
            "Epoch: [0]  [760/781]  eta: 0:00:02  lr: 0.000001  loss: 6.1002 (6.6488)  time: 0.1272  data: 0.0595  max mem: 3355\n",
            "Epoch: [0]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 6.0975 (6.6417)  time: 0.1336  data: 0.0648  max mem: 3355\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 6.0859 (6.6346)  time: 0.1336  data: 0.0656  max mem: 3355\n",
            "Epoch: [0] Total time: 0:01:47 (0.1377 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 6.0859 (6.6346)\n",
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Test:  [ 0/53]  eta: 0:00:51  loss: 6.2612 (6.2612)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.9763  data: 0.8613  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 5.6581 (5.6785)  acc1: 0.0000 (0.2841)  acc5: 2.0833 (2.9356)  time: 0.1805  data: 0.1426  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 5.6958 (5.7334)  acc1: 0.0000 (0.5952)  acc5: 2.0833 (3.3482)  time: 0.1242  data: 0.0945  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.7621 (5.7956)  acc1: 0.0000 (0.5040)  acc5: 2.6042 (3.0578)  time: 0.1280  data: 0.0987  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.7441 (5.7959)  acc1: 0.0000 (0.9527)  acc5: 2.6042 (3.9888)  time: 0.1299  data: 0.1006  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 5.9975 (5.8669)  acc1: 0.0000 (0.7761)  acc5: 0.5208 (3.2373)  time: 0.1312  data: 0.1020  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 6.0846 (5.8842)  acc1: 0.0000 (0.7600)  acc5: 0.0000 (3.1700)  time: 0.1104  data: 0.0814  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1372 s / it)\n",
            "* Acc@1 0.760 Acc@5 3.170 loss 5.884\n",
            "Accuracy of the network on the 10000 test images: 0.8%\n",
            "Max accuracy: 0.76%\n",
            "Epoch: [1]  [  0/781]  eta: 0:11:54  lr: 0.000001  loss: 6.1161 (6.1161)  time: 0.9153  data: 0.8192  max mem: 3355\n",
            "Epoch: [1]  [ 10/781]  eta: 0:02:24  lr: 0.000001  loss: 6.0859 (6.0910)  time: 0.1869  data: 0.1146  max mem: 3355\n",
            "Epoch: [1]  [ 20/781]  eta: 0:02:03  lr: 0.000001  loss: 6.1007 (6.0969)  time: 0.1247  data: 0.0567  max mem: 3355\n",
            "Epoch: [1]  [ 30/781]  eta: 0:01:56  lr: 0.000001  loss: 6.1007 (6.0913)  time: 0.1379  data: 0.0704  max mem: 3355\n",
            "Epoch: [1]  [ 40/781]  eta: 0:01:50  lr: 0.000001  loss: 6.0712 (6.0869)  time: 0.1358  data: 0.0670  max mem: 3355\n",
            "Epoch: [1]  [ 50/781]  eta: 0:01:49  lr: 0.000001  loss: 6.0712 (6.0895)  time: 0.1425  data: 0.0735  max mem: 3355\n",
            "Epoch: [1]  [ 60/781]  eta: 0:01:45  lr: 0.000001  loss: 6.0690 (6.0861)  time: 0.1410  data: 0.0713  max mem: 3355\n",
            "Epoch: [1]  [ 70/781]  eta: 0:01:44  lr: 0.000001  loss: 6.0839 (6.0871)  time: 0.1394  data: 0.0701  max mem: 3355\n",
            "Epoch: [1]  [ 80/781]  eta: 0:01:42  lr: 0.000001  loss: 6.0501 (6.0803)  time: 0.1434  data: 0.0739  max mem: 3355\n",
            "Epoch: [1]  [ 90/781]  eta: 0:01:41  lr: 0.000001  loss: 6.0472 (6.0776)  time: 0.1464  data: 0.0764  max mem: 3355\n",
            "Epoch: [1]  [100/781]  eta: 0:01:38  lr: 0.000001  loss: 6.0595 (6.0775)  time: 0.1414  data: 0.0732  max mem: 3355\n",
            "Epoch: [1]  [110/781]  eta: 0:01:37  lr: 0.000001  loss: 6.0737 (6.0760)  time: 0.1397  data: 0.0717  max mem: 3355\n",
            "Epoch: [1]  [120/781]  eta: 0:01:35  lr: 0.000001  loss: 6.0572 (6.0735)  time: 0.1444  data: 0.0757  max mem: 3355\n",
            "Epoch: [1]  [130/781]  eta: 0:01:34  lr: 0.000001  loss: 6.0400 (6.0703)  time: 0.1435  data: 0.0766  max mem: 3355\n",
            "Epoch: [1]  [140/781]  eta: 0:01:32  lr: 0.000001  loss: 6.0240 (6.0676)  time: 0.1407  data: 0.0741  max mem: 3355\n",
            "Epoch: [1]  [150/781]  eta: 0:01:31  lr: 0.000001  loss: 6.0402 (6.0653)  time: 0.1433  data: 0.0769  max mem: 3355\n",
            "Epoch: [1]  [160/781]  eta: 0:01:29  lr: 0.000001  loss: 6.0313 (6.0641)  time: 0.1411  data: 0.0757  max mem: 3355\n",
            "Epoch: [1]  [170/781]  eta: 0:01:28  lr: 0.000001  loss: 6.0302 (6.0629)  time: 0.1424  data: 0.0733  max mem: 3355\n",
            "Epoch: [1]  [180/781]  eta: 0:01:26  lr: 0.000001  loss: 6.0302 (6.0604)  time: 0.1457  data: 0.0732  max mem: 3355\n",
            "Epoch: [1]  [190/781]  eta: 0:01:25  lr: 0.000001  loss: 6.0033 (6.0567)  time: 0.1446  data: 0.0753  max mem: 3355\n",
            "Epoch: [1]  [200/781]  eta: 0:01:23  lr: 0.000001  loss: 6.0222 (6.0564)  time: 0.1420  data: 0.0746  max mem: 3355\n",
            "Epoch: [1]  [210/781]  eta: 0:01:22  lr: 0.000001  loss: 6.0362 (6.0544)  time: 0.1385  data: 0.0716  max mem: 3355\n",
            "Epoch: [1]  [220/781]  eta: 0:01:20  lr: 0.000001  loss: 6.0089 (6.0528)  time: 0.1389  data: 0.0735  max mem: 3355\n",
            "Epoch: [1]  [230/781]  eta: 0:01:19  lr: 0.000001  loss: 6.0078 (6.0496)  time: 0.1428  data: 0.0773  max mem: 3355\n",
            "Epoch: [1]  [240/781]  eta: 0:01:17  lr: 0.000001  loss: 5.9720 (6.0468)  time: 0.1422  data: 0.0765  max mem: 3355\n",
            "Epoch: [1]  [250/781]  eta: 0:01:16  lr: 0.000001  loss: 5.9738 (6.0443)  time: 0.1436  data: 0.0765  max mem: 3355\n",
            "Epoch: [1]  [260/781]  eta: 0:01:14  lr: 0.000001  loss: 5.9794 (6.0420)  time: 0.1447  data: 0.0748  max mem: 3355\n",
            "Epoch: [1]  [270/781]  eta: 0:01:13  lr: 0.000001  loss: 5.9794 (6.0398)  time: 0.1347  data: 0.0648  max mem: 3355\n",
            "Epoch: [1]  [280/781]  eta: 0:01:11  lr: 0.000001  loss: 5.9831 (6.0382)  time: 0.1265  data: 0.0594  max mem: 3355\n",
            "Epoch: [1]  [290/781]  eta: 0:01:09  lr: 0.000001  loss: 5.9860 (6.0370)  time: 0.1238  data: 0.0575  max mem: 3355\n",
            "Epoch: [1]  [300/781]  eta: 0:01:07  lr: 0.000001  loss: 5.9860 (6.0349)  time: 0.1257  data: 0.0594  max mem: 3355\n",
            "Epoch: [1]  [310/781]  eta: 0:01:06  lr: 0.000001  loss: 5.9770 (6.0333)  time: 0.1308  data: 0.0538  max mem: 3355\n",
            "Epoch: [1]  [320/781]  eta: 0:01:04  lr: 0.000001  loss: 5.9849 (6.0325)  time: 0.1312  data: 0.0543  max mem: 3355\n",
            "Epoch: [1]  [330/781]  eta: 0:01:03  lr: 0.000001  loss: 5.9847 (6.0309)  time: 0.1303  data: 0.0641  max mem: 3355\n",
            "Epoch: [1]  [340/781]  eta: 0:01:01  lr: 0.000001  loss: 5.9731 (6.0299)  time: 0.1301  data: 0.0632  max mem: 3355\n",
            "Epoch: [1]  [350/781]  eta: 0:01:00  lr: 0.000001  loss: 5.9714 (6.0284)  time: 0.1331  data: 0.0652  max mem: 3355\n",
            "Epoch: [1]  [360/781]  eta: 0:00:58  lr: 0.000001  loss: 5.9564 (6.0265)  time: 0.1359  data: 0.0660  max mem: 3355\n",
            "Epoch: [1]  [370/781]  eta: 0:00:57  lr: 0.000001  loss: 5.9471 (6.0240)  time: 0.1373  data: 0.0675  max mem: 3355\n",
            "Epoch: [1]  [380/781]  eta: 0:00:55  lr: 0.000001  loss: 5.9355 (6.0221)  time: 0.1371  data: 0.0680  max mem: 3355\n",
            "Epoch: [1]  [390/781]  eta: 0:00:54  lr: 0.000001  loss: 5.9548 (6.0210)  time: 0.1341  data: 0.0635  max mem: 3355\n",
            "Epoch: [1]  [400/781]  eta: 0:00:53  lr: 0.000001  loss: 5.9619 (6.0195)  time: 0.1319  data: 0.0610  max mem: 3355\n",
            "Epoch: [1]  [410/781]  eta: 0:00:51  lr: 0.000001  loss: 5.9520 (6.0178)  time: 0.1241  data: 0.0531  max mem: 3355\n",
            "Epoch: [1]  [420/781]  eta: 0:00:49  lr: 0.000001  loss: 5.9554 (6.0163)  time: 0.1216  data: 0.0524  max mem: 3355\n",
            "Epoch: [1]  [430/781]  eta: 0:00:48  lr: 0.000001  loss: 5.9554 (6.0148)  time: 0.1248  data: 0.0584  max mem: 3355\n",
            "Epoch: [1]  [440/781]  eta: 0:00:47  lr: 0.000001  loss: 5.9536 (6.0131)  time: 0.1274  data: 0.0609  max mem: 3355\n",
            "Epoch: [1]  [450/781]  eta: 0:00:45  lr: 0.000001  loss: 5.9577 (6.0122)  time: 0.1298  data: 0.0618  max mem: 3355\n",
            "Epoch: [1]  [460/781]  eta: 0:00:44  lr: 0.000001  loss: 5.9651 (6.0111)  time: 0.1374  data: 0.0691  max mem: 3355\n",
            "Epoch: [1]  [470/781]  eta: 0:00:42  lr: 0.000001  loss: 5.9498 (6.0096)  time: 0.1260  data: 0.0576  max mem: 3355\n",
            "Epoch: [1]  [480/781]  eta: 0:00:41  lr: 0.000001  loss: 5.9408 (6.0080)  time: 0.1322  data: 0.0638  max mem: 3355\n",
            "Epoch: [1]  [490/781]  eta: 0:00:39  lr: 0.000001  loss: 5.9435 (6.0069)  time: 0.1331  data: 0.0640  max mem: 3355\n",
            "Epoch: [1]  [500/781]  eta: 0:00:38  lr: 0.000001  loss: 5.9612 (6.0060)  time: 0.1406  data: 0.0718  max mem: 3355\n",
            "Epoch: [1]  [510/781]  eta: 0:00:37  lr: 0.000001  loss: 5.9358 (6.0043)  time: 0.1410  data: 0.0741  max mem: 3355\n",
            "Epoch: [1]  [520/781]  eta: 0:00:35  lr: 0.000001  loss: 5.9150 (6.0030)  time: 0.1298  data: 0.0631  max mem: 3355\n",
            "Epoch: [1]  [530/781]  eta: 0:00:34  lr: 0.000001  loss: 5.9286 (6.0018)  time: 0.1332  data: 0.0659  max mem: 3355\n",
            "Epoch: [1]  [540/781]  eta: 0:00:33  lr: 0.000001  loss: 5.9519 (6.0006)  time: 0.1440  data: 0.0758  max mem: 3355\n",
            "Epoch: [1]  [550/781]  eta: 0:00:31  lr: 0.000001  loss: 5.9257 (5.9991)  time: 0.1409  data: 0.0717  max mem: 3355\n",
            "Epoch: [1]  [560/781]  eta: 0:00:30  lr: 0.000001  loss: 5.9031 (5.9976)  time: 0.1312  data: 0.0627  max mem: 3355\n",
            "Epoch: [1]  [570/781]  eta: 0:00:28  lr: 0.000001  loss: 5.9031 (5.9964)  time: 0.1372  data: 0.0709  max mem: 3355\n",
            "Epoch: [1]  [580/781]  eta: 0:00:27  lr: 0.000001  loss: 5.9301 (5.9954)  time: 0.1282  data: 0.0630  max mem: 3355\n",
            "Epoch: [1]  [590/781]  eta: 0:00:26  lr: 0.000001  loss: 5.9312 (5.9945)  time: 0.1346  data: 0.0681  max mem: 3355\n",
            "Epoch: [1]  [600/781]  eta: 0:00:24  lr: 0.000001  loss: 5.9304 (5.9934)  time: 0.1345  data: 0.0673  max mem: 3355\n",
            "Epoch: [1]  [610/781]  eta: 0:00:23  lr: 0.000001  loss: 5.9181 (5.9922)  time: 0.1267  data: 0.0606  max mem: 3355\n",
            "Epoch: [1]  [620/781]  eta: 0:00:22  lr: 0.000001  loss: 5.9333 (5.9912)  time: 0.1287  data: 0.0633  max mem: 3355\n",
            "Epoch: [1]  [630/781]  eta: 0:00:20  lr: 0.000001  loss: 5.9350 (5.9901)  time: 0.1299  data: 0.0618  max mem: 3355\n",
            "Epoch: [1]  [640/781]  eta: 0:00:19  lr: 0.000001  loss: 5.8771 (5.9883)  time: 0.1484  data: 0.0763  max mem: 3355\n",
            "Epoch: [1]  [650/781]  eta: 0:00:17  lr: 0.000001  loss: 5.8796 (5.9871)  time: 0.1414  data: 0.0714  max mem: 3355\n",
            "Epoch: [1]  [660/781]  eta: 0:00:16  lr: 0.000001  loss: 5.8894 (5.9856)  time: 0.1287  data: 0.0625  max mem: 3355\n",
            "Epoch: [1]  [670/781]  eta: 0:00:15  lr: 0.000001  loss: 5.8845 (5.9841)  time: 0.1255  data: 0.0592  max mem: 3355\n",
            "Epoch: [1]  [680/781]  eta: 0:00:13  lr: 0.000001  loss: 5.8796 (5.9827)  time: 0.1321  data: 0.0656  max mem: 3355\n",
            "Epoch: [1]  [690/781]  eta: 0:00:12  lr: 0.000001  loss: 5.8858 (5.9812)  time: 0.1347  data: 0.0684  max mem: 3355\n",
            "Epoch: [1]  [700/781]  eta: 0:00:11  lr: 0.000001  loss: 5.8703 (5.9795)  time: 0.1311  data: 0.0642  max mem: 3355\n",
            "Epoch: [1]  [710/781]  eta: 0:00:09  lr: 0.000001  loss: 5.8892 (5.9784)  time: 0.1311  data: 0.0648  max mem: 3355\n",
            "Epoch: [1]  [720/781]  eta: 0:00:08  lr: 0.000001  loss: 5.9005 (5.9771)  time: 0.1386  data: 0.0723  max mem: 3355\n",
            "Epoch: [1]  [730/781]  eta: 0:00:06  lr: 0.000001  loss: 5.8662 (5.9759)  time: 0.1457  data: 0.0766  max mem: 3355\n",
            "Epoch: [1]  [740/781]  eta: 0:00:05  lr: 0.000001  loss: 5.8925 (5.9749)  time: 0.1391  data: 0.0697  max mem: 3355\n",
            "Epoch: [1]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 5.8956 (5.9737)  time: 0.1336  data: 0.0644  max mem: 3355\n",
            "Epoch: [1]  [760/781]  eta: 0:00:02  lr: 0.000001  loss: 5.8845 (5.9723)  time: 0.1327  data: 0.0625  max mem: 3355\n",
            "Epoch: [1]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 5.8677 (5.9709)  time: 0.1312  data: 0.0638  max mem: 3355\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 5.8871 (5.9701)  time: 0.1297  data: 0.0644  max mem: 3355\n",
            "Epoch: [1] Total time: 0:01:46 (0.1365 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 5.8871 (5.9701)\n",
            "Test:  [ 0/53]  eta: 0:00:47  loss: 5.7804 (5.7804)  acc1: 0.0000 (0.0000)  acc5: 1.0417 (1.0417)  time: 0.9015  data: 0.8719  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 5.4132 (5.4923)  acc1: 0.0000 (0.4735)  acc5: 2.0833 (3.6458)  time: 0.1785  data: 0.1480  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 5.4132 (5.4713)  acc1: 0.0000 (0.7688)  acc5: 3.1250 (4.1171)  time: 0.1252  data: 0.0953  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.4934 (5.5375)  acc1: 0.0000 (0.7728)  acc5: 2.6042 (3.8810)  time: 0.1334  data: 0.1041  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.4934 (5.5423)  acc1: 0.5208 (1.0290)  acc5: 3.1250 (4.5732)  time: 0.1387  data: 0.1094  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 5.5317 (5.5612)  acc1: 0.5208 (0.8783)  acc5: 2.6042 (4.1054)  time: 0.1304  data: 0.1012  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 5.6186 (5.5752)  acc1: 0.0000 (0.8600)  acc5: 2.0833 (4.1700)  time: 0.1084  data: 0.0800  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1383 s / it)\n",
            "* Acc@1 0.860 Acc@5 4.170 loss 5.575\n",
            "Accuracy of the network on the 10000 test images: 0.9%\n",
            "Max accuracy: 0.86%\n",
            "Epoch: [2]  [  0/781]  eta: 0:11:44  lr: 0.000032  loss: 5.8721 (5.8721)  time: 0.9025  data: 0.8255  max mem: 3355\n",
            "Epoch: [2]  [ 10/781]  eta: 0:02:28  lr: 0.000032  loss: 5.8629 (5.8609)  time: 0.1921  data: 0.1227  max mem: 3355\n",
            "Epoch: [2]  [ 20/781]  eta: 0:02:18  lr: 0.000032  loss: 5.8433 (5.8521)  time: 0.1464  data: 0.0780  max mem: 3355\n",
            "Epoch: [2]  [ 30/781]  eta: 0:02:05  lr: 0.000032  loss: 5.8155 (5.8342)  time: 0.1526  data: 0.0842  max mem: 3355\n",
            "Epoch: [2]  [ 40/781]  eta: 0:02:01  lr: 0.000032  loss: 5.7858 (5.8129)  time: 0.1453  data: 0.0772  max mem: 3355\n",
            "Epoch: [2]  [ 50/781]  eta: 0:01:53  lr: 0.000032  loss: 5.7186 (5.7923)  time: 0.1378  data: 0.0700  max mem: 3355\n",
            "Epoch: [2]  [ 60/781]  eta: 0:01:51  lr: 0.000032  loss: 5.6942 (5.7719)  time: 0.1345  data: 0.0637  max mem: 3355\n",
            "Epoch: [2]  [ 70/781]  eta: 0:01:47  lr: 0.000032  loss: 5.6543 (5.7537)  time: 0.1399  data: 0.0665  max mem: 3355\n",
            "Epoch: [2]  [ 80/781]  eta: 0:01:45  lr: 0.000032  loss: 5.6040 (5.7332)  time: 0.1410  data: 0.0706  max mem: 3355\n",
            "Epoch: [2]  [ 90/781]  eta: 0:01:41  lr: 0.000032  loss: 5.5418 (5.7101)  time: 0.1323  data: 0.0637  max mem: 3355\n",
            "Epoch: [2]  [100/781]  eta: 0:01:39  lr: 0.000032  loss: 5.4869 (5.6869)  time: 0.1250  data: 0.0565  max mem: 3355\n",
            "Epoch: [2]  [110/781]  eta: 0:01:36  lr: 0.000032  loss: 5.4643 (5.6644)  time: 0.1281  data: 0.0616  max mem: 3355\n",
            "Epoch: [2]  [120/781]  eta: 0:01:35  lr: 0.000032  loss: 5.4240 (5.6485)  time: 0.1353  data: 0.0687  max mem: 3355\n",
            "Epoch: [2]  [130/781]  eta: 0:01:32  lr: 0.000032  loss: 5.3969 (5.6301)  time: 0.1391  data: 0.0707  max mem: 3355\n",
            "Epoch: [2]  [140/781]  eta: 0:01:32  lr: 0.000032  loss: 5.3189 (5.6089)  time: 0.1412  data: 0.0726  max mem: 3355\n",
            "Epoch: [2]  [150/781]  eta: 0:01:29  lr: 0.000032  loss: 5.2336 (5.5867)  time: 0.1388  data: 0.0713  max mem: 3355\n",
            "Epoch: [2]  [160/781]  eta: 0:01:29  lr: 0.000032  loss: 5.2261 (5.5721)  time: 0.1427  data: 0.0758  max mem: 3355\n",
            "Epoch: [2]  [170/781]  eta: 0:01:26  lr: 0.000032  loss: 5.2236 (5.5521)  time: 0.1408  data: 0.0750  max mem: 3355\n",
            "Epoch: [2]  [180/781]  eta: 0:01:25  lr: 0.000032  loss: 5.1614 (5.5271)  time: 0.1362  data: 0.0711  max mem: 3355\n",
            "Epoch: [2]  [190/781]  eta: 0:01:23  lr: 0.000032  loss: 5.1220 (5.5056)  time: 0.1417  data: 0.0756  max mem: 3355\n",
            "Epoch: [2]  [200/781]  eta: 0:01:23  lr: 0.000032  loss: 5.0961 (5.4851)  time: 0.1454  data: 0.0771  max mem: 3355\n",
            "Epoch: [2]  [210/781]  eta: 0:01:20  lr: 0.000032  loss: 5.0299 (5.4619)  time: 0.1393  data: 0.0713  max mem: 3355\n",
            "Epoch: [2]  [220/781]  eta: 0:01:19  lr: 0.000032  loss: 4.9142 (5.4361)  time: 0.1330  data: 0.0657  max mem: 3355\n",
            "Epoch: [2]  [230/781]  eta: 0:01:17  lr: 0.000032  loss: 4.8946 (5.4149)  time: 0.1322  data: 0.0657  max mem: 3355\n",
            "Epoch: [2]  [240/781]  eta: 0:01:16  lr: 0.000032  loss: 4.8636 (5.3966)  time: 0.1346  data: 0.0653  max mem: 3355\n",
            "Epoch: [2]  [250/781]  eta: 0:01:14  lr: 0.000032  loss: 4.8636 (5.3780)  time: 0.1365  data: 0.0641  max mem: 3355\n",
            "Epoch: [2]  [260/781]  eta: 0:01:13  lr: 0.000032  loss: 4.8462 (5.3579)  time: 0.1321  data: 0.0631  max mem: 3355\n",
            "Epoch: [2]  [270/781]  eta: 0:01:11  lr: 0.000032  loss: 4.7713 (5.3368)  time: 0.1341  data: 0.0663  max mem: 3355\n",
            "Epoch: [2]  [280/781]  eta: 0:01:10  lr: 0.000032  loss: 4.7648 (5.3199)  time: 0.1312  data: 0.0619  max mem: 3355\n",
            "Epoch: [2]  [290/781]  eta: 0:01:08  lr: 0.000032  loss: 4.7648 (5.3022)  time: 0.1294  data: 0.0610  max mem: 3355\n",
            "Epoch: [2]  [300/781]  eta: 0:01:07  lr: 0.000032  loss: 4.6632 (5.2795)  time: 0.1386  data: 0.0699  max mem: 3355\n",
            "Epoch: [2]  [310/781]  eta: 0:01:05  lr: 0.000032  loss: 4.6231 (5.2601)  time: 0.1404  data: 0.0714  max mem: 3355\n",
            "Epoch: [2]  [320/781]  eta: 0:01:04  lr: 0.000032  loss: 4.6225 (5.2389)  time: 0.1304  data: 0.0629  max mem: 3355\n",
            "Epoch: [2]  [330/781]  eta: 0:01:02  lr: 0.000032  loss: 4.5367 (5.2210)  time: 0.1305  data: 0.0628  max mem: 3355\n",
            "Epoch: [2]  [340/781]  eta: 0:01:01  lr: 0.000032  loss: 4.5418 (5.2027)  time: 0.1374  data: 0.0677  max mem: 3355\n",
            "Epoch: [2]  [350/781]  eta: 0:00:59  lr: 0.000032  loss: 4.5673 (5.1856)  time: 0.1369  data: 0.0670  max mem: 3355\n",
            "Epoch: [2]  [360/781]  eta: 0:00:58  lr: 0.000032  loss: 4.5035 (5.1682)  time: 0.1312  data: 0.0640  max mem: 3355\n",
            "Epoch: [2]  [370/781]  eta: 0:00:56  lr: 0.000032  loss: 4.4546 (5.1494)  time: 0.1307  data: 0.0653  max mem: 3355\n",
            "Epoch: [2]  [380/781]  eta: 0:00:55  lr: 0.000032  loss: 4.4883 (5.1362)  time: 0.1343  data: 0.0670  max mem: 3355\n",
            "Epoch: [2]  [390/781]  eta: 0:00:53  lr: 0.000032  loss: 4.5346 (5.1253)  time: 0.1305  data: 0.0633  max mem: 3355\n",
            "Epoch: [2]  [400/781]  eta: 0:00:52  lr: 0.000032  loss: 4.5151 (5.1133)  time: 0.1237  data: 0.0580  max mem: 3355\n",
            "Epoch: [2]  [410/781]  eta: 0:00:50  lr: 0.000032  loss: 4.4215 (5.0959)  time: 0.1261  data: 0.0591  max mem: 3355\n",
            "Epoch: [2]  [420/781]  eta: 0:00:49  lr: 0.000032  loss: 4.3984 (5.0832)  time: 0.1308  data: 0.0631  max mem: 3355\n",
            "Epoch: [2]  [430/781]  eta: 0:00:48  lr: 0.000032  loss: 4.4018 (5.0705)  time: 0.1349  data: 0.0670  max mem: 3355\n",
            "Epoch: [2]  [440/781]  eta: 0:00:46  lr: 0.000032  loss: 4.2815 (5.0536)  time: 0.1392  data: 0.0691  max mem: 3355\n",
            "Epoch: [2]  [450/781]  eta: 0:00:45  lr: 0.000032  loss: 4.2425 (5.0393)  time: 0.1330  data: 0.0629  max mem: 3355\n",
            "Epoch: [2]  [460/781]  eta: 0:00:44  lr: 0.000032  loss: 4.2875 (5.0267)  time: 0.1313  data: 0.0644  max mem: 3355\n",
            "Epoch: [2]  [470/781]  eta: 0:00:42  lr: 0.000032  loss: 4.2485 (5.0115)  time: 0.1354  data: 0.0689  max mem: 3355\n",
            "Epoch: [2]  [480/781]  eta: 0:00:41  lr: 0.000032  loss: 4.1807 (4.9969)  time: 0.1289  data: 0.0628  max mem: 3355\n",
            "Epoch: [2]  [490/781]  eta: 0:00:39  lr: 0.000032  loss: 4.1860 (4.9820)  time: 0.1270  data: 0.0616  max mem: 3355\n",
            "Epoch: [2]  [500/781]  eta: 0:00:38  lr: 0.000032  loss: 4.1916 (4.9690)  time: 0.1354  data: 0.0700  max mem: 3355\n",
            "Epoch: [2]  [510/781]  eta: 0:00:36  lr: 0.000032  loss: 4.1392 (4.9527)  time: 0.1338  data: 0.0689  max mem: 3355\n",
            "Epoch: [2]  [520/781]  eta: 0:00:35  lr: 0.000032  loss: 4.0951 (4.9353)  time: 0.1301  data: 0.0629  max mem: 3355\n",
            "Epoch: [2]  [530/781]  eta: 0:00:34  lr: 0.000032  loss: 4.0185 (4.9205)  time: 0.1333  data: 0.0634  max mem: 3355\n",
            "Epoch: [2]  [540/781]  eta: 0:00:32  lr: 0.000032  loss: 4.0653 (4.9067)  time: 0.1324  data: 0.0638  max mem: 3355\n",
            "Epoch: [2]  [550/781]  eta: 0:00:31  lr: 0.000032  loss: 4.0371 (4.8949)  time: 0.1338  data: 0.0662  max mem: 3355\n",
            "Epoch: [2]  [560/781]  eta: 0:00:30  lr: 0.000032  loss: 4.0318 (4.8820)  time: 0.1329  data: 0.0664  max mem: 3355\n",
            "Epoch: [2]  [570/781]  eta: 0:00:28  lr: 0.000032  loss: 4.0546 (4.8677)  time: 0.1320  data: 0.0668  max mem: 3355\n",
            "Epoch: [2]  [580/781]  eta: 0:00:27  lr: 0.000032  loss: 4.0195 (4.8524)  time: 0.1340  data: 0.0680  max mem: 3355\n",
            "Epoch: [2]  [590/781]  eta: 0:00:25  lr: 0.000032  loss: 3.9760 (4.8415)  time: 0.1320  data: 0.0654  max mem: 3355\n",
            "Epoch: [2]  [600/781]  eta: 0:00:24  lr: 0.000032  loss: 3.9368 (4.8263)  time: 0.1321  data: 0.0657  max mem: 3355\n",
            "Epoch: [2]  [610/781]  eta: 0:00:23  lr: 0.000032  loss: 3.9572 (4.8199)  time: 0.1310  data: 0.0638  max mem: 3355\n",
            "Epoch: [2]  [620/781]  eta: 0:00:21  lr: 0.000032  loss: 4.0665 (4.8094)  time: 0.1318  data: 0.0624  max mem: 3355\n",
            "Epoch: [2]  [630/781]  eta: 0:00:20  lr: 0.000032  loss: 4.0550 (4.7977)  time: 0.1311  data: 0.0621  max mem: 3355\n",
            "Epoch: [2]  [640/781]  eta: 0:00:19  lr: 0.000032  loss: 3.9544 (4.7894)  time: 0.1309  data: 0.0634  max mem: 3355\n",
            "Epoch: [2]  [650/781]  eta: 0:00:17  lr: 0.000032  loss: 4.0366 (4.7799)  time: 0.1315  data: 0.0635  max mem: 3355\n",
            "Epoch: [2]  [660/781]  eta: 0:00:16  lr: 0.000032  loss: 3.8981 (4.7663)  time: 0.1293  data: 0.0629  max mem: 3355\n",
            "Epoch: [2]  [670/781]  eta: 0:00:15  lr: 0.000032  loss: 3.7843 (4.7523)  time: 0.1319  data: 0.0667  max mem: 3355\n",
            "Epoch: [2]  [680/781]  eta: 0:00:13  lr: 0.000032  loss: 3.8045 (4.7430)  time: 0.1335  data: 0.0680  max mem: 3355\n",
            "Epoch: [2]  [690/781]  eta: 0:00:12  lr: 0.000032  loss: 3.8927 (4.7333)  time: 0.1313  data: 0.0646  max mem: 3355\n",
            "Epoch: [2]  [700/781]  eta: 0:00:10  lr: 0.000032  loss: 3.9062 (4.7222)  time: 0.1263  data: 0.0589  max mem: 3355\n",
            "Epoch: [2]  [710/781]  eta: 0:00:09  lr: 0.000032  loss: 3.9022 (4.7104)  time: 0.1302  data: 0.0618  max mem: 3355\n",
            "Epoch: [2]  [720/781]  eta: 0:00:08  lr: 0.000032  loss: 3.8448 (4.6989)  time: 0.1379  data: 0.0667  max mem: 3355\n",
            "Epoch: [2]  [730/781]  eta: 0:00:06  lr: 0.000032  loss: 3.8560 (4.6914)  time: 0.1335  data: 0.0641  max mem: 3355\n",
            "Epoch: [2]  [740/781]  eta: 0:00:05  lr: 0.000032  loss: 3.8560 (4.6794)  time: 0.1296  data: 0.0623  max mem: 3355\n",
            "Epoch: [2]  [750/781]  eta: 0:00:04  lr: 0.000032  loss: 3.8267 (4.6683)  time: 0.1357  data: 0.0683  max mem: 3355\n",
            "Epoch: [2]  [760/781]  eta: 0:00:02  lr: 0.000032  loss: 3.8267 (4.6586)  time: 0.1345  data: 0.0672  max mem: 3355\n",
            "Epoch: [2]  [770/781]  eta: 0:00:01  lr: 0.000032  loss: 3.8098 (4.6505)  time: 0.1340  data: 0.0670  max mem: 3355\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000032  loss: 3.8098 (4.6445)  time: 0.1281  data: 0.0617  max mem: 3355\n",
            "Epoch: [2] Total time: 0:01:45 (0.1352 s / it)\n",
            "Averaged stats: lr: 0.000032  loss: 3.8098 (4.6445)\n",
            "Test:  [ 0/53]  eta: 0:00:46  loss: 1.2604 (1.2604)  acc1: 69.7917 (69.7917)  acc5: 90.1042 (90.1042)  time: 0.8728  data: 0.8433  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 2.0072 (1.8783)  acc1: 55.2083 (58.0492)  acc5: 85.4167 (83.7595)  time: 0.1809  data: 0.1517  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 2.0702 (2.1031)  acc1: 48.4375 (54.3651)  acc5: 79.1667 (80.4564)  time: 0.1317  data: 0.1024  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 2.3837 (2.1618)  acc1: 51.5625 (54.7379)  acc5: 76.5625 (80.0739)  time: 0.1383  data: 0.1090  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 2.4173 (2.2935)  acc1: 51.0417 (51.9436)  acc5: 74.4792 (77.4517)  time: 0.1362  data: 0.1069  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 2.5173 (2.3039)  acc1: 43.7500 (51.5421)  acc5: 72.9167 (76.9914)  time: 0.1378  data: 0.1082  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 2.5173 (2.3159)  acc1: 43.7500 (51.3800)  acc5: 73.4375 (77.0500)  time: 0.1209  data: 0.0920  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1435 s / it)\n",
            "* Acc@1 51.380 Acc@5 77.050 loss 2.316\n",
            "Accuracy of the network on the 10000 test images: 51.4%\n",
            "Max accuracy: 51.38%\n",
            "Epoch: [3]  [  0/781]  eta: 0:10:30  lr: 0.000057  loss: 3.4071 (3.4071)  time: 0.8071  data: 0.7231  max mem: 3355\n",
            "Epoch: [3]  [ 10/781]  eta: 0:02:23  lr: 0.000057  loss: 3.7707 (4.0225)  time: 0.1858  data: 0.1191  max mem: 3355\n",
            "Epoch: [3]  [ 20/781]  eta: 0:02:03  lr: 0.000057  loss: 3.7994 (3.9716)  time: 0.1301  data: 0.0642  max mem: 3355\n",
            "Epoch: [3]  [ 30/781]  eta: 0:01:54  lr: 0.000057  loss: 3.7015 (3.9407)  time: 0.1332  data: 0.0673  max mem: 3355\n",
            "Epoch: [3]  [ 40/781]  eta: 0:01:48  lr: 0.000057  loss: 3.7468 (3.9342)  time: 0.1289  data: 0.0637  max mem: 3355\n",
            "Epoch: [3]  [ 50/781]  eta: 0:01:46  lr: 0.000057  loss: 3.8186 (3.9517)  time: 0.1376  data: 0.0721  max mem: 3355\n",
            "Epoch: [3]  [ 60/781]  eta: 0:01:43  lr: 0.000057  loss: 3.7472 (3.9346)  time: 0.1386  data: 0.0699  max mem: 3355\n",
            "Epoch: [3]  [ 70/781]  eta: 0:01:41  lr: 0.000057  loss: 3.6067 (3.9061)  time: 0.1343  data: 0.0635  max mem: 3355\n",
            "Epoch: [3]  [ 80/781]  eta: 0:01:38  lr: 0.000057  loss: 3.6067 (3.8953)  time: 0.1292  data: 0.0614  max mem: 3355\n",
            "Epoch: [3]  [ 90/781]  eta: 0:01:37  lr: 0.000057  loss: 3.6945 (3.8934)  time: 0.1337  data: 0.0682  max mem: 3355\n",
            "Epoch: [3]  [100/781]  eta: 0:01:34  lr: 0.000057  loss: 3.7603 (3.8807)  time: 0.1319  data: 0.0664  max mem: 3355\n",
            "Epoch: [3]  [110/781]  eta: 0:01:32  lr: 0.000057  loss: 3.7683 (3.8667)  time: 0.1277  data: 0.0624  max mem: 3355\n",
            "Epoch: [3]  [120/781]  eta: 0:01:30  lr: 0.000057  loss: 3.6260 (3.8532)  time: 0.1286  data: 0.0626  max mem: 3355\n",
            "Epoch: [3]  [130/781]  eta: 0:01:29  lr: 0.000057  loss: 3.6260 (3.8452)  time: 0.1285  data: 0.0608  max mem: 3355\n",
            "Epoch: [3]  [140/781]  eta: 0:01:27  lr: 0.000057  loss: 3.6908 (3.8543)  time: 0.1320  data: 0.0636  max mem: 3355\n",
            "Epoch: [3]  [150/781]  eta: 0:01:26  lr: 0.000057  loss: 3.6908 (3.8436)  time: 0.1320  data: 0.0625  max mem: 3355\n",
            "Epoch: [3]  [160/781]  eta: 0:01:24  lr: 0.000057  loss: 3.6563 (3.8428)  time: 0.1381  data: 0.0674  max mem: 3355\n",
            "Epoch: [3]  [170/781]  eta: 0:01:22  lr: 0.000057  loss: 3.5857 (3.8340)  time: 0.1296  data: 0.0614  max mem: 3355\n",
            "Epoch: [3]  [180/781]  eta: 0:01:21  lr: 0.000057  loss: 3.5699 (3.8331)  time: 0.1260  data: 0.0601  max mem: 3355\n",
            "Epoch: [3]  [190/781]  eta: 0:01:19  lr: 0.000057  loss: 3.6189 (3.8334)  time: 0.1252  data: 0.0588  max mem: 3355\n",
            "Epoch: [3]  [200/781]  eta: 0:01:18  lr: 0.000057  loss: 3.6423 (3.8406)  time: 0.1309  data: 0.0647  max mem: 3355\n",
            "Epoch: [3]  [210/781]  eta: 0:01:16  lr: 0.000057  loss: 3.5815 (3.8345)  time: 0.1307  data: 0.0643  max mem: 3355\n",
            "Epoch: [3]  [220/781]  eta: 0:01:15  lr: 0.000057  loss: 3.5719 (3.8222)  time: 0.1301  data: 0.0636  max mem: 3355\n",
            "Epoch: [3]  [230/781]  eta: 0:01:14  lr: 0.000057  loss: 3.5835 (3.8264)  time: 0.1396  data: 0.0719  max mem: 3355\n",
            "Epoch: [3]  [240/781]  eta: 0:01:12  lr: 0.000057  loss: 3.5333 (3.8264)  time: 0.1356  data: 0.0657  max mem: 3355\n",
            "Epoch: [3]  [250/781]  eta: 0:01:11  lr: 0.000057  loss: 3.4945 (3.8213)  time: 0.1376  data: 0.0667  max mem: 3355\n",
            "Epoch: [3]  [260/781]  eta: 0:01:10  lr: 0.000057  loss: 3.5445 (3.8118)  time: 0.1354  data: 0.0660  max mem: 3355\n",
            "Epoch: [3]  [270/781]  eta: 0:01:08  lr: 0.000057  loss: 3.4895 (3.8040)  time: 0.1364  data: 0.0698  max mem: 3355\n",
            "Epoch: [3]  [280/781]  eta: 0:01:07  lr: 0.000057  loss: 3.4711 (3.8023)  time: 0.1268  data: 0.0615  max mem: 3355\n",
            "Epoch: [3]  [290/781]  eta: 0:01:05  lr: 0.000057  loss: 3.4894 (3.7940)  time: 0.1261  data: 0.0608  max mem: 3355\n",
            "Epoch: [3]  [300/781]  eta: 0:01:04  lr: 0.000057  loss: 3.4523 (3.7895)  time: 0.1331  data: 0.0675  max mem: 3355\n",
            "Epoch: [3]  [310/781]  eta: 0:01:03  lr: 0.000057  loss: 3.4420 (3.7788)  time: 0.1318  data: 0.0663  max mem: 3355\n",
            "Epoch: [3]  [320/781]  eta: 0:01:01  lr: 0.000057  loss: 3.4420 (3.7731)  time: 0.1340  data: 0.0689  max mem: 3355\n",
            "Epoch: [3]  [330/781]  eta: 0:01:00  lr: 0.000057  loss: 3.5260 (3.7685)  time: 0.1278  data: 0.0624  max mem: 3355\n",
            "Epoch: [3]  [340/781]  eta: 0:00:59  lr: 0.000057  loss: 3.4878 (3.7600)  time: 0.1418  data: 0.0738  max mem: 3355\n",
            "Epoch: [3]  [350/781]  eta: 0:00:57  lr: 0.000057  loss: 3.4556 (3.7576)  time: 0.1379  data: 0.0693  max mem: 3355\n",
            "Epoch: [3]  [360/781]  eta: 0:00:56  lr: 0.000057  loss: 3.5068 (3.7529)  time: 0.1368  data: 0.0704  max mem: 3355\n",
            "Epoch: [3]  [370/781]  eta: 0:00:55  lr: 0.000057  loss: 3.5068 (3.7557)  time: 0.1370  data: 0.0694  max mem: 3355\n",
            "Epoch: [3]  [380/781]  eta: 0:00:53  lr: 0.000057  loss: 3.4836 (3.7489)  time: 0.1331  data: 0.0659  max mem: 3355\n",
            "Epoch: [3]  [390/781]  eta: 0:00:52  lr: 0.000057  loss: 3.4453 (3.7534)  time: 0.1331  data: 0.0676  max mem: 3355\n",
            "Epoch: [3]  [400/781]  eta: 0:00:51  lr: 0.000057  loss: 3.5187 (3.7542)  time: 0.1348  data: 0.0691  max mem: 3355\n",
            "Epoch: [3]  [410/781]  eta: 0:00:49  lr: 0.000057  loss: 3.5005 (3.7468)  time: 0.1351  data: 0.0694  max mem: 3355\n",
            "Epoch: [3]  [420/781]  eta: 0:00:48  lr: 0.000057  loss: 3.4564 (3.7387)  time: 0.1384  data: 0.0700  max mem: 3355\n",
            "Epoch: [3]  [430/781]  eta: 0:00:47  lr: 0.000057  loss: 3.4163 (3.7335)  time: 0.1407  data: 0.0689  max mem: 3355\n",
            "Epoch: [3]  [440/781]  eta: 0:00:45  lr: 0.000057  loss: 3.3422 (3.7280)  time: 0.1378  data: 0.0665  max mem: 3355\n",
            "Epoch: [3]  [450/781]  eta: 0:00:44  lr: 0.000057  loss: 3.3496 (3.7209)  time: 0.1352  data: 0.0665  max mem: 3355\n",
            "Epoch: [3]  [460/781]  eta: 0:00:43  lr: 0.000057  loss: 3.3845 (3.7160)  time: 0.1330  data: 0.0654  max mem: 3355\n",
            "Epoch: [3]  [470/781]  eta: 0:00:41  lr: 0.000057  loss: 3.4199 (3.7166)  time: 0.1322  data: 0.0644  max mem: 3355\n",
            "Epoch: [3]  [480/781]  eta: 0:00:40  lr: 0.000057  loss: 3.4274 (3.7098)  time: 0.1361  data: 0.0688  max mem: 3355\n",
            "Epoch: [3]  [490/781]  eta: 0:00:39  lr: 0.000057  loss: 3.3611 (3.7026)  time: 0.1387  data: 0.0718  max mem: 3355\n",
            "Epoch: [3]  [500/781]  eta: 0:00:37  lr: 0.000057  loss: 3.2650 (3.6996)  time: 0.1418  data: 0.0743  max mem: 3355\n",
            "Epoch: [3]  [510/781]  eta: 0:00:36  lr: 0.000057  loss: 3.3579 (3.6959)  time: 0.1427  data: 0.0640  max mem: 3355\n",
            "Epoch: [3]  [520/781]  eta: 0:00:35  lr: 0.000057  loss: 3.4080 (3.6959)  time: 0.1395  data: 0.0619  max mem: 3355\n",
            "Epoch: [3]  [530/781]  eta: 0:00:33  lr: 0.000057  loss: 3.4055 (3.6889)  time: 0.1391  data: 0.0731  max mem: 3355\n",
            "Epoch: [3]  [540/781]  eta: 0:00:32  lr: 0.000057  loss: 3.3300 (3.6828)  time: 0.1378  data: 0.0712  max mem: 3355\n",
            "Epoch: [3]  [550/781]  eta: 0:00:31  lr: 0.000057  loss: 3.3430 (3.6877)  time: 0.1359  data: 0.0685  max mem: 3355\n",
            "Epoch: [3]  [560/781]  eta: 0:00:29  lr: 0.000057  loss: 3.2766 (3.6791)  time: 0.1353  data: 0.0679  max mem: 3355\n",
            "Epoch: [3]  [570/781]  eta: 0:00:28  lr: 0.000057  loss: 3.2508 (3.6756)  time: 0.1360  data: 0.0681  max mem: 3355\n",
            "Epoch: [3]  [580/781]  eta: 0:00:27  lr: 0.000057  loss: 3.2954 (3.6740)  time: 0.1390  data: 0.0708  max mem: 3355\n",
            "Epoch: [3]  [590/781]  eta: 0:00:25  lr: 0.000057  loss: 3.3184 (3.6720)  time: 0.1408  data: 0.0734  max mem: 3355\n",
            "Epoch: [3]  [600/781]  eta: 0:00:24  lr: 0.000057  loss: 3.3065 (3.6683)  time: 0.1422  data: 0.0729  max mem: 3355\n",
            "Epoch: [3]  [610/781]  eta: 0:00:23  lr: 0.000057  loss: 3.2105 (3.6670)  time: 0.1431  data: 0.0687  max mem: 3355\n",
            "Epoch: [3]  [620/781]  eta: 0:00:21  lr: 0.000057  loss: 3.2446 (3.6611)  time: 0.1397  data: 0.0660  max mem: 3355\n",
            "Epoch: [3]  [630/781]  eta: 0:00:20  lr: 0.000057  loss: 3.2456 (3.6589)  time: 0.1371  data: 0.0685  max mem: 3355\n",
            "Epoch: [3]  [640/781]  eta: 0:00:19  lr: 0.000057  loss: 3.3092 (3.6525)  time: 0.1359  data: 0.0687  max mem: 3355\n",
            "Epoch: [3]  [650/781]  eta: 0:00:17  lr: 0.000057  loss: 3.3162 (3.6532)  time: 0.1358  data: 0.0678  max mem: 3355\n",
            "Epoch: [3]  [660/781]  eta: 0:00:16  lr: 0.000057  loss: 3.2551 (3.6492)  time: 0.1343  data: 0.0659  max mem: 3355\n",
            "Epoch: [3]  [670/781]  eta: 0:00:15  lr: 0.000057  loss: 3.2551 (3.6493)  time: 0.1338  data: 0.0661  max mem: 3355\n",
            "Epoch: [3]  [680/781]  eta: 0:00:13  lr: 0.000057  loss: 3.2758 (3.6448)  time: 0.1370  data: 0.0706  max mem: 3355\n",
            "Epoch: [3]  [690/781]  eta: 0:00:12  lr: 0.000057  loss: 3.2780 (3.6407)  time: 0.1395  data: 0.0720  max mem: 3355\n",
            "Epoch: [3]  [700/781]  eta: 0:00:11  lr: 0.000057  loss: 3.2806 (3.6354)  time: 0.1446  data: 0.0738  max mem: 3355\n",
            "Epoch: [3]  [710/781]  eta: 0:00:09  lr: 0.000057  loss: 3.2806 (3.6343)  time: 0.1420  data: 0.0724  max mem: 3355\n",
            "Epoch: [3]  [720/781]  eta: 0:00:08  lr: 0.000057  loss: 3.3556 (3.6344)  time: 0.1367  data: 0.0691  max mem: 3355\n",
            "Epoch: [3]  [730/781]  eta: 0:00:06  lr: 0.000057  loss: 3.3420 (3.6290)  time: 0.1362  data: 0.0672  max mem: 3355\n",
            "Epoch: [3]  [740/781]  eta: 0:00:05  lr: 0.000057  loss: 3.1814 (3.6263)  time: 0.1295  data: 0.0603  max mem: 3355\n",
            "Epoch: [3]  [750/781]  eta: 0:00:04  lr: 0.000057  loss: 3.2239 (3.6237)  time: 0.1284  data: 0.0603  max mem: 3355\n",
            "Epoch: [3]  [760/781]  eta: 0:00:02  lr: 0.000057  loss: 3.2631 (3.6213)  time: 0.1378  data: 0.0696  max mem: 3355\n",
            "Epoch: [3]  [770/781]  eta: 0:00:01  lr: 0.000057  loss: 3.2591 (3.6165)  time: 0.1360  data: 0.0673  max mem: 3355\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000057  loss: 3.2325 (3.6132)  time: 0.1303  data: 0.0634  max mem: 3355\n",
            "Epoch: [3] Total time: 0:01:46 (0.1359 s / it)\n",
            "Averaged stats: lr: 0.000057  loss: 3.2325 (3.6132)\n",
            "Test:  [ 0/53]  eta: 0:00:49  loss: 1.2211 (1.2211)  acc1: 67.7083 (67.7083)  acc5: 90.1042 (90.1042)  time: 0.9402  data: 0.9107  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.3494 (1.3451)  acc1: 67.7083 (68.1818)  acc5: 90.6250 (89.2519)  time: 0.1819  data: 0.1525  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.3621 (1.4594)  acc1: 65.1042 (66.5923)  acc5: 86.4583 (87.3760)  time: 0.1238  data: 0.0945  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.6033 (1.4974)  acc1: 64.0625 (66.3642)  acc5: 85.9375 (86.8784)  time: 0.1287  data: 0.0994  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.6541 (1.6012)  acc1: 63.5417 (63.7195)  acc5: 83.3333 (85.4167)  time: 0.1302  data: 0.1008  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.6534 (1.6031)  acc1: 59.8958 (63.5621)  acc5: 84.3750 (85.3656)  time: 0.1287  data: 0.0993  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.6534 (1.6098)  acc1: 59.8958 (63.4500)  acc5: 84.3750 (85.4800)  time: 0.1087  data: 0.0802  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1366 s / it)\n",
            "* Acc@1 63.450 Acc@5 85.480 loss 1.610\n",
            "Accuracy of the network on the 10000 test images: 63.5%\n",
            "Max accuracy: 63.45%\n",
            "Epoch: [4]  [  0/781]  eta: 0:11:01  lr: 0.000052  loss: 3.1797 (3.1797)  time: 0.8475  data: 0.7490  max mem: 3355\n",
            "Epoch: [4]  [ 10/781]  eta: 0:02:28  lr: 0.000052  loss: 3.1872 (3.2114)  time: 0.1924  data: 0.1231  max mem: 3355\n",
            "Epoch: [4]  [ 20/781]  eta: 0:02:13  lr: 0.000052  loss: 3.1872 (3.2777)  time: 0.1412  data: 0.0737  max mem: 3355\n",
            "Epoch: [4]  [ 30/781]  eta: 0:02:01  lr: 0.000052  loss: 3.1422 (3.2919)  time: 0.1458  data: 0.0771  max mem: 3355\n",
            "Epoch: [4]  [ 40/781]  eta: 0:01:59  lr: 0.000052  loss: 3.0853 (3.2468)  time: 0.1484  data: 0.0798  max mem: 3355\n",
            "Epoch: [4]  [ 50/781]  eta: 0:01:54  lr: 0.000052  loss: 3.0820 (3.2761)  time: 0.1476  data: 0.0786  max mem: 3355\n",
            "Epoch: [4]  [ 60/781]  eta: 0:01:51  lr: 0.000052  loss: 3.2508 (3.3045)  time: 0.1399  data: 0.0696  max mem: 3355\n",
            "Epoch: [4]  [ 70/781]  eta: 0:01:47  lr: 0.000052  loss: 3.2219 (3.3189)  time: 0.1365  data: 0.0672  max mem: 3355\n",
            "Epoch: [4]  [ 80/781]  eta: 0:01:45  lr: 0.000052  loss: 3.2170 (3.3259)  time: 0.1371  data: 0.0695  max mem: 3355\n",
            "Epoch: [4]  [ 90/781]  eta: 0:01:43  lr: 0.000052  loss: 3.2170 (3.3294)  time: 0.1455  data: 0.0783  max mem: 3355\n",
            "Epoch: [4]  [100/781]  eta: 0:01:41  lr: 0.000052  loss: 3.1502 (3.3088)  time: 0.1420  data: 0.0738  max mem: 3355\n",
            "Epoch: [4]  [110/781]  eta: 0:01:38  lr: 0.000052  loss: 3.1583 (3.3172)  time: 0.1321  data: 0.0645  max mem: 3355\n",
            "Epoch: [4]  [120/781]  eta: 0:01:37  lr: 0.000052  loss: 3.2022 (3.3019)  time: 0.1417  data: 0.0717  max mem: 3355\n",
            "Epoch: [4]  [130/781]  eta: 0:01:34  lr: 0.000052  loss: 3.0854 (3.2805)  time: 0.1424  data: 0.0703  max mem: 3355\n",
            "Epoch: [4]  [140/781]  eta: 0:01:34  lr: 0.000052  loss: 3.0854 (3.2813)  time: 0.1421  data: 0.0728  max mem: 3355\n",
            "Epoch: [4]  [150/781]  eta: 0:01:31  lr: 0.000052  loss: 3.1533 (3.2964)  time: 0.1394  data: 0.0714  max mem: 3355\n",
            "Epoch: [4]  [160/781]  eta: 0:01:30  lr: 0.000052  loss: 3.2364 (3.2908)  time: 0.1349  data: 0.0671  max mem: 3355\n",
            "Epoch: [4]  [170/781]  eta: 0:01:27  lr: 0.000052  loss: 3.2392 (3.2981)  time: 0.1331  data: 0.0658  max mem: 3355\n",
            "Epoch: [4]  [180/781]  eta: 0:01:26  lr: 0.000052  loss: 3.2268 (3.2947)  time: 0.1328  data: 0.0652  max mem: 3355\n",
            "Epoch: [4]  [190/781]  eta: 0:01:24  lr: 0.000052  loss: 3.1059 (3.2847)  time: 0.1337  data: 0.0660  max mem: 3355\n",
            "Epoch: [4]  [200/781]  eta: 0:01:23  lr: 0.000052  loss: 3.0866 (3.2861)  time: 0.1345  data: 0.0665  max mem: 3355\n",
            "Epoch: [4]  [210/781]  eta: 0:01:21  lr: 0.000052  loss: 3.2049 (3.2918)  time: 0.1380  data: 0.0664  max mem: 3355\n",
            "Epoch: [4]  [220/781]  eta: 0:01:19  lr: 0.000052  loss: 3.2504 (3.3049)  time: 0.1331  data: 0.0604  max mem: 3355\n",
            "Epoch: [4]  [230/781]  eta: 0:01:17  lr: 0.000052  loss: 3.3374 (3.3188)  time: 0.1286  data: 0.0596  max mem: 3355\n",
            "Epoch: [4]  [240/781]  eta: 0:01:16  lr: 0.000052  loss: 3.2489 (3.3189)  time: 0.1323  data: 0.0641  max mem: 3355\n",
            "Epoch: [4]  [250/781]  eta: 0:01:14  lr: 0.000052  loss: 3.2031 (3.3133)  time: 0.1324  data: 0.0634  max mem: 3355\n",
            "Epoch: [4]  [260/781]  eta: 0:01:13  lr: 0.000052  loss: 3.0856 (3.3136)  time: 0.1378  data: 0.0678  max mem: 3355\n",
            "Epoch: [4]  [270/781]  eta: 0:01:11  lr: 0.000052  loss: 3.2056 (3.3121)  time: 0.1428  data: 0.0732  max mem: 3355\n",
            "Epoch: [4]  [280/781]  eta: 0:01:10  lr: 0.000052  loss: 3.2022 (3.3069)  time: 0.1351  data: 0.0675  max mem: 3355\n",
            "Epoch: [4]  [290/781]  eta: 0:01:08  lr: 0.000052  loss: 3.1559 (3.3072)  time: 0.1294  data: 0.0619  max mem: 3355\n",
            "Epoch: [4]  [300/781]  eta: 0:01:07  lr: 0.000052  loss: 3.1559 (3.3071)  time: 0.1361  data: 0.0672  max mem: 3355\n",
            "Epoch: [4]  [310/781]  eta: 0:01:05  lr: 0.000052  loss: 3.1188 (3.3078)  time: 0.1384  data: 0.0682  max mem: 3355\n",
            "Epoch: [4]  [320/781]  eta: 0:01:04  lr: 0.000052  loss: 3.1057 (3.3067)  time: 0.1341  data: 0.0655  max mem: 3355\n",
            "Epoch: [4]  [330/781]  eta: 0:01:02  lr: 0.000052  loss: 3.0620 (3.3031)  time: 0.1323  data: 0.0651  max mem: 3355\n",
            "Epoch: [4]  [340/781]  eta: 0:01:01  lr: 0.000052  loss: 3.0620 (3.3030)  time: 0.1265  data: 0.0582  max mem: 3355\n",
            "Epoch: [4]  [350/781]  eta: 0:00:59  lr: 0.000052  loss: 3.1035 (3.2987)  time: 0.1299  data: 0.0614  max mem: 3355\n",
            "Epoch: [4]  [360/781]  eta: 0:00:58  lr: 0.000052  loss: 3.1526 (3.2995)  time: 0.1282  data: 0.0581  max mem: 3355\n",
            "Epoch: [4]  [370/781]  eta: 0:00:56  lr: 0.000052  loss: 3.1925 (3.3005)  time: 0.1352  data: 0.0640  max mem: 3355\n",
            "Epoch: [4]  [380/781]  eta: 0:00:55  lr: 0.000052  loss: 3.2547 (3.3136)  time: 0.1309  data: 0.0609  max mem: 3355\n",
            "Epoch: [4]  [390/781]  eta: 0:00:53  lr: 0.000052  loss: 3.2022 (3.3094)  time: 0.1302  data: 0.0613  max mem: 3355\n",
            "Epoch: [4]  [400/781]  eta: 0:00:52  lr: 0.000052  loss: 3.1396 (3.3095)  time: 0.1356  data: 0.0653  max mem: 3355\n",
            "Epoch: [4]  [410/781]  eta: 0:00:51  lr: 0.000052  loss: 3.1200 (3.3084)  time: 0.1353  data: 0.0634  max mem: 3355\n",
            "Epoch: [4]  [420/781]  eta: 0:00:49  lr: 0.000052  loss: 3.2428 (3.3155)  time: 0.1363  data: 0.0669  max mem: 3355\n",
            "Epoch: [4]  [430/781]  eta: 0:00:48  lr: 0.000052  loss: 3.2072 (3.3110)  time: 0.1307  data: 0.0641  max mem: 3355\n",
            "Epoch: [4]  [440/781]  eta: 0:00:46  lr: 0.000052  loss: 3.1072 (3.3095)  time: 0.1296  data: 0.0634  max mem: 3355\n",
            "Epoch: [4]  [450/781]  eta: 0:00:45  lr: 0.000052  loss: 3.1072 (3.3180)  time: 0.1293  data: 0.0634  max mem: 3355\n",
            "Epoch: [4]  [460/781]  eta: 0:00:43  lr: 0.000052  loss: 3.2424 (3.3170)  time: 0.1250  data: 0.0598  max mem: 3355\n",
            "Epoch: [4]  [470/781]  eta: 0:00:42  lr: 0.000052  loss: 3.0465 (3.3131)  time: 0.1326  data: 0.0675  max mem: 3355\n",
            "Epoch: [4]  [480/781]  eta: 0:00:41  lr: 0.000052  loss: 2.9987 (3.3093)  time: 0.1330  data: 0.0673  max mem: 3355\n",
            "Epoch: [4]  [490/781]  eta: 0:00:39  lr: 0.000052  loss: 3.0665 (3.3073)  time: 0.1348  data: 0.0670  max mem: 3355\n",
            "Epoch: [4]  [500/781]  eta: 0:00:38  lr: 0.000052  loss: 3.0288 (3.3024)  time: 0.1390  data: 0.0710  max mem: 3355\n",
            "Epoch: [4]  [510/781]  eta: 0:00:37  lr: 0.000052  loss: 3.1320 (3.3073)  time: 0.1309  data: 0.0641  max mem: 3355\n",
            "Epoch: [4]  [520/781]  eta: 0:00:35  lr: 0.000052  loss: 3.2037 (3.3036)  time: 0.1386  data: 0.0720  max mem: 3355\n",
            "Epoch: [4]  [530/781]  eta: 0:00:34  lr: 0.000052  loss: 3.1132 (3.3053)  time: 0.1415  data: 0.0741  max mem: 3355\n",
            "Epoch: [4]  [540/781]  eta: 0:00:33  lr: 0.000052  loss: 3.1301 (3.3033)  time: 0.1402  data: 0.0729  max mem: 3355\n",
            "Epoch: [4]  [550/781]  eta: 0:00:31  lr: 0.000052  loss: 3.0599 (3.2978)  time: 0.1325  data: 0.0668  max mem: 3355\n",
            "Epoch: [4]  [560/781]  eta: 0:00:30  lr: 0.000052  loss: 3.0686 (3.3030)  time: 0.1284  data: 0.0631  max mem: 3355\n",
            "Epoch: [4]  [570/781]  eta: 0:00:28  lr: 0.000052  loss: 3.1456 (3.3019)  time: 0.1271  data: 0.0615  max mem: 3355\n",
            "Epoch: [4]  [580/781]  eta: 0:00:27  lr: 0.000052  loss: 3.0359 (3.3010)  time: 0.1290  data: 0.0635  max mem: 3355\n",
            "Epoch: [4]  [590/781]  eta: 0:00:26  lr: 0.000052  loss: 3.0568 (3.3057)  time: 0.1337  data: 0.0671  max mem: 3355\n",
            "Epoch: [4]  [600/781]  eta: 0:00:24  lr: 0.000052  loss: 3.0927 (3.3037)  time: 0.1339  data: 0.0659  max mem: 3355\n",
            "Epoch: [4]  [610/781]  eta: 0:00:23  lr: 0.000052  loss: 3.1012 (3.3015)  time: 0.1296  data: 0.0617  max mem: 3355\n",
            "Epoch: [4]  [620/781]  eta: 0:00:21  lr: 0.000052  loss: 3.1255 (3.2998)  time: 0.1285  data: 0.0608  max mem: 3355\n",
            "Epoch: [4]  [630/781]  eta: 0:00:20  lr: 0.000052  loss: 3.1225 (3.3035)  time: 0.1275  data: 0.0604  max mem: 3355\n",
            "Epoch: [4]  [640/781]  eta: 0:00:19  lr: 0.000052  loss: 3.1154 (3.3050)  time: 0.1341  data: 0.0675  max mem: 3355\n",
            "Epoch: [4]  [650/781]  eta: 0:00:17  lr: 0.000052  loss: 3.0430 (3.3032)  time: 0.1375  data: 0.0694  max mem: 3355\n",
            "Epoch: [4]  [660/781]  eta: 0:00:16  lr: 0.000052  loss: 2.9809 (3.3005)  time: 0.1383  data: 0.0699  max mem: 3355\n",
            "Epoch: [4]  [670/781]  eta: 0:00:15  lr: 0.000052  loss: 3.0529 (3.3004)  time: 0.1384  data: 0.0675  max mem: 3355\n",
            "Epoch: [4]  [680/781]  eta: 0:00:13  lr: 0.000052  loss: 3.1317 (3.3006)  time: 0.1324  data: 0.0604  max mem: 3355\n",
            "Epoch: [4]  [690/781]  eta: 0:00:12  lr: 0.000052  loss: 3.0537 (3.2973)  time: 0.1315  data: 0.0628  max mem: 3355\n",
            "Epoch: [4]  [700/781]  eta: 0:00:11  lr: 0.000052  loss: 2.9951 (3.2982)  time: 0.1310  data: 0.0643  max mem: 3355\n",
            "Epoch: [4]  [710/781]  eta: 0:00:09  lr: 0.000052  loss: 3.0987 (3.2958)  time: 0.1314  data: 0.0658  max mem: 3355\n",
            "Epoch: [4]  [720/781]  eta: 0:00:08  lr: 0.000052  loss: 3.0423 (3.2930)  time: 0.1300  data: 0.0650  max mem: 3355\n",
            "Epoch: [4]  [730/781]  eta: 0:00:06  lr: 0.000052  loss: 3.0210 (3.2907)  time: 0.1322  data: 0.0666  max mem: 3355\n",
            "Epoch: [4]  [740/781]  eta: 0:00:05  lr: 0.000052  loss: 3.0210 (3.2919)  time: 0.1308  data: 0.0645  max mem: 3355\n",
            "Epoch: [4]  [750/781]  eta: 0:00:04  lr: 0.000052  loss: 3.0554 (3.2911)  time: 0.1292  data: 0.0618  max mem: 3355\n",
            "Epoch: [4]  [760/781]  eta: 0:00:02  lr: 0.000052  loss: 3.0822 (3.2928)  time: 0.1337  data: 0.0629  max mem: 3355\n",
            "Epoch: [4]  [770/781]  eta: 0:00:01  lr: 0.000052  loss: 3.0598 (3.2902)  time: 0.1323  data: 0.0610  max mem: 3355\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000052  loss: 3.1683 (3.2981)  time: 0.1301  data: 0.0603  max mem: 3355\n",
            "Epoch: [4] Total time: 0:01:45 (0.1356 s / it)\n",
            "Averaged stats: lr: 0.000052  loss: 3.1683 (3.2981)\n",
            "Test:  [ 0/53]  eta: 0:00:46  loss: 1.0941 (1.0941)  acc1: 73.4375 (73.4375)  acc5: 94.7917 (94.7917)  time: 0.8757  data: 0.8462  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.3174 (1.2780)  acc1: 71.3542 (69.4602)  acc5: 88.5417 (90.0095)  time: 0.1824  data: 0.1532  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.3806 (1.3413)  acc1: 65.1042 (69.0724)  acc5: 87.5000 (88.8641)  time: 0.1331  data: 0.1039  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.5648 (1.3876)  acc1: 65.1042 (68.2628)  acc5: 88.0208 (88.5753)  time: 0.1322  data: 0.1030  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.5913 (1.4478)  acc1: 65.6250 (67.0224)  acc5: 86.4583 (87.6651)  time: 0.1331  data: 0.1039  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4394 (1.4391)  acc1: 66.1458 (67.2079)  acc5: 86.9792 (87.7349)  time: 0.1297  data: 0.1005  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4446 (1.4432)  acc1: 65.6250 (67.1300)  acc5: 86.9792 (87.8400)  time: 0.1079  data: 0.0796  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1386 s / it)\n",
            "* Acc@1 67.130 Acc@5 87.840 loss 1.443\n",
            "Accuracy of the network on the 10000 test images: 67.1%\n",
            "Max accuracy: 67.13%\n",
            "Epoch: [5]  [  0/781]  eta: 0:11:31  lr: 0.000044  loss: 2.7911 (2.7911)  time: 0.8849  data: 0.8007  max mem: 3355\n",
            "Epoch: [5]  [ 10/781]  eta: 0:02:30  lr: 0.000044  loss: 3.0897 (3.3541)  time: 0.1947  data: 0.1263  max mem: 3355\n",
            "Epoch: [5]  [ 20/781]  eta: 0:02:16  lr: 0.000044  loss: 3.0131 (3.2428)  time: 0.1445  data: 0.0751  max mem: 3355\n",
            "Epoch: [5]  [ 30/781]  eta: 0:02:04  lr: 0.000044  loss: 2.9585 (3.1857)  time: 0.1498  data: 0.0801  max mem: 3355\n",
            "Epoch: [5]  [ 40/781]  eta: 0:02:00  lr: 0.000044  loss: 2.9669 (3.1256)  time: 0.1454  data: 0.0775  max mem: 3355\n",
            "Epoch: [5]  [ 50/781]  eta: 0:01:52  lr: 0.000044  loss: 3.0702 (3.1747)  time: 0.1347  data: 0.0652  max mem: 3355\n",
            "Epoch: [5]  [ 60/781]  eta: 0:01:49  lr: 0.000044  loss: 3.1468 (3.2223)  time: 0.1274  data: 0.0596  max mem: 3355\n",
            "Epoch: [5]  [ 70/781]  eta: 0:01:48  lr: 0.000044  loss: 3.1376 (3.2691)  time: 0.1497  data: 0.0838  max mem: 3355\n",
            "Epoch: [5]  [ 80/781]  eta: 0:01:45  lr: 0.000044  loss: 2.9146 (3.2427)  time: 0.1492  data: 0.0825  max mem: 3355\n",
            "Epoch: [5]  [ 90/781]  eta: 0:01:44  lr: 0.000044  loss: 3.0484 (3.2732)  time: 0.1464  data: 0.0781  max mem: 3355\n",
            "Epoch: [5]  [100/781]  eta: 0:01:41  lr: 0.000044  loss: 3.1736 (3.2609)  time: 0.1439  data: 0.0738  max mem: 3355\n",
            "Epoch: [5]  [110/781]  eta: 0:01:40  lr: 0.000044  loss: 3.0742 (3.2803)  time: 0.1465  data: 0.0742  max mem: 3355\n",
            "Epoch: [5]  [120/781]  eta: 0:01:37  lr: 0.000044  loss: 3.0766 (3.2807)  time: 0.1382  data: 0.0674  max mem: 3355\n",
            "Epoch: [5]  [130/781]  eta: 0:01:36  lr: 0.000044  loss: 3.0740 (3.2881)  time: 0.1381  data: 0.0716  max mem: 3355\n",
            "Epoch: [5]  [140/781]  eta: 0:01:34  lr: 0.000044  loss: 3.0649 (3.2870)  time: 0.1441  data: 0.0785  max mem: 3355\n",
            "Epoch: [5]  [150/781]  eta: 0:01:33  lr: 0.000044  loss: 3.0830 (3.2948)  time: 0.1413  data: 0.0760  max mem: 3355\n",
            "Epoch: [5]  [160/781]  eta: 0:01:30  lr: 0.000044  loss: 2.9879 (3.2759)  time: 0.1386  data: 0.0725  max mem: 3355\n",
            "Epoch: [5]  [170/781]  eta: 0:01:29  lr: 0.000044  loss: 2.9796 (3.2598)  time: 0.1425  data: 0.0766  max mem: 3355\n",
            "Epoch: [5]  [180/781]  eta: 0:01:27  lr: 0.000044  loss: 2.9509 (3.2565)  time: 0.1425  data: 0.0771  max mem: 3355\n",
            "Epoch: [5]  [190/781]  eta: 0:01:26  lr: 0.000044  loss: 3.0506 (3.2579)  time: 0.1448  data: 0.0762  max mem: 3355\n",
            "Epoch: [5]  [200/781]  eta: 0:01:24  lr: 0.000044  loss: 2.9657 (3.2534)  time: 0.1519  data: 0.0804  max mem: 3355\n",
            "Epoch: [5]  [210/781]  eta: 0:01:23  lr: 0.000044  loss: 2.9657 (3.2567)  time: 0.1501  data: 0.0786  max mem: 3355\n",
            "Epoch: [5]  [220/781]  eta: 0:01:22  lr: 0.000044  loss: 2.9913 (3.2487)  time: 0.1481  data: 0.0784  max mem: 3355\n",
            "Epoch: [5]  [230/781]  eta: 0:01:20  lr: 0.000044  loss: 2.9459 (3.2459)  time: 0.1422  data: 0.0724  max mem: 3355\n",
            "Epoch: [5]  [240/781]  eta: 0:01:18  lr: 0.000044  loss: 2.9603 (3.2444)  time: 0.1333  data: 0.0632  max mem: 3355\n",
            "Epoch: [5]  [250/781]  eta: 0:01:17  lr: 0.000044  loss: 2.9089 (3.2310)  time: 0.1301  data: 0.0595  max mem: 3355\n",
            "Epoch: [5]  [260/781]  eta: 0:01:15  lr: 0.000044  loss: 2.9234 (3.2343)  time: 0.1386  data: 0.0676  max mem: 3355\n",
            "Epoch: [5]  [270/781]  eta: 0:01:14  lr: 0.000044  loss: 3.0065 (3.2305)  time: 0.1413  data: 0.0728  max mem: 3355\n",
            "Epoch: [5]  [280/781]  eta: 0:01:12  lr: 0.000044  loss: 2.9990 (3.2223)  time: 0.1374  data: 0.0668  max mem: 3355\n",
            "Epoch: [5]  [290/781]  eta: 0:01:10  lr: 0.000044  loss: 3.0774 (3.2263)  time: 0.1320  data: 0.0605  max mem: 3355\n",
            "Epoch: [5]  [300/781]  eta: 0:01:08  lr: 0.000044  loss: 3.0624 (3.2264)  time: 0.1267  data: 0.0579  max mem: 3355\n",
            "Epoch: [5]  [310/781]  eta: 0:01:07  lr: 0.000044  loss: 2.9350 (3.2251)  time: 0.1268  data: 0.0589  max mem: 3355\n",
            "Epoch: [5]  [320/781]  eta: 0:01:05  lr: 0.000044  loss: 3.0187 (3.2271)  time: 0.1265  data: 0.0586  max mem: 3355\n",
            "Epoch: [5]  [330/781]  eta: 0:01:04  lr: 0.000044  loss: 2.9461 (3.2217)  time: 0.1287  data: 0.0590  max mem: 3355\n",
            "Epoch: [5]  [340/781]  eta: 0:01:02  lr: 0.000044  loss: 2.9866 (3.2307)  time: 0.1308  data: 0.0593  max mem: 3355\n",
            "Epoch: [5]  [350/781]  eta: 0:01:00  lr: 0.000044  loss: 3.0034 (3.2241)  time: 0.1300  data: 0.0605  max mem: 3355\n",
            "Epoch: [5]  [360/781]  eta: 0:00:59  lr: 0.000044  loss: 2.9497 (3.2239)  time: 0.1303  data: 0.0631  max mem: 3355\n",
            "Epoch: [5]  [370/781]  eta: 0:00:57  lr: 0.000044  loss: 2.9789 (3.2212)  time: 0.1334  data: 0.0629  max mem: 3355\n",
            "Epoch: [5]  [380/781]  eta: 0:00:56  lr: 0.000044  loss: 2.8483 (3.2124)  time: 0.1382  data: 0.0666  max mem: 3355\n",
            "Epoch: [5]  [390/781]  eta: 0:00:55  lr: 0.000044  loss: 2.8653 (3.2173)  time: 0.1383  data: 0.0701  max mem: 3355\n",
            "Epoch: [5]  [400/781]  eta: 0:00:53  lr: 0.000044  loss: 2.9353 (3.2116)  time: 0.1346  data: 0.0677  max mem: 3355\n",
            "Epoch: [5]  [410/781]  eta: 0:00:52  lr: 0.000044  loss: 2.8980 (3.2096)  time: 0.1318  data: 0.0657  max mem: 3355\n",
            "Epoch: [5]  [420/781]  eta: 0:00:50  lr: 0.000044  loss: 2.9364 (3.2086)  time: 0.1308  data: 0.0651  max mem: 3355\n",
            "Epoch: [5]  [430/781]  eta: 0:00:49  lr: 0.000044  loss: 2.9820 (3.2062)  time: 0.1269  data: 0.0616  max mem: 3355\n",
            "Epoch: [5]  [440/781]  eta: 0:00:47  lr: 0.000044  loss: 2.9220 (3.2087)  time: 0.1296  data: 0.0629  max mem: 3355\n",
            "Epoch: [5]  [450/781]  eta: 0:00:46  lr: 0.000044  loss: 2.9190 (3.2045)  time: 0.1352  data: 0.0678  max mem: 3355\n",
            "Epoch: [5]  [460/781]  eta: 0:00:44  lr: 0.000044  loss: 2.8851 (3.1984)  time: 0.1346  data: 0.0662  max mem: 3355\n",
            "Epoch: [5]  [470/781]  eta: 0:00:43  lr: 0.000044  loss: 2.8592 (3.1971)  time: 0.1352  data: 0.0628  max mem: 3355\n",
            "Epoch: [5]  [480/781]  eta: 0:00:41  lr: 0.000044  loss: 2.9130 (3.1973)  time: 0.1369  data: 0.0628  max mem: 3355\n",
            "Epoch: [5]  [490/781]  eta: 0:00:40  lr: 0.000044  loss: 2.9270 (3.1970)  time: 0.1313  data: 0.0611  max mem: 3355\n",
            "Epoch: [5]  [500/781]  eta: 0:00:38  lr: 0.000044  loss: 2.9270 (3.1934)  time: 0.1253  data: 0.0593  max mem: 3355\n",
            "Epoch: [5]  [510/781]  eta: 0:00:37  lr: 0.000044  loss: 2.9976 (3.1901)  time: 0.1259  data: 0.0605  max mem: 3355\n",
            "Epoch: [5]  [520/781]  eta: 0:00:36  lr: 0.000044  loss: 3.0335 (3.1891)  time: 0.1274  data: 0.0622  max mem: 3355\n",
            "Epoch: [5]  [530/781]  eta: 0:00:34  lr: 0.000044  loss: 3.0389 (3.1899)  time: 0.1285  data: 0.0626  max mem: 3355\n",
            "Epoch: [5]  [540/781]  eta: 0:00:33  lr: 0.000044  loss: 2.9874 (3.1890)  time: 0.1313  data: 0.0648  max mem: 3355\n",
            "Epoch: [5]  [550/781]  eta: 0:00:31  lr: 0.000044  loss: 2.9772 (3.1871)  time: 0.1310  data: 0.0650  max mem: 3355\n",
            "Epoch: [5]  [560/781]  eta: 0:00:30  lr: 0.000044  loss: 2.8779 (3.1848)  time: 0.1311  data: 0.0640  max mem: 3355\n",
            "Epoch: [5]  [570/781]  eta: 0:00:29  lr: 0.000044  loss: 2.9766 (3.1821)  time: 0.1386  data: 0.0698  max mem: 3355\n",
            "Epoch: [5]  [580/781]  eta: 0:00:27  lr: 0.000044  loss: 3.0836 (3.1848)  time: 0.1340  data: 0.0648  max mem: 3355\n",
            "Epoch: [5]  [590/781]  eta: 0:00:26  lr: 0.000044  loss: 3.0325 (3.1846)  time: 0.1246  data: 0.0558  max mem: 3355\n",
            "Epoch: [5]  [600/781]  eta: 0:00:24  lr: 0.000044  loss: 3.0325 (3.1859)  time: 0.1441  data: 0.0766  max mem: 3355\n",
            "Epoch: [5]  [610/781]  eta: 0:00:23  lr: 0.000044  loss: 3.0210 (3.1878)  time: 0.1384  data: 0.0723  max mem: 3355\n",
            "Epoch: [5]  [620/781]  eta: 0:00:22  lr: 0.000044  loss: 2.9471 (3.1890)  time: 0.1363  data: 0.0698  max mem: 3355\n",
            "Epoch: [5]  [630/781]  eta: 0:00:20  lr: 0.000044  loss: 2.9357 (3.1867)  time: 0.1361  data: 0.0684  max mem: 3355\n",
            "Epoch: [5]  [640/781]  eta: 0:00:19  lr: 0.000044  loss: 2.8448 (3.1820)  time: 0.1345  data: 0.0661  max mem: 3355\n",
            "Epoch: [5]  [650/781]  eta: 0:00:18  lr: 0.000044  loss: 2.8928 (3.1875)  time: 0.1353  data: 0.0652  max mem: 3355\n",
            "Epoch: [5]  [660/781]  eta: 0:00:16  lr: 0.000044  loss: 3.0176 (3.1846)  time: 0.1365  data: 0.0668  max mem: 3355\n",
            "Epoch: [5]  [670/781]  eta: 0:00:15  lr: 0.000044  loss: 2.9191 (3.1795)  time: 0.1360  data: 0.0693  max mem: 3355\n",
            "Epoch: [5]  [680/781]  eta: 0:00:13  lr: 0.000044  loss: 2.8695 (3.1756)  time: 0.1370  data: 0.0718  max mem: 3355\n",
            "Epoch: [5]  [690/781]  eta: 0:00:12  lr: 0.000044  loss: 2.9333 (3.1781)  time: 0.1384  data: 0.0725  max mem: 3355\n",
            "Epoch: [5]  [700/781]  eta: 0:00:11  lr: 0.000044  loss: 3.1064 (3.1773)  time: 0.1346  data: 0.0679  max mem: 3355\n",
            "Epoch: [5]  [710/781]  eta: 0:00:09  lr: 0.000044  loss: 3.0787 (3.1772)  time: 0.1322  data: 0.0649  max mem: 3355\n",
            "Epoch: [5]  [720/781]  eta: 0:00:08  lr: 0.000044  loss: 3.0465 (3.1788)  time: 0.1391  data: 0.0703  max mem: 3355\n",
            "Epoch: [5]  [730/781]  eta: 0:00:06  lr: 0.000044  loss: 2.9127 (3.1795)  time: 0.1369  data: 0.0685  max mem: 3355\n",
            "Epoch: [5]  [740/781]  eta: 0:00:05  lr: 0.000044  loss: 2.9127 (3.1807)  time: 0.1391  data: 0.0705  max mem: 3355\n",
            "Epoch: [5]  [750/781]  eta: 0:00:04  lr: 0.000044  loss: 3.0698 (3.1832)  time: 0.1451  data: 0.0736  max mem: 3355\n",
            "Epoch: [5]  [760/781]  eta: 0:00:02  lr: 0.000044  loss: 2.9317 (3.1827)  time: 0.1435  data: 0.0738  max mem: 3355\n",
            "Epoch: [5]  [770/781]  eta: 0:00:01  lr: 0.000044  loss: 2.9522 (3.1866)  time: 0.1548  data: 0.0778  max mem: 3355\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000044  loss: 3.0303 (3.1848)  time: 0.1392  data: 0.0630  max mem: 3355\n",
            "Epoch: [5] Total time: 0:01:47 (0.1379 s / it)\n",
            "Averaged stats: lr: 0.000044  loss: 3.0303 (3.1848)\n",
            "Test:  [ 0/53]  eta: 0:00:47  loss: 1.1100 (1.1100)  acc1: 73.4375 (73.4375)  acc5: 92.1875 (92.1875)  time: 0.9032  data: 0.8737  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2021 (1.1132)  acc1: 73.9583 (73.4375)  acc5: 92.1875 (91.7140)  time: 0.1799  data: 0.1507  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2440 (1.1990)  acc1: 69.7917 (72.1478)  acc5: 89.0625 (90.3522)  time: 0.1276  data: 0.0984  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3871 (1.2554)  acc1: 69.7917 (71.4046)  acc5: 88.0208 (89.6673)  time: 0.1281  data: 0.0988  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3920 (1.3352)  acc1: 66.6667 (69.3852)  acc5: 86.9792 (88.7449)  time: 0.1299  data: 0.1007  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2979 (1.3186)  acc1: 68.7500 (69.6998)  acc5: 89.5833 (88.9400)  time: 0.1301  data: 0.1008  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2979 (1.3267)  acc1: 68.2292 (69.5300)  acc5: 89.0625 (89.0500)  time: 0.1093  data: 0.0810  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1365 s / it)\n",
            "* Acc@1 69.530 Acc@5 89.050 loss 1.327\n",
            "Accuracy of the network on the 10000 test images: 69.5%\n",
            "Max accuracy: 69.53%\n",
            "Epoch: [6]  [  0/781]  eta: 0:11:02  lr: 0.000036  loss: 2.7818 (2.7818)  time: 0.8484  data: 0.7693  max mem: 3355\n",
            "Epoch: [6]  [ 10/781]  eta: 0:02:30  lr: 0.000036  loss: 2.8530 (2.8654)  time: 0.1954  data: 0.1272  max mem: 3355\n",
            "Epoch: [6]  [ 20/781]  eta: 0:02:11  lr: 0.000036  loss: 2.8722 (2.9737)  time: 0.1390  data: 0.0715  max mem: 3355\n",
            "Epoch: [6]  [ 30/781]  eta: 0:02:02  lr: 0.000036  loss: 2.8722 (2.9683)  time: 0.1459  data: 0.0767  max mem: 3355\n",
            "Epoch: [6]  [ 40/781]  eta: 0:01:55  lr: 0.000036  loss: 2.9873 (3.1351)  time: 0.1372  data: 0.0678  max mem: 3355\n",
            "Epoch: [6]  [ 50/781]  eta: 0:01:54  lr: 0.000036  loss: 2.8704 (3.1430)  time: 0.1474  data: 0.0782  max mem: 3355\n",
            "Epoch: [6]  [ 60/781]  eta: 0:01:51  lr: 0.000036  loss: 2.8482 (3.1062)  time: 0.1509  data: 0.0833  max mem: 3355\n",
            "Epoch: [6]  [ 70/781]  eta: 0:01:49  lr: 0.000036  loss: 2.8182 (3.1011)  time: 0.1465  data: 0.0799  max mem: 3355\n",
            "Epoch: [6]  [ 80/781]  eta: 0:01:46  lr: 0.000036  loss: 2.9119 (3.0952)  time: 0.1465  data: 0.0774  max mem: 3355\n",
            "Epoch: [6]  [ 90/781]  eta: 0:01:45  lr: 0.000036  loss: 2.9238 (3.1184)  time: 0.1474  data: 0.0783  max mem: 3355\n",
            "Epoch: [6]  [100/781]  eta: 0:01:43  lr: 0.000036  loss: 2.8819 (3.1083)  time: 0.1491  data: 0.0822  max mem: 3355\n",
            "Epoch: [6]  [110/781]  eta: 0:01:42  lr: 0.000036  loss: 2.9191 (3.1103)  time: 0.1556  data: 0.0886  max mem: 3355\n",
            "Epoch: [6]  [120/781]  eta: 0:01:40  lr: 0.000036  loss: 2.9191 (3.0903)  time: 0.1527  data: 0.0853  max mem: 3355\n",
            "Epoch: [6]  [130/781]  eta: 0:01:39  lr: 0.000036  loss: 2.7827 (3.0675)  time: 0.1504  data: 0.0821  max mem: 3355\n",
            "Epoch: [6]  [140/781]  eta: 0:01:36  lr: 0.000036  loss: 2.8793 (3.0813)  time: 0.1464  data: 0.0788  max mem: 3355\n",
            "Epoch: [6]  [150/781]  eta: 0:01:34  lr: 0.000036  loss: 2.9749 (3.0930)  time: 0.1353  data: 0.0697  max mem: 3355\n",
            "Epoch: [6]  [160/781]  eta: 0:01:32  lr: 0.000036  loss: 2.9198 (3.0918)  time: 0.1371  data: 0.0684  max mem: 3355\n",
            "Epoch: [6]  [170/781]  eta: 0:01:31  lr: 0.000036  loss: 2.8899 (3.0761)  time: 0.1408  data: 0.0682  max mem: 3355\n",
            "Epoch: [6]  [180/781]  eta: 0:01:28  lr: 0.000036  loss: 2.8899 (3.0871)  time: 0.1380  data: 0.0672  max mem: 3355\n",
            "Epoch: [6]  [190/781]  eta: 0:01:27  lr: 0.000036  loss: 2.8891 (3.0829)  time: 0.1309  data: 0.0642  max mem: 3355\n",
            "Epoch: [6]  [200/781]  eta: 0:01:25  lr: 0.000036  loss: 2.8891 (3.0922)  time: 0.1428  data: 0.0774  max mem: 3355\n",
            "Epoch: [6]  [210/781]  eta: 0:01:23  lr: 0.000036  loss: 2.9399 (3.0944)  time: 0.1387  data: 0.0730  max mem: 3355\n",
            "Epoch: [6]  [220/781]  eta: 0:01:22  lr: 0.000036  loss: 3.0219 (3.1034)  time: 0.1445  data: 0.0776  max mem: 3355\n",
            "Epoch: [6]  [230/781]  eta: 0:01:20  lr: 0.000036  loss: 2.8553 (3.0948)  time: 0.1442  data: 0.0762  max mem: 3355\n",
            "Epoch: [6]  [240/781]  eta: 0:01:19  lr: 0.000036  loss: 2.8409 (3.0946)  time: 0.1418  data: 0.0737  max mem: 3355\n",
            "Epoch: [6]  [250/781]  eta: 0:01:17  lr: 0.000036  loss: 2.7893 (3.0883)  time: 0.1442  data: 0.0753  max mem: 3355\n",
            "Epoch: [6]  [260/781]  eta: 0:01:16  lr: 0.000036  loss: 2.9274 (3.0997)  time: 0.1456  data: 0.0754  max mem: 3355\n",
            "Epoch: [6]  [270/781]  eta: 0:01:14  lr: 0.000036  loss: 2.9486 (3.0963)  time: 0.1478  data: 0.0788  max mem: 3355\n",
            "Epoch: [6]  [280/781]  eta: 0:01:13  lr: 0.000036  loss: 2.8087 (3.0856)  time: 0.1469  data: 0.0796  max mem: 3355\n",
            "Epoch: [6]  [290/781]  eta: 0:01:11  lr: 0.000036  loss: 2.8281 (3.0928)  time: 0.1411  data: 0.0734  max mem: 3355\n",
            "Epoch: [6]  [300/781]  eta: 0:01:10  lr: 0.000036  loss: 2.8868 (3.0975)  time: 0.1430  data: 0.0745  max mem: 3355\n",
            "Epoch: [6]  [310/781]  eta: 0:01:08  lr: 0.000036  loss: 2.8868 (3.0961)  time: 0.1431  data: 0.0760  max mem: 3355\n",
            "Epoch: [6]  [320/781]  eta: 0:01:07  lr: 0.000036  loss: 2.8216 (3.0917)  time: 0.1441  data: 0.0776  max mem: 3355\n",
            "Epoch: [6]  [330/781]  eta: 0:01:05  lr: 0.000036  loss: 2.8977 (3.0900)  time: 0.1465  data: 0.0774  max mem: 3355\n",
            "Epoch: [6]  [340/781]  eta: 0:01:04  lr: 0.000036  loss: 2.9372 (3.0874)  time: 0.1444  data: 0.0725  max mem: 3355\n",
            "Epoch: [6]  [350/781]  eta: 0:01:02  lr: 0.000036  loss: 2.8762 (3.0881)  time: 0.1454  data: 0.0764  max mem: 3355\n",
            "Epoch: [6]  [360/781]  eta: 0:01:01  lr: 0.000036  loss: 2.8589 (3.0898)  time: 0.1433  data: 0.0761  max mem: 3355\n",
            "Epoch: [6]  [370/781]  eta: 0:00:59  lr: 0.000036  loss: 2.8502 (3.0863)  time: 0.1390  data: 0.0716  max mem: 3355\n",
            "Epoch: [6]  [380/781]  eta: 0:00:58  lr: 0.000036  loss: 2.7185 (3.0778)  time: 0.1356  data: 0.0701  max mem: 3355\n",
            "Epoch: [6]  [390/781]  eta: 0:00:56  lr: 0.000036  loss: 2.7947 (3.0808)  time: 0.1407  data: 0.0754  max mem: 3355\n",
            "Epoch: [6]  [400/781]  eta: 0:00:55  lr: 0.000036  loss: 2.9517 (3.0813)  time: 0.1491  data: 0.0831  max mem: 3355\n",
            "Epoch: [6]  [410/781]  eta: 0:00:53  lr: 0.000036  loss: 2.9014 (3.0796)  time: 0.1419  data: 0.0756  max mem: 3355\n",
            "Epoch: [6]  [420/781]  eta: 0:00:52  lr: 0.000036  loss: 2.8638 (3.0852)  time: 0.1440  data: 0.0758  max mem: 3355\n",
            "Epoch: [6]  [430/781]  eta: 0:00:51  lr: 0.000036  loss: 2.8698 (3.0904)  time: 0.1523  data: 0.0822  max mem: 3355\n",
            "Epoch: [6]  [440/781]  eta: 0:00:49  lr: 0.000036  loss: 2.8698 (3.0947)  time: 0.1454  data: 0.0743  max mem: 3355\n",
            "Epoch: [6]  [450/781]  eta: 0:00:48  lr: 0.000036  loss: 2.8494 (3.0901)  time: 0.1418  data: 0.0704  max mem: 3355\n",
            "Epoch: [6]  [460/781]  eta: 0:00:46  lr: 0.000036  loss: 2.8494 (3.0881)  time: 0.1438  data: 0.0762  max mem: 3355\n",
            "Epoch: [6]  [470/781]  eta: 0:00:45  lr: 0.000036  loss: 2.8493 (3.0920)  time: 0.1426  data: 0.0759  max mem: 3355\n",
            "Epoch: [6]  [480/781]  eta: 0:00:43  lr: 0.000036  loss: 2.8669 (3.0969)  time: 0.1414  data: 0.0732  max mem: 3355\n",
            "Epoch: [6]  [490/781]  eta: 0:00:42  lr: 0.000036  loss: 3.0705 (3.1013)  time: 0.1408  data: 0.0729  max mem: 3355\n",
            "Epoch: [6]  [500/781]  eta: 0:00:40  lr: 0.000036  loss: 2.8871 (3.0961)  time: 0.1476  data: 0.0778  max mem: 3355\n",
            "Epoch: [6]  [510/781]  eta: 0:00:39  lr: 0.000036  loss: 2.8509 (3.0938)  time: 0.1520  data: 0.0796  max mem: 3355\n",
            "Epoch: [6]  [520/781]  eta: 0:00:37  lr: 0.000036  loss: 2.9053 (3.1004)  time: 0.1449  data: 0.0731  max mem: 3355\n",
            "Epoch: [6]  [530/781]  eta: 0:00:36  lr: 0.000036  loss: 2.9130 (3.1007)  time: 0.1394  data: 0.0695  max mem: 3355\n",
            "Epoch: [6]  [540/781]  eta: 0:00:35  lr: 0.000036  loss: 2.8768 (3.0980)  time: 0.1416  data: 0.0727  max mem: 3355\n",
            "Epoch: [6]  [550/781]  eta: 0:00:33  lr: 0.000036  loss: 2.8048 (3.0922)  time: 0.1419  data: 0.0736  max mem: 3355\n",
            "Epoch: [6]  [560/781]  eta: 0:00:32  lr: 0.000036  loss: 2.7939 (3.0869)  time: 0.1387  data: 0.0709  max mem: 3355\n",
            "Epoch: [6]  [570/781]  eta: 0:00:30  lr: 0.000036  loss: 2.8723 (3.0873)  time: 0.1434  data: 0.0753  max mem: 3355\n",
            "Epoch: [6]  [580/781]  eta: 0:00:29  lr: 0.000036  loss: 2.9633 (3.0893)  time: 0.1503  data: 0.0827  max mem: 3355\n",
            "Epoch: [6]  [590/781]  eta: 0:00:27  lr: 0.000036  loss: 2.9799 (3.0982)  time: 0.1552  data: 0.0876  max mem: 3355\n",
            "Epoch: [6]  [600/781]  eta: 0:00:26  lr: 0.000036  loss: 2.8932 (3.0957)  time: 0.1586  data: 0.0897  max mem: 3355\n",
            "Epoch: [6]  [610/781]  eta: 0:00:24  lr: 0.000036  loss: 2.8710 (3.1004)  time: 0.1484  data: 0.0803  max mem: 3355\n",
            "Epoch: [6]  [620/781]  eta: 0:00:23  lr: 0.000036  loss: 2.8707 (3.0961)  time: 0.1383  data: 0.0723  max mem: 3355\n",
            "Epoch: [6]  [630/781]  eta: 0:00:21  lr: 0.000036  loss: 2.8593 (3.0972)  time: 0.1413  data: 0.0752  max mem: 3355\n",
            "Epoch: [6]  [640/781]  eta: 0:00:20  lr: 0.000036  loss: 2.8593 (3.0940)  time: 0.1409  data: 0.0741  max mem: 3355\n",
            "Epoch: [6]  [650/781]  eta: 0:00:18  lr: 0.000036  loss: 2.8821 (3.0968)  time: 0.1403  data: 0.0736  max mem: 3355\n",
            "Epoch: [6]  [660/781]  eta: 0:00:17  lr: 0.000036  loss: 2.8393 (3.0926)  time: 0.1421  data: 0.0747  max mem: 3355\n",
            "Epoch: [6]  [670/781]  eta: 0:00:16  lr: 0.000036  loss: 2.8359 (3.0918)  time: 0.1413  data: 0.0744  max mem: 3355\n",
            "Epoch: [6]  [680/781]  eta: 0:00:14  lr: 0.000036  loss: 2.8439 (3.0912)  time: 0.1474  data: 0.0813  max mem: 3355\n",
            "Epoch: [6]  [690/781]  eta: 0:00:13  lr: 0.000036  loss: 2.9019 (3.0952)  time: 0.1500  data: 0.0814  max mem: 3355\n",
            "Epoch: [6]  [700/781]  eta: 0:00:11  lr: 0.000036  loss: 2.9107 (3.0951)  time: 0.1492  data: 0.0809  max mem: 3355\n",
            "Epoch: [6]  [710/781]  eta: 0:00:10  lr: 0.000036  loss: 2.8830 (3.0942)  time: 0.1522  data: 0.0830  max mem: 3355\n",
            "Epoch: [6]  [720/781]  eta: 0:00:08  lr: 0.000036  loss: 2.8695 (3.0911)  time: 0.1381  data: 0.0675  max mem: 3355\n",
            "Epoch: [6]  [730/781]  eta: 0:00:07  lr: 0.000036  loss: 2.8019 (3.0905)  time: 0.1339  data: 0.0651  max mem: 3355\n",
            "Epoch: [6]  [740/781]  eta: 0:00:05  lr: 0.000036  loss: 2.8019 (3.0900)  time: 0.1334  data: 0.0641  max mem: 3355\n",
            "Epoch: [6]  [750/781]  eta: 0:00:04  lr: 0.000036  loss: 2.9146 (3.0909)  time: 0.1267  data: 0.0586  max mem: 3355\n",
            "Epoch: [6]  [760/781]  eta: 0:00:03  lr: 0.000036  loss: 2.8908 (3.0891)  time: 0.1318  data: 0.0657  max mem: 3355\n",
            "Epoch: [6]  [770/781]  eta: 0:00:01  lr: 0.000036  loss: 2.7670 (3.0851)  time: 0.1381  data: 0.0686  max mem: 3355\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000036  loss: 2.7820 (3.0820)  time: 0.1356  data: 0.0668  max mem: 3355\n",
            "Epoch: [6] Total time: 0:01:52 (0.1444 s / it)\n",
            "Averaged stats: lr: 0.000036  loss: 2.7820 (3.0820)\n",
            "Test:  [ 0/53]  eta: 0:00:48  loss: 0.9126 (0.9126)  acc1: 77.0833 (77.0833)  acc5: 93.7500 (93.7500)  time: 0.9065  data: 0.8771  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1094 (1.0466)  acc1: 74.4792 (74.8580)  acc5: 92.1875 (91.9981)  time: 0.1759  data: 0.1466  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2058 (1.1199)  acc1: 71.8750 (73.7351)  acc5: 90.1042 (90.9474)  time: 0.1255  data: 0.0963  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2489 (1.1692)  acc1: 70.3125 (72.8159)  acc5: 89.5833 (90.3730)  time: 0.1284  data: 0.0992  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3042 (1.2305)  acc1: 69.2708 (71.2907)  acc5: 88.5417 (89.6341)  time: 0.1259  data: 0.0967  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2775 (1.2170)  acc1: 69.2708 (71.5278)  acc5: 89.5833 (89.8591)  time: 0.1278  data: 0.0986  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2775 (1.2196)  acc1: 69.2708 (71.4600)  acc5: 90.1042 (89.9300)  time: 0.1097  data: 0.0813  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1349 s / it)\n",
            "* Acc@1 71.460 Acc@5 89.930 loss 1.220\n",
            "Accuracy of the network on the 10000 test images: 71.5%\n",
            "Max accuracy: 71.46%\n",
            "Epoch: [7]  [  0/781]  eta: 0:11:32  lr: 0.000028  loss: 2.9971 (2.9971)  time: 0.8862  data: 0.8122  max mem: 3355\n",
            "Epoch: [7]  [ 10/781]  eta: 0:02:34  lr: 0.000028  loss: 2.7313 (2.9048)  time: 0.1998  data: 0.1308  max mem: 3355\n",
            "Epoch: [7]  [ 20/781]  eta: 0:02:16  lr: 0.000028  loss: 2.7823 (3.0379)  time: 0.1444  data: 0.0737  max mem: 3355\n",
            "Epoch: [7]  [ 30/781]  eta: 0:02:01  lr: 0.000028  loss: 2.8768 (3.0742)  time: 0.1402  data: 0.0697  max mem: 3355\n",
            "Epoch: [7]  [ 40/781]  eta: 0:01:55  lr: 0.000028  loss: 2.8706 (3.0471)  time: 0.1312  data: 0.0621  max mem: 3355\n",
            "Epoch: [7]  [ 50/781]  eta: 0:01:48  lr: 0.000028  loss: 2.8245 (3.0613)  time: 0.1268  data: 0.0574  max mem: 3355\n",
            "Epoch: [7]  [ 60/781]  eta: 0:01:45  lr: 0.000028  loss: 2.8245 (3.0678)  time: 0.1253  data: 0.0566  max mem: 3355\n",
            "Epoch: [7]  [ 70/781]  eta: 0:01:41  lr: 0.000028  loss: 2.7178 (3.0456)  time: 0.1303  data: 0.0617  max mem: 3355\n",
            "Epoch: [7]  [ 80/781]  eta: 0:01:39  lr: 0.000028  loss: 2.7074 (3.0728)  time: 0.1320  data: 0.0641  max mem: 3355\n",
            "Epoch: [7]  [ 90/781]  eta: 0:01:37  lr: 0.000028  loss: 2.8156 (3.1026)  time: 0.1336  data: 0.0650  max mem: 3355\n",
            "Epoch: [7]  [100/781]  eta: 0:01:37  lr: 0.000028  loss: 2.8156 (3.0975)  time: 0.1437  data: 0.0743  max mem: 3355\n",
            "Epoch: [7]  [110/781]  eta: 0:01:34  lr: 0.000028  loss: 2.8517 (3.1063)  time: 0.1387  data: 0.0694  max mem: 3355\n",
            "Epoch: [7]  [120/781]  eta: 0:01:32  lr: 0.000028  loss: 2.8713 (3.0924)  time: 0.1292  data: 0.0572  max mem: 3355\n",
            "Epoch: [7]  [130/781]  eta: 0:01:30  lr: 0.000028  loss: 2.7358 (3.0745)  time: 0.1282  data: 0.0556  max mem: 3355\n",
            "Epoch: [7]  [140/781]  eta: 0:01:28  lr: 0.000028  loss: 2.7429 (3.0548)  time: 0.1263  data: 0.0578  max mem: 3355\n",
            "Epoch: [7]  [150/781]  eta: 0:01:26  lr: 0.000028  loss: 2.7749 (3.0526)  time: 0.1275  data: 0.0603  max mem: 3355\n",
            "Epoch: [7]  [160/781]  eta: 0:01:25  lr: 0.000028  loss: 2.8043 (3.0625)  time: 0.1289  data: 0.0608  max mem: 3355\n",
            "Epoch: [7]  [170/781]  eta: 0:01:23  lr: 0.000028  loss: 2.8055 (3.0749)  time: 0.1301  data: 0.0634  max mem: 3355\n",
            "Epoch: [7]  [180/781]  eta: 0:01:21  lr: 0.000028  loss: 2.8424 (3.0810)  time: 0.1243  data: 0.0587  max mem: 3355\n",
            "Epoch: [7]  [190/781]  eta: 0:01:20  lr: 0.000028  loss: 2.8155 (3.0715)  time: 0.1327  data: 0.0664  max mem: 3355\n",
            "Epoch: [7]  [200/781]  eta: 0:01:19  lr: 0.000028  loss: 2.7983 (3.0674)  time: 0.1381  data: 0.0696  max mem: 3355\n",
            "Epoch: [7]  [210/781]  eta: 0:01:17  lr: 0.000028  loss: 2.7408 (3.0507)  time: 0.1344  data: 0.0654  max mem: 3355\n",
            "Epoch: [7]  [220/781]  eta: 0:01:16  lr: 0.000028  loss: 2.7050 (3.0350)  time: 0.1362  data: 0.0700  max mem: 3355\n",
            "Epoch: [7]  [230/781]  eta: 0:01:14  lr: 0.000028  loss: 2.7324 (3.0330)  time: 0.1300  data: 0.0635  max mem: 3355\n",
            "Epoch: [7]  [240/781]  eta: 0:01:13  lr: 0.000028  loss: 2.8197 (3.0262)  time: 0.1277  data: 0.0611  max mem: 3355\n",
            "Epoch: [7]  [250/781]  eta: 0:01:11  lr: 0.000028  loss: 2.7617 (3.0173)  time: 0.1271  data: 0.0617  max mem: 3355\n",
            "Epoch: [7]  [260/781]  eta: 0:01:10  lr: 0.000028  loss: 2.7365 (3.0083)  time: 0.1271  data: 0.0615  max mem: 3355\n",
            "Epoch: [7]  [270/781]  eta: 0:01:08  lr: 0.000028  loss: 2.7863 (3.0088)  time: 0.1287  data: 0.0632  max mem: 3355\n",
            "Epoch: [7]  [280/781]  eta: 0:01:07  lr: 0.000028  loss: 2.8344 (3.0173)  time: 0.1288  data: 0.0632  max mem: 3355\n",
            "Epoch: [7]  [290/781]  eta: 0:01:05  lr: 0.000028  loss: 2.8765 (3.0153)  time: 0.1313  data: 0.0653  max mem: 3355\n",
            "Epoch: [7]  [300/781]  eta: 0:01:04  lr: 0.000028  loss: 2.8717 (3.0139)  time: 0.1352  data: 0.0668  max mem: 3355\n",
            "Epoch: [7]  [310/781]  eta: 0:01:03  lr: 0.000028  loss: 2.8162 (3.0084)  time: 0.1348  data: 0.0631  max mem: 3355\n",
            "Epoch: [7]  [320/781]  eta: 0:01:01  lr: 0.000028  loss: 2.6398 (2.9993)  time: 0.1295  data: 0.0586  max mem: 3355\n",
            "Epoch: [7]  [330/781]  eta: 0:01:00  lr: 0.000028  loss: 2.7281 (2.9987)  time: 0.1326  data: 0.0637  max mem: 3355\n",
            "Epoch: [7]  [340/781]  eta: 0:00:59  lr: 0.000028  loss: 2.7647 (2.9931)  time: 0.1326  data: 0.0635  max mem: 3355\n",
            "Epoch: [7]  [350/781]  eta: 0:00:57  lr: 0.000028  loss: 2.7340 (2.9862)  time: 0.1306  data: 0.0614  max mem: 3355\n",
            "Epoch: [7]  [360/781]  eta: 0:00:56  lr: 0.000028  loss: 2.7254 (2.9790)  time: 0.1296  data: 0.0626  max mem: 3355\n",
            "Epoch: [7]  [370/781]  eta: 0:00:54  lr: 0.000028  loss: 2.6657 (2.9721)  time: 0.1327  data: 0.0676  max mem: 3355\n",
            "Epoch: [7]  [380/781]  eta: 0:00:53  lr: 0.000028  loss: 2.7523 (2.9755)  time: 0.1278  data: 0.0616  max mem: 3355\n",
            "Epoch: [7]  [390/781]  eta: 0:00:52  lr: 0.000028  loss: 2.7856 (2.9730)  time: 0.1297  data: 0.0607  max mem: 3355\n",
            "Epoch: [7]  [400/781]  eta: 0:00:50  lr: 0.000028  loss: 2.8975 (2.9758)  time: 0.1345  data: 0.0634  max mem: 3355\n",
            "Epoch: [7]  [410/781]  eta: 0:00:49  lr: 0.000028  loss: 2.8029 (2.9775)  time: 0.1326  data: 0.0632  max mem: 3355\n",
            "Epoch: [7]  [420/781]  eta: 0:00:48  lr: 0.000028  loss: 2.8029 (2.9784)  time: 0.1244  data: 0.0568  max mem: 3355\n",
            "Epoch: [7]  [430/781]  eta: 0:00:46  lr: 0.000028  loss: 2.8135 (2.9789)  time: 0.1289  data: 0.0610  max mem: 3355\n",
            "Epoch: [7]  [440/781]  eta: 0:00:45  lr: 0.000028  loss: 2.8877 (2.9850)  time: 0.1302  data: 0.0636  max mem: 3355\n",
            "Epoch: [7]  [450/781]  eta: 0:00:44  lr: 0.000028  loss: 2.8868 (2.9870)  time: 0.1282  data: 0.0625  max mem: 3355\n",
            "Epoch: [7]  [460/781]  eta: 0:00:42  lr: 0.000028  loss: 2.7744 (2.9883)  time: 0.1285  data: 0.0627  max mem: 3355\n",
            "Epoch: [7]  [470/781]  eta: 0:00:41  lr: 0.000028  loss: 2.8550 (2.9916)  time: 0.1301  data: 0.0643  max mem: 3355\n",
            "Epoch: [7]  [480/781]  eta: 0:00:39  lr: 0.000028  loss: 2.8472 (2.9896)  time: 0.1273  data: 0.0606  max mem: 3355\n",
            "Epoch: [7]  [490/781]  eta: 0:00:38  lr: 0.000028  loss: 2.8224 (2.9914)  time: 0.1348  data: 0.0666  max mem: 3355\n",
            "Epoch: [7]  [500/781]  eta: 0:00:37  lr: 0.000028  loss: 2.8099 (2.9951)  time: 0.1378  data: 0.0691  max mem: 3355\n",
            "Epoch: [7]  [510/781]  eta: 0:00:36  lr: 0.000028  loss: 2.8328 (2.9956)  time: 0.1345  data: 0.0671  max mem: 3355\n",
            "Epoch: [7]  [520/781]  eta: 0:00:34  lr: 0.000028  loss: 2.8151 (2.9927)  time: 0.1334  data: 0.0664  max mem: 3355\n",
            "Epoch: [7]  [530/781]  eta: 0:00:33  lr: 0.000028  loss: 2.8501 (2.9980)  time: 0.1308  data: 0.0637  max mem: 3355\n",
            "Epoch: [7]  [540/781]  eta: 0:00:31  lr: 0.000028  loss: 2.8524 (2.9990)  time: 0.1274  data: 0.0601  max mem: 3355\n",
            "Epoch: [7]  [550/781]  eta: 0:00:30  lr: 0.000028  loss: 2.8466 (2.9961)  time: 0.1270  data: 0.0604  max mem: 3355\n",
            "Epoch: [7]  [560/781]  eta: 0:00:29  lr: 0.000028  loss: 2.8278 (2.9981)  time: 0.1307  data: 0.0654  max mem: 3355\n",
            "Epoch: [7]  [570/781]  eta: 0:00:28  lr: 0.000028  loss: 2.8151 (2.9990)  time: 0.1365  data: 0.0706  max mem: 3355\n",
            "Epoch: [7]  [580/781]  eta: 0:00:26  lr: 0.000028  loss: 2.8580 (3.0035)  time: 0.1373  data: 0.0683  max mem: 3355\n",
            "Epoch: [7]  [590/781]  eta: 0:00:25  lr: 0.000028  loss: 2.9156 (3.0053)  time: 0.1351  data: 0.0647  max mem: 3355\n",
            "Epoch: [7]  [600/781]  eta: 0:00:24  lr: 0.000028  loss: 2.9387 (3.0046)  time: 0.1351  data: 0.0659  max mem: 3355\n",
            "Epoch: [7]  [610/781]  eta: 0:00:22  lr: 0.000028  loss: 2.9387 (3.0075)  time: 0.1363  data: 0.0679  max mem: 3355\n",
            "Epoch: [7]  [620/781]  eta: 0:00:21  lr: 0.000028  loss: 2.9097 (3.0057)  time: 0.1341  data: 0.0676  max mem: 3355\n",
            "Epoch: [7]  [630/781]  eta: 0:00:20  lr: 0.000028  loss: 2.8608 (3.0050)  time: 0.1314  data: 0.0663  max mem: 3355\n",
            "Epoch: [7]  [640/781]  eta: 0:00:18  lr: 0.000028  loss: 2.7811 (3.0040)  time: 0.1306  data: 0.0655  max mem: 3355\n",
            "Epoch: [7]  [650/781]  eta: 0:00:17  lr: 0.000028  loss: 2.7279 (3.0002)  time: 0.1279  data: 0.0620  max mem: 3355\n",
            "Epoch: [7]  [660/781]  eta: 0:00:16  lr: 0.000028  loss: 2.7734 (2.9988)  time: 0.1279  data: 0.0604  max mem: 3355\n",
            "Epoch: [7]  [670/781]  eta: 0:00:14  lr: 0.000028  loss: 2.8692 (2.9986)  time: 0.1304  data: 0.0588  max mem: 3355\n",
            "Epoch: [7]  [680/781]  eta: 0:00:13  lr: 0.000028  loss: 2.7222 (2.9967)  time: 0.1327  data: 0.0618  max mem: 3355\n",
            "Epoch: [7]  [690/781]  eta: 0:00:12  lr: 0.000028  loss: 2.7010 (2.9930)  time: 0.1332  data: 0.0655  max mem: 3355\n",
            "Epoch: [7]  [700/781]  eta: 0:00:10  lr: 0.000028  loss: 2.7875 (2.9907)  time: 0.1297  data: 0.0612  max mem: 3355\n",
            "Epoch: [7]  [710/781]  eta: 0:00:09  lr: 0.000028  loss: 2.8170 (2.9942)  time: 0.1251  data: 0.0586  max mem: 3355\n",
            "Epoch: [7]  [720/781]  eta: 0:00:08  lr: 0.000028  loss: 2.7917 (2.9927)  time: 0.1295  data: 0.0647  max mem: 3355\n",
            "Epoch: [7]  [730/781]  eta: 0:00:06  lr: 0.000028  loss: 2.7054 (2.9900)  time: 0.1275  data: 0.0625  max mem: 3355\n",
            "Epoch: [7]  [740/781]  eta: 0:00:05  lr: 0.000028  loss: 2.7547 (2.9868)  time: 0.1325  data: 0.0676  max mem: 3355\n",
            "Epoch: [7]  [750/781]  eta: 0:00:04  lr: 0.000028  loss: 2.7778 (2.9860)  time: 0.1314  data: 0.0666  max mem: 3355\n",
            "Epoch: [7]  [760/781]  eta: 0:00:02  lr: 0.000028  loss: 2.6857 (2.9833)  time: 0.1290  data: 0.0627  max mem: 3355\n",
            "Epoch: [7]  [770/781]  eta: 0:00:01  lr: 0.000028  loss: 2.6859 (2.9823)  time: 0.1407  data: 0.0711  max mem: 3355\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000028  loss: 2.8177 (2.9829)  time: 0.1350  data: 0.0648  max mem: 3355\n",
            "Epoch: [7] Total time: 0:01:43 (0.1325 s / it)\n",
            "Averaged stats: lr: 0.000028  loss: 2.8177 (2.9829)\n",
            "Test:  [ 0/53]  eta: 0:00:46  loss: 0.8499 (0.8499)  acc1: 79.1667 (79.1667)  acc5: 93.2292 (93.2292)  time: 0.8712  data: 0.8417  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1286 (1.0625)  acc1: 72.9167 (74.2898)  acc5: 92.7083 (92.0928)  time: 0.1779  data: 0.1486  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1311 (1.1177)  acc1: 71.8750 (73.6855)  acc5: 90.1042 (91.1458)  time: 0.1292  data: 0.0999  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2844 (1.1638)  acc1: 69.7917 (72.9671)  acc5: 89.5833 (90.6082)  time: 0.1335  data: 0.1042  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3316 (1.2144)  acc1: 70.3125 (71.7353)  acc5: 88.5417 (90.0534)  time: 0.1342  data: 0.1050  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2258 (1.2087)  acc1: 70.3125 (71.8546)  acc5: 90.1042 (90.1450)  time: 0.1318  data: 0.1025  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3316 (1.2213)  acc1: 69.7917 (71.6600)  acc5: 90.6250 (90.1800)  time: 0.1110  data: 0.0825  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1389 s / it)\n",
            "* Acc@1 71.660 Acc@5 90.180 loss 1.221\n",
            "Accuracy of the network on the 10000 test images: 71.7%\n",
            "Max accuracy: 71.66%\n",
            "Epoch: [8]  [  0/781]  eta: 0:11:22  lr: 0.000021  loss: 2.7594 (2.7594)  time: 0.8734  data: 0.7977  max mem: 3355\n",
            "Epoch: [8]  [ 10/781]  eta: 0:02:33  lr: 0.000021  loss: 2.7594 (3.0344)  time: 0.1991  data: 0.1312  max mem: 3355\n",
            "Epoch: [8]  [ 20/781]  eta: 0:02:20  lr: 0.000021  loss: 2.8250 (3.0393)  time: 0.1499  data: 0.0819  max mem: 3355\n",
            "Epoch: [8]  [ 30/781]  eta: 0:02:05  lr: 0.000021  loss: 2.8296 (2.9727)  time: 0.1485  data: 0.0792  max mem: 3355\n",
            "Epoch: [8]  [ 40/781]  eta: 0:01:58  lr: 0.000021  loss: 2.7812 (2.9641)  time: 0.1344  data: 0.0654  max mem: 3355\n",
            "Epoch: [8]  [ 50/781]  eta: 0:01:52  lr: 0.000021  loss: 2.6941 (2.9490)  time: 0.1361  data: 0.0681  max mem: 3355\n",
            "Epoch: [8]  [ 60/781]  eta: 0:01:49  lr: 0.000021  loss: 2.7784 (2.9955)  time: 0.1360  data: 0.0687  max mem: 3355\n",
            "Epoch: [8]  [ 70/781]  eta: 0:01:47  lr: 0.000021  loss: 2.8129 (3.0177)  time: 0.1418  data: 0.0749  max mem: 3355\n",
            "Epoch: [8]  [ 80/781]  eta: 0:01:43  lr: 0.000021  loss: 2.7514 (3.0425)  time: 0.1342  data: 0.0663  max mem: 3355\n",
            "Epoch: [8]  [ 90/781]  eta: 0:01:41  lr: 0.000021  loss: 2.7376 (3.0392)  time: 0.1352  data: 0.0667  max mem: 3355\n",
            "Epoch: [8]  [100/781]  eta: 0:01:39  lr: 0.000021  loss: 2.7639 (3.0238)  time: 0.1389  data: 0.0693  max mem: 3355\n",
            "Epoch: [8]  [110/781]  eta: 0:01:38  lr: 0.000021  loss: 2.7365 (3.0028)  time: 0.1433  data: 0.0717  max mem: 3355\n",
            "Epoch: [8]  [120/781]  eta: 0:01:35  lr: 0.000021  loss: 2.5840 (2.9834)  time: 0.1383  data: 0.0683  max mem: 3355\n",
            "Epoch: [8]  [130/781]  eta: 0:01:34  lr: 0.000021  loss: 2.6981 (2.9714)  time: 0.1369  data: 0.0704  max mem: 3355\n",
            "Epoch: [8]  [140/781]  eta: 0:01:31  lr: 0.000021  loss: 2.7496 (2.9711)  time: 0.1358  data: 0.0597  max mem: 3355\n",
            "Epoch: [8]  [150/781]  eta: 0:01:31  lr: 0.000021  loss: 2.7536 (2.9815)  time: 0.1405  data: 0.0630  max mem: 3355\n",
            "Epoch: [8]  [160/781]  eta: 0:01:29  lr: 0.000021  loss: 2.7833 (2.9764)  time: 0.1428  data: 0.0746  max mem: 3355\n",
            "Epoch: [8]  [170/781]  eta: 0:01:28  lr: 0.000021  loss: 2.7833 (2.9755)  time: 0.1453  data: 0.0764  max mem: 3355\n",
            "Epoch: [8]  [180/781]  eta: 0:01:26  lr: 0.000021  loss: 2.6772 (2.9630)  time: 0.1443  data: 0.0761  max mem: 3355\n",
            "Epoch: [8]  [190/781]  eta: 0:01:25  lr: 0.000021  loss: 2.6916 (2.9630)  time: 0.1445  data: 0.0743  max mem: 3355\n",
            "Epoch: [8]  [200/781]  eta: 0:01:23  lr: 0.000021  loss: 2.7315 (2.9619)  time: 0.1498  data: 0.0780  max mem: 3355\n",
            "Epoch: [8]  [210/781]  eta: 0:01:22  lr: 0.000021  loss: 2.7575 (2.9593)  time: 0.1410  data: 0.0725  max mem: 3355\n",
            "Epoch: [8]  [220/781]  eta: 0:01:20  lr: 0.000021  loss: 2.6949 (2.9451)  time: 0.1395  data: 0.0730  max mem: 3355\n",
            "Epoch: [8]  [230/781]  eta: 0:01:19  lr: 0.000021  loss: 2.7005 (2.9440)  time: 0.1448  data: 0.0779  max mem: 3355\n",
            "Epoch: [8]  [240/781]  eta: 0:01:17  lr: 0.000021  loss: 2.7595 (2.9412)  time: 0.1452  data: 0.0784  max mem: 3355\n",
            "Epoch: [8]  [250/781]  eta: 0:01:16  lr: 0.000021  loss: 2.7349 (2.9438)  time: 0.1449  data: 0.0778  max mem: 3355\n",
            "Epoch: [8]  [260/781]  eta: 0:01:15  lr: 0.000021  loss: 2.7349 (2.9504)  time: 0.1478  data: 0.0807  max mem: 3355\n",
            "Epoch: [8]  [270/781]  eta: 0:01:13  lr: 0.000021  loss: 2.7338 (2.9511)  time: 0.1476  data: 0.0803  max mem: 3355\n",
            "Epoch: [8]  [280/781]  eta: 0:01:12  lr: 0.000021  loss: 2.8979 (2.9665)  time: 0.1414  data: 0.0722  max mem: 3355\n",
            "Epoch: [8]  [290/781]  eta: 0:01:11  lr: 0.000021  loss: 2.7891 (2.9579)  time: 0.1445  data: 0.0756  max mem: 3355\n",
            "Epoch: [8]  [300/781]  eta: 0:01:09  lr: 0.000021  loss: 2.7433 (2.9578)  time: 0.1449  data: 0.0767  max mem: 3355\n",
            "Epoch: [8]  [310/781]  eta: 0:01:07  lr: 0.000021  loss: 2.7594 (2.9625)  time: 0.1367  data: 0.0688  max mem: 3355\n",
            "Epoch: [8]  [320/781]  eta: 0:01:06  lr: 0.000021  loss: 2.7793 (2.9593)  time: 0.1357  data: 0.0680  max mem: 3355\n",
            "Epoch: [8]  [330/781]  eta: 0:01:04  lr: 0.000021  loss: 2.7005 (2.9533)  time: 0.1391  data: 0.0710  max mem: 3355\n",
            "Epoch: [8]  [340/781]  eta: 0:01:03  lr: 0.000021  loss: 2.6779 (2.9519)  time: 0.1392  data: 0.0716  max mem: 3355\n",
            "Epoch: [8]  [350/781]  eta: 0:01:01  lr: 0.000021  loss: 2.6798 (2.9434)  time: 0.1427  data: 0.0718  max mem: 3355\n",
            "Epoch: [8]  [360/781]  eta: 0:01:00  lr: 0.000021  loss: 2.6974 (2.9425)  time: 0.1455  data: 0.0720  max mem: 3355\n",
            "Epoch: [8]  [370/781]  eta: 0:00:59  lr: 0.000021  loss: 2.7192 (2.9369)  time: 0.1433  data: 0.0738  max mem: 3355\n",
            "Epoch: [8]  [380/781]  eta: 0:00:57  lr: 0.000021  loss: 2.7341 (2.9366)  time: 0.1428  data: 0.0751  max mem: 3355\n",
            "Epoch: [8]  [390/781]  eta: 0:00:56  lr: 0.000021  loss: 2.7649 (2.9514)  time: 0.1437  data: 0.0767  max mem: 3355\n",
            "Epoch: [8]  [400/781]  eta: 0:00:54  lr: 0.000021  loss: 2.7994 (2.9474)  time: 0.1459  data: 0.0800  max mem: 3355\n",
            "Epoch: [8]  [410/781]  eta: 0:00:53  lr: 0.000021  loss: 2.7994 (2.9537)  time: 0.1432  data: 0.0764  max mem: 3355\n",
            "Epoch: [8]  [420/781]  eta: 0:00:51  lr: 0.000021  loss: 2.7199 (2.9504)  time: 0.1460  data: 0.0787  max mem: 3355\n",
            "Epoch: [8]  [430/781]  eta: 0:00:50  lr: 0.000021  loss: 2.6712 (2.9450)  time: 0.1445  data: 0.0764  max mem: 3355\n",
            "Epoch: [8]  [440/781]  eta: 0:00:48  lr: 0.000021  loss: 2.7001 (2.9406)  time: 0.1362  data: 0.0684  max mem: 3355\n",
            "Epoch: [8]  [450/781]  eta: 0:00:47  lr: 0.000021  loss: 2.6708 (2.9379)  time: 0.1405  data: 0.0724  max mem: 3355\n",
            "Epoch: [8]  [460/781]  eta: 0:00:46  lr: 0.000021  loss: 2.6626 (2.9332)  time: 0.1454  data: 0.0739  max mem: 3355\n",
            "Epoch: [8]  [470/781]  eta: 0:00:44  lr: 0.000021  loss: 2.7528 (2.9399)  time: 0.1479  data: 0.0774  max mem: 3355\n",
            "Epoch: [8]  [480/781]  eta: 0:00:43  lr: 0.000021  loss: 2.9423 (2.9485)  time: 0.1469  data: 0.0784  max mem: 3355\n",
            "Epoch: [8]  [490/781]  eta: 0:00:41  lr: 0.000021  loss: 2.7384 (2.9456)  time: 0.1521  data: 0.0841  max mem: 3355\n",
            "Epoch: [8]  [500/781]  eta: 0:00:40  lr: 0.000021  loss: 2.7331 (2.9467)  time: 0.1512  data: 0.0843  max mem: 3355\n",
            "Epoch: [8]  [510/781]  eta: 0:00:39  lr: 0.000021  loss: 2.7690 (2.9489)  time: 0.1460  data: 0.0765  max mem: 3355\n",
            "Epoch: [8]  [520/781]  eta: 0:00:37  lr: 0.000021  loss: 2.8002 (2.9500)  time: 0.1455  data: 0.0757  max mem: 3355\n",
            "Epoch: [8]  [530/781]  eta: 0:00:36  lr: 0.000021  loss: 2.7731 (2.9482)  time: 0.1445  data: 0.0745  max mem: 3355\n",
            "Epoch: [8]  [540/781]  eta: 0:00:34  lr: 0.000021  loss: 2.7503 (2.9458)  time: 0.1472  data: 0.0761  max mem: 3355\n",
            "Epoch: [8]  [550/781]  eta: 0:00:33  lr: 0.000021  loss: 2.7773 (2.9421)  time: 0.1432  data: 0.0740  max mem: 3355\n",
            "Epoch: [8]  [560/781]  eta: 0:00:31  lr: 0.000021  loss: 2.7814 (2.9479)  time: 0.1444  data: 0.0766  max mem: 3355\n",
            "Epoch: [8]  [570/781]  eta: 0:00:30  lr: 0.000021  loss: 2.7794 (2.9479)  time: 0.1478  data: 0.0808  max mem: 3355\n",
            "Epoch: [8]  [580/781]  eta: 0:00:28  lr: 0.000021  loss: 2.7116 (2.9443)  time: 0.1471  data: 0.0802  max mem: 3355\n",
            "Epoch: [8]  [590/781]  eta: 0:00:27  lr: 0.000021  loss: 2.6631 (2.9503)  time: 0.1501  data: 0.0821  max mem: 3355\n",
            "Epoch: [8]  [600/781]  eta: 0:00:26  lr: 0.000021  loss: 2.6365 (2.9468)  time: 0.1440  data: 0.0744  max mem: 3355\n",
            "Epoch: [8]  [610/781]  eta: 0:00:24  lr: 0.000021  loss: 2.6993 (2.9449)  time: 0.1362  data: 0.0671  max mem: 3355\n",
            "Epoch: [8]  [620/781]  eta: 0:00:23  lr: 0.000021  loss: 2.8402 (2.9491)  time: 0.1450  data: 0.0764  max mem: 3355\n",
            "Epoch: [8]  [630/781]  eta: 0:00:21  lr: 0.000021  loss: 2.7712 (2.9465)  time: 0.1493  data: 0.0794  max mem: 3355\n",
            "Epoch: [8]  [640/781]  eta: 0:00:20  lr: 0.000021  loss: 2.7590 (2.9485)  time: 0.1452  data: 0.0760  max mem: 3355\n",
            "Epoch: [8]  [650/781]  eta: 0:00:18  lr: 0.000021  loss: 2.7531 (2.9490)  time: 0.1398  data: 0.0715  max mem: 3355\n",
            "Epoch: [8]  [660/781]  eta: 0:00:17  lr: 0.000021  loss: 2.7617 (2.9487)  time: 0.1384  data: 0.0707  max mem: 3355\n",
            "Epoch: [8]  [670/781]  eta: 0:00:15  lr: 0.000021  loss: 2.7151 (2.9480)  time: 0.1401  data: 0.0727  max mem: 3355\n",
            "Epoch: [8]  [680/781]  eta: 0:00:14  lr: 0.000021  loss: 2.7221 (2.9528)  time: 0.1396  data: 0.0720  max mem: 3355\n",
            "Epoch: [8]  [690/781]  eta: 0:00:13  lr: 0.000021  loss: 2.8268 (2.9512)  time: 0.1401  data: 0.0728  max mem: 3355\n",
            "Epoch: [8]  [700/781]  eta: 0:00:11  lr: 0.000021  loss: 2.7247 (2.9504)  time: 0.1377  data: 0.0708  max mem: 3355\n",
            "Epoch: [8]  [710/781]  eta: 0:00:10  lr: 0.000021  loss: 2.7220 (2.9494)  time: 0.1369  data: 0.0698  max mem: 3355\n",
            "Epoch: [8]  [720/781]  eta: 0:00:08  lr: 0.000021  loss: 2.6752 (2.9468)  time: 0.1423  data: 0.0734  max mem: 3355\n",
            "Epoch: [8]  [730/781]  eta: 0:00:07  lr: 0.000021  loss: 2.7119 (2.9470)  time: 0.1480  data: 0.0792  max mem: 3355\n",
            "Epoch: [8]  [740/781]  eta: 0:00:05  lr: 0.000021  loss: 2.7383 (2.9474)  time: 0.1426  data: 0.0755  max mem: 3355\n",
            "Epoch: [8]  [750/781]  eta: 0:00:04  lr: 0.000021  loss: 2.8084 (2.9522)  time: 0.1385  data: 0.0714  max mem: 3355\n",
            "Epoch: [8]  [760/781]  eta: 0:00:03  lr: 0.000021  loss: 2.7765 (2.9495)  time: 0.1395  data: 0.0716  max mem: 3355\n",
            "Epoch: [8]  [770/781]  eta: 0:00:01  lr: 0.000021  loss: 2.7703 (2.9519)  time: 0.1398  data: 0.0726  max mem: 3355\n",
            "Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000021  loss: 2.7703 (2.9482)  time: 0.1390  data: 0.0735  max mem: 3355\n",
            "Epoch: [8] Total time: 0:01:52 (0.1435 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 2.7703 (2.9482)\n",
            "Test:  [ 0/53]  eta: 0:00:49  loss: 0.8752 (0.8752)  acc1: 77.6042 (77.6042)  acc5: 94.2708 (94.2708)  time: 0.9347  data: 0.9051  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1085 (1.0034)  acc1: 73.4375 (75.0000)  acc5: 94.2708 (92.7557)  time: 0.1783  data: 0.1490  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1575 (1.0848)  acc1: 70.8333 (74.2560)  acc5: 91.1458 (91.5427)  time: 0.1262  data: 0.0969  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1756 (1.1250)  acc1: 70.8333 (73.7063)  acc5: 89.5833 (91.3306)  time: 0.1317  data: 0.1024  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2337 (1.1672)  acc1: 71.3542 (72.5864)  acc5: 89.5833 (90.8791)  time: 0.1304  data: 0.1011  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1896 (1.1628)  acc1: 71.8750 (72.7124)  acc5: 90.1042 (90.9110)  time: 0.1306  data: 0.1013  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2260 (1.1685)  acc1: 71.3542 (72.5900)  acc5: 90.6250 (90.9600)  time: 0.1113  data: 0.0829  max mem: 3355\n",
            "Test: Total time: 0:00:07 (0.1377 s / it)\n",
            "* Acc@1 72.590 Acc@5 90.960 loss 1.168\n",
            "Accuracy of the network on the 10000 test images: 72.6%\n",
            "Max accuracy: 72.59%\n",
            "Epoch: [9]  [  0/781]  eta: 0:10:03  lr: 0.000015  loss: 2.8448 (2.8448)  time: 0.7731  data: 0.6884  max mem: 3355\n",
            "Epoch: [9]  [ 10/781]  eta: 0:02:26  lr: 0.000015  loss: 2.6871 (2.6902)  time: 0.1894  data: 0.1209  max mem: 3355\n",
            "Epoch: [9]  [ 20/781]  eta: 0:02:08  lr: 0.000015  loss: 2.6851 (2.8136)  time: 0.1384  data: 0.0714  max mem: 3355\n",
            "Epoch: [9]  [ 30/781]  eta: 0:01:52  lr: 0.000015  loss: 2.7124 (2.8532)  time: 0.1285  data: 0.0611  max mem: 3355\n",
            "Epoch: [9]  [ 40/781]  eta: 0:01:49  lr: 0.000015  loss: 2.7124 (2.9067)  time: 0.1266  data: 0.0594  max mem: 3355\n",
            "Epoch: [9]  [ 50/781]  eta: 0:01:45  lr: 0.000015  loss: 2.7075 (2.9264)  time: 0.1339  data: 0.0637  max mem: 3355\n",
            "Epoch: [9]  [ 60/781]  eta: 0:01:44  lr: 0.000015  loss: 2.7282 (2.9597)  time: 0.1366  data: 0.0638  max mem: 3355\n",
            "Epoch: [9]  [ 70/781]  eta: 0:01:40  lr: 0.000015  loss: 2.7669 (3.0226)  time: 0.1342  data: 0.0630  max mem: 3355\n",
            "Epoch: [9]  [ 80/781]  eta: 0:01:39  lr: 0.000015  loss: 2.9137 (3.0494)  time: 0.1342  data: 0.0646  max mem: 3355\n",
            "Epoch: [9]  [ 90/781]  eta: 0:01:36  lr: 0.000015  loss: 2.9029 (3.0772)  time: 0.1330  data: 0.0657  max mem: 3355\n",
            "Epoch: [9]  [100/781]  eta: 0:01:35  lr: 0.000015  loss: 2.7321 (3.0406)  time: 0.1325  data: 0.0664  max mem: 3355\n",
            "Epoch: [9]  [110/781]  eta: 0:01:32  lr: 0.000015  loss: 2.6627 (3.0453)  time: 0.1349  data: 0.0675  max mem: 3355\n",
            "Epoch: [9]  [120/781]  eta: 0:01:32  lr: 0.000015  loss: 2.7578 (3.0459)  time: 0.1427  data: 0.0753  max mem: 3355\n",
            "Epoch: [9]  [130/781]  eta: 0:01:30  lr: 0.000015  loss: 2.6336 (3.0291)  time: 0.1443  data: 0.0773  max mem: 3355\n",
            "Epoch: [9]  [140/781]  eta: 0:01:30  lr: 0.000015  loss: 2.6047 (3.0524)  time: 0.1411  data: 0.0723  max mem: 3355\n",
            "Epoch: [9]  [150/781]  eta: 0:01:27  lr: 0.000015  loss: 2.6791 (3.0420)  time: 0.1391  data: 0.0702  max mem: 3355\n",
            "Epoch: [9]  [160/781]  eta: 0:01:26  lr: 0.000015  loss: 2.6791 (3.0301)  time: 0.1330  data: 0.0665  max mem: 3355\n",
            "Epoch: [9]  [170/781]  eta: 0:01:24  lr: 0.000015  loss: 2.7469 (3.0287)  time: 0.1326  data: 0.0664  max mem: 3355\n",
            "Epoch: [9]  [180/781]  eta: 0:01:23  lr: 0.000015  loss: 2.6915 (3.0194)  time: 0.1319  data: 0.0658  max mem: 3355\n",
            "Epoch: [9]  [190/781]  eta: 0:01:21  lr: 0.000015  loss: 2.6915 (3.0199)  time: 0.1291  data: 0.0640  max mem: 3355\n",
            "Epoch: [9]  [200/781]  eta: 0:01:20  lr: 0.000015  loss: 2.7526 (3.0059)  time: 0.1305  data: 0.0653  max mem: 3355\n",
            "Epoch: [9]  [210/781]  eta: 0:01:18  lr: 0.000015  loss: 2.7303 (2.9971)  time: 0.1291  data: 0.0639  max mem: 3355\n",
            "Epoch: [9]  [220/781]  eta: 0:01:17  lr: 0.000015  loss: 2.6680 (2.9942)  time: 0.1359  data: 0.0705  max mem: 3355\n",
            "Epoch: [9]  [230/781]  eta: 0:01:15  lr: 0.000015  loss: 2.6670 (2.9842)  time: 0.1446  data: 0.0776  max mem: 3355\n",
            "Epoch: [9]  [240/781]  eta: 0:01:14  lr: 0.000015  loss: 2.6250 (2.9696)  time: 0.1462  data: 0.0785  max mem: 3355\n",
            "Epoch: [9]  [250/781]  eta: 0:01:13  lr: 0.000015  loss: 2.6159 (2.9661)  time: 0.1388  data: 0.0714  max mem: 3355\n",
            "Epoch: [9]  [260/781]  eta: 0:01:11  lr: 0.000015  loss: 2.8166 (2.9744)  time: 0.1286  data: 0.0613  max mem: 3355\n",
            "Epoch: [9]  [270/781]  eta: 0:01:10  lr: 0.000015  loss: 2.8862 (2.9705)  time: 0.1326  data: 0.0660  max mem: 3355\n",
            "Epoch: [9]  [280/781]  eta: 0:01:08  lr: 0.000015  loss: 2.6402 (2.9618)  time: 0.1315  data: 0.0650  max mem: 3355\n",
            "Epoch: [9]  [290/781]  eta: 0:01:07  lr: 0.000015  loss: 2.6925 (2.9526)  time: 0.1290  data: 0.0613  max mem: 3355\n",
            "Epoch: [9]  [300/781]  eta: 0:01:05  lr: 0.000015  loss: 2.6689 (2.9511)  time: 0.1285  data: 0.0615  max mem: 3355\n",
            "Epoch: [9]  [310/781]  eta: 0:01:04  lr: 0.000015  loss: 2.5872 (2.9437)  time: 0.1262  data: 0.0611  max mem: 3355\n",
            "Epoch: [9]  [320/781]  eta: 0:01:02  lr: 0.000015  loss: 2.6689 (2.9386)  time: 0.1307  data: 0.0624  max mem: 3355\n",
            "Epoch: [9]  [330/781]  eta: 0:01:01  lr: 0.000015  loss: 2.6874 (2.9314)  time: 0.1332  data: 0.0628  max mem: 3355\n",
            "Epoch: [9]  [340/781]  eta: 0:00:59  lr: 0.000015  loss: 2.6937 (2.9296)  time: 0.1295  data: 0.0597  max mem: 3355\n",
            "Epoch: [9]  [350/781]  eta: 0:00:58  lr: 0.000015  loss: 2.6922 (2.9250)  time: 0.1307  data: 0.0619  max mem: 3355\n",
            "Epoch: [9]  [360/781]  eta: 0:00:57  lr: 0.000015  loss: 2.6922 (2.9297)  time: 0.1312  data: 0.0632  max mem: 3355\n",
            "Epoch: [9]  [370/781]  eta: 0:00:55  lr: 0.000015  loss: 2.6267 (2.9338)  time: 0.1306  data: 0.0628  max mem: 3355\n",
            "Epoch: [9]  [380/781]  eta: 0:00:54  lr: 0.000015  loss: 2.6554 (2.9326)  time: 0.1341  data: 0.0667  max mem: 3355\n",
            "Epoch: [9]  [390/781]  eta: 0:00:52  lr: 0.000015  loss: 2.6815 (2.9312)  time: 0.1366  data: 0.0661  max mem: 3355\n",
            "Epoch: [9]  [400/781]  eta: 0:00:51  lr: 0.000015  loss: 2.7203 (2.9277)  time: 0.1369  data: 0.0650  max mem: 3355\n",
            "Epoch: [9]  [410/781]  eta: 0:00:50  lr: 0.000015  loss: 2.7305 (2.9257)  time: 0.1356  data: 0.0653  max mem: 3355\n",
            "Epoch: [9]  [420/781]  eta: 0:00:48  lr: 0.000015  loss: 2.8524 (2.9370)  time: 0.1375  data: 0.0678  max mem: 3355\n",
            "Epoch: [9]  [430/781]  eta: 0:00:47  lr: 0.000015  loss: 2.9823 (2.9358)  time: 0.1432  data: 0.0738  max mem: 3355\n",
            "Epoch: [9]  [440/781]  eta: 0:00:46  lr: 0.000015  loss: 2.6964 (2.9322)  time: 0.1338  data: 0.0657  max mem: 3355\n",
            "Epoch: [9]  [450/781]  eta: 0:00:44  lr: 0.000015  loss: 2.6894 (2.9299)  time: 0.1318  data: 0.0655  max mem: 3355\n",
            "Epoch: [9]  [460/781]  eta: 0:00:43  lr: 0.000015  loss: 2.7243 (2.9290)  time: 0.1340  data: 0.0680  max mem: 3355\n",
            "Epoch: [9]  [470/781]  eta: 0:00:42  lr: 0.000015  loss: 2.7243 (2.9272)  time: 0.1286  data: 0.0620  max mem: 3355\n",
            "Epoch: [9]  [480/781]  eta: 0:00:40  lr: 0.000015  loss: 2.7232 (2.9265)  time: 0.1321  data: 0.0648  max mem: 3355\n",
            "Epoch: [9]  [490/781]  eta: 0:00:39  lr: 0.000015  loss: 2.7816 (2.9250)  time: 0.1315  data: 0.0655  max mem: 3355\n",
            "Epoch: [9]  [500/781]  eta: 0:00:37  lr: 0.000015  loss: 2.7155 (2.9285)  time: 0.1292  data: 0.0630  max mem: 3355\n",
            "Epoch: [9]  [510/781]  eta: 0:00:36  lr: 0.000015  loss: 2.7155 (2.9312)  time: 0.1309  data: 0.0622  max mem: 3355\n",
            "Epoch: [9]  [520/781]  eta: 0:00:35  lr: 0.000015  loss: 2.7000 (2.9321)  time: 0.1292  data: 0.0586  max mem: 3355\n",
            "Epoch: [9]  [530/781]  eta: 0:00:33  lr: 0.000015  loss: 2.7107 (2.9311)  time: 0.1285  data: 0.0590  max mem: 3355\n",
            "Epoch: [9]  [540/781]  eta: 0:00:32  lr: 0.000015  loss: 2.6903 (2.9287)  time: 0.1346  data: 0.0664  max mem: 3355\n",
            "Epoch: [9]  [550/781]  eta: 0:00:31  lr: 0.000015  loss: 2.6696 (2.9290)  time: 0.1321  data: 0.0649  max mem: 3355\n",
            "Epoch: [9]  [560/781]  eta: 0:00:29  lr: 0.000015  loss: 2.8278 (2.9367)  time: 0.1307  data: 0.0651  max mem: 3355\n",
            "Epoch: [9]  [570/781]  eta: 0:00:28  lr: 0.000015  loss: 2.8675 (2.9415)  time: 0.1320  data: 0.0654  max mem: 3355\n",
            "Epoch: [9]  [580/781]  eta: 0:00:27  lr: 0.000015  loss: 2.8031 (2.9412)  time: 0.1347  data: 0.0674  max mem: 3355\n",
            "Epoch: [9]  [590/781]  eta: 0:00:25  lr: 0.000015  loss: 2.6947 (2.9412)  time: 0.1391  data: 0.0720  max mem: 3355\n",
            "Epoch: [9]  [600/781]  eta: 0:00:24  lr: 0.000015  loss: 2.6414 (2.9364)  time: 0.1397  data: 0.0713  max mem: 3355\n",
            "Epoch: [9]  [610/781]  eta: 0:00:23  lr: 0.000015  loss: 2.6259 (2.9322)  time: 0.1339  data: 0.0643  max mem: 3355\n",
            "Epoch: [9]  [620/781]  eta: 0:00:21  lr: 0.000015  loss: 2.6394 (2.9298)  time: 0.1264  data: 0.0572  max mem: 3355\n",
            "Epoch: [9]  [630/781]  eta: 0:00:20  lr: 0.000015  loss: 2.7637 (2.9292)  time: 0.1282  data: 0.0596  max mem: 3355\n",
            "Epoch: [9]  [640/781]  eta: 0:00:18  lr: 0.000015  loss: 2.6676 (2.9266)  time: 0.1281  data: 0.0600  max mem: 3355\n",
            "Epoch: [9]  [650/781]  eta: 0:00:17  lr: 0.000015  loss: 2.7921 (2.9274)  time: 0.1258  data: 0.0590  max mem: 3355\n",
            "Epoch: [9]  [660/781]  eta: 0:00:16  lr: 0.000015  loss: 2.8067 (2.9261)  time: 0.1279  data: 0.0610  max mem: 3355\n",
            "Epoch: [9]  [670/781]  eta: 0:00:14  lr: 0.000015  loss: 2.7361 (2.9246)  time: 0.1307  data: 0.0634  max mem: 3355\n",
            "Epoch: [9]  [680/781]  eta: 0:00:13  lr: 0.000015  loss: 2.6782 (2.9231)  time: 0.1318  data: 0.0656  max mem: 3355\n",
            "Epoch: [9]  [690/781]  eta: 0:00:12  lr: 0.000015  loss: 2.6682 (2.9206)  time: 0.1330  data: 0.0646  max mem: 3355\n",
            "Epoch: [9]  [700/781]  eta: 0:00:10  lr: 0.000015  loss: 2.7274 (2.9238)  time: 0.1357  data: 0.0648  max mem: 3355\n",
            "Epoch: [9]  [710/781]  eta: 0:00:09  lr: 0.000015  loss: 2.7750 (2.9243)  time: 0.1391  data: 0.0676  max mem: 3355\n",
            "Epoch: [9]  [720/781]  eta: 0:00:08  lr: 0.000015  loss: 2.7750 (2.9284)  time: 0.1334  data: 0.0635  max mem: 3355\n",
            "Epoch: [9]  [730/781]  eta: 0:00:06  lr: 0.000015  loss: 2.7578 (2.9263)  time: 0.1275  data: 0.0597  max mem: 3355\n",
            "Epoch: [9]  [740/781]  eta: 0:00:05  lr: 0.000015  loss: 2.6909 (2.9233)  time: 0.1269  data: 0.0592  max mem: 3355\n",
            "Epoch: [9]  [750/781]  eta: 0:00:04  lr: 0.000015  loss: 2.6995 (2.9220)  time: 0.1285  data: 0.0604  max mem: 3355\n",
            "Epoch: [9]  [760/781]  eta: 0:00:02  lr: 0.000015  loss: 2.7496 (2.9209)  time: 0.1313  data: 0.0633  max mem: 3355\n",
            "Epoch: [9]  [770/781]  eta: 0:00:01  lr: 0.000015  loss: 2.7308 (2.9194)  time: 0.1367  data: 0.0691  max mem: 3355\n",
            "Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000015  loss: 2.6052 (2.9179)  time: 0.1361  data: 0.0698  max mem: 3355\n",
            "Epoch: [9] Total time: 0:01:44 (0.1341 s / it)\n",
            "Averaged stats: lr: 0.000015  loss: 2.6052 (2.9179)\n",
            "Test:  [ 0/53]  eta: 0:00:47  loss: 0.8794 (0.8794)  acc1: 79.6875 (79.6875)  acc5: 93.2292 (93.2292)  time: 0.9020  data: 0.8725  max mem: 3355\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0903 (0.9833)  acc1: 75.0000 (76.2784)  acc5: 93.2292 (93.2765)  time: 0.1822  data: 0.1530  max mem: 3355\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0960 (1.0578)  acc1: 71.8750 (75.2976)  acc5: 90.6250 (91.7163)  time: 0.1209  data: 0.0916  max mem: 3355\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1536 (1.1029)  acc1: 70.3125 (74.7144)  acc5: 89.5833 (91.3811)  time: 0.1187  data: 0.0895  max mem: 3355\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2214 (1.1488)  acc1: 71.8750 (73.5772)  acc5: 89.5833 (90.9934)  time: 0.1168  data: 0.0876  max mem: 3355\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1781 (1.1380)  acc1: 71.8750 (73.8460)  acc5: 90.6250 (91.0948)  time: 0.1151  data: 0.0859  max mem: 3355\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2062 (1.1423)  acc1: 71.8750 (73.7200)  acc5: 91.1458 (91.1900)  time: 0.1005  data: 0.0721  max mem: 3355\n",
            "Test: Total time: 0:00:06 (0.1280 s / it)\n",
            "* Acc@1 73.720 Acc@5 91.190 loss 1.142\n",
            "Accuracy of the network on the 10000 test images: 73.7%\n",
            "Max accuracy: 73.72%\n",
            "Training time 0:19:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}