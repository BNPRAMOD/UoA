{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b46a967-6a70-4539-9f34-063b4c3039b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "a85d025e-c811-4a26-d27c-986f38d93222"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "bd3e72ba-1d2d-4195-9776-ec19757b6d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 22.75 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "7d137041-f72d-4ef1-a388-448acc5b18b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "2b095442-e302-4bab-d4b4-d60279a61ccc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "ef1077b8-11ad-4a4f-e222-48909297ba55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "# !pip -q install timm==0.6.13 submitit\n",
        "!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q uninstall -y timm\n",
        "#!pip -q install -U pip setuptools wheel\n",
        "#!pip -q install -U \"timm>=1.0.0\""
      ],
      "metadata": {
        "id": "q0Mim13um2k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\"\n",
        "#0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "7d81256c-606f-4df5-c3a6-03d7691acb97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "1f84d1e3-294e-4715-b363-44e55d089510"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runtime → Restart runtime"
      ],
      "metadata": {
        "id": "M0ZDDe3uvU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip -q install timm submitit"
      ],
      "metadata": {
        "id": "H3T5zLnQukuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "f31a1e4e-e154-43f1-aa6e-6d95adf67082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "206fa8c3-cdf3-4306-b9f9-c1e3e8b336ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.4.12\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "31ab67fd-2263-4d7c-b52a-992bd2de0d07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "17e7909e-92ff-4ffd-b228-959d70706b28",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "e0ff20da-9034-4587-f5be-63c385d8eb89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "842439af-c915-40de-fdd7-b3d54abccbe4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n03891332\n",
            "/content/tiny-imagenet-200/val/n04275548\n",
            "/content/tiny-imagenet-200/val/n07695742\n",
            "/content/tiny-imagenet-200/val/n01950731\n",
            "/content/tiny-imagenet-200/val/n01770393\n",
            "/content/tiny-imagenet-200/val/n03393912\n",
            "/content/tiny-imagenet-200/val/n03179701\n",
            "/content/tiny-imagenet-200/val/n09256479\n",
            "/content/tiny-imagenet-200/val/n04070727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "2137e285-b8eb-40cc-801a-2364d40d6ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Feb  9  2015 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Jan 25 22:46 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Jan 25 22:50 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "ea23f78c-3242-484c-894f-d2f187cb0211"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched augment.py for timm compatibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!sed -n '1,200p' models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cm_gVMz1-_",
        "outputId": "e64be62e-fe9f-41ad-f7df-9feb39fd83b2",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "# Copyright (c) 2015-present, Facebook, Inc.\n",
            "# All rights reserved.\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from functools import partial\n",
            "\n",
            "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
            "from timm.models.registry import register_model\n",
            "from timm.models.layers import trunc_normal_\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
            "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
            "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
            "    'deit_base_distilled_patch16_384',\n",
            "]\n",
            "\n",
            "\n",
            "class DistilledVisionTransformer(VisionTransformer):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
            "        num_patches = self.patch_embed.num_patches\n",
            "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
            "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
            "\n",
            "        trunc_normal_(self.dist_token, std=.02)\n",
            "        trunc_normal_(self.pos_embed, std=.02)\n",
            "        self.head_dist.apply(self._init_weights)\n",
            "\n",
            "    def forward_features(self, x):\n",
            "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
            "        # with slight modifications to add the dist_token\n",
            "        B = x.shape[0]\n",
            "        x = self.patch_embed(x)\n",
            "\n",
            "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
            "        dist_token = self.dist_token.expand(B, -1, -1)\n",
            "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
            "\n",
            "        x = x + self.pos_embed\n",
            "        x = self.pos_drop(x)\n",
            "\n",
            "        for blk in self.blocks:\n",
            "            x = blk(x)\n",
            "\n",
            "        x = self.norm(x)\n",
            "        return x[:, 0], x[:, 1]\n",
            "\n",
            "    def forward(self, x):\n",
            "        x, x_dist = self.forward_features(x)\n",
            "        x = self.head(x)\n",
            "        x_dist = self.head_dist(x_dist)\n",
            "        if self.training:\n",
            "            return x, x_dist\n",
            "        else:\n",
            "            # during inference, return the average of both classifier predictions\n",
            "            return (x + x_dist) / 2\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_384(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RizknqA6MBXb",
        "outputId": "4096332b-91fc-4530-c330-507bc23e07b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "ls: cannot access 'multiteacher_loss.py': No such file or directory\n",
            "✅ old file removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Normalize teacher weights along the last dimension.\n",
        "    lmb: (T,) or (B,T)\n",
        "    \"\"\"\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(\n",
        "    teacher_logits: Dict[str, torch.Tensor],\n",
        "    teacher_order: List[str],\n",
        "    lambdas: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    teacher_logits: dict{name -> (B,C)}\n",
        "    teacher_order: stable list of teacher names\n",
        "    lambdas: (B,T) or (T,)\n",
        "    returns: (B,C)\n",
        "    \"\"\"\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)  # (B,T,C)\n",
        "\n",
        "    lambdas = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lambdas.dim() == 1:\n",
        "        lambdas = lambdas.unsqueeze(0).expand(stacked.size(0), -1)  # (B,T)\n",
        "\n",
        "    return (stacked * lambdas.unsqueeze(-1)).sum(dim=1)  # (B,C)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    KL( teacher || student ) with temperature scaling.\n",
        "    \"\"\"\n",
        "    T = float(T)\n",
        "    teacher_logits = teacher_logits.to(student_logits.device)\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Hard distillation: use teacher argmax labels.\n",
        "    \"\"\"\n",
        "    y_t = teacher_logits.argmax(dim=-1)\n",
        "    return F.cross_entropy(student_logits, y_t)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Teachers\n",
        "# ---------------------------\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Loads timm teachers (ImageNet pretrained), freezes them, and returns logits dict.\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        models = {}\n",
        "        for name in teacher_names:\n",
        "            m = timm.create_model(name, pretrained=True, num_classes=1000)\n",
        "            m.eval()\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            models[name] = m.to(device)\n",
        "        self.models = nn.ModuleDict(models)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "class TeacherLogitAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps teacher logits (1000) -> student_num_classes (e.g., 200 for Tiny-ImageNet).\n",
        "    This adapter MUST be optimized in main.py by adding its params to optimizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_keys: List[str], student_num_classes: int):\n",
        "        super().__init__()\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            k: nn.Linear(1000, student_num_classes, bias=False) for k in teacher_keys\n",
        "        })\n",
        "\n",
        "    def forward(self, teacher_logits: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        return {k: self.adapters[k](v) for k, v in teacher_logits.items()}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# HDTSE (mixup-safe)\n",
        "# ---------------------------\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic teacher weights based on teacher confidence on the target.\n",
        "    Works for:\n",
        "      - hard labels: targets shape (B,)\n",
        "      - mixup/cutmix soft labels: targets shape (B,C)\n",
        "    \"\"\"\n",
        "    def __init__(self, temp: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.temp = float(temp)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        student_logits: torch.Tensor,  # unused, but kept for extensibility\n",
        "        teacher_logits: Dict[str, torch.Tensor],\n",
        "        teacher_order: List[str],\n",
        "        targets: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "        stacked = torch.stack(logits_list, dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Case A: hard labels (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Case B: soft labels from mixup/cutmix (B,C)\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)  # (B,C)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)  # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Multi-teacher distillation loss\n",
        "# ---------------------------\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Called from engine as:\n",
        "        loss = criterion(samples, outputs, targets)\n",
        "\n",
        "    - base_criterion: CE / SoftTargetCE etc (from timm)\n",
        "    - distillation_type: \"soft\" or \"hard\"\n",
        "    - alpha: weight for KD\n",
        "    - tau: temperature for KD\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion: nn.Module,\n",
        "        student_num_classes: int,\n",
        "        teacher_names: List[str],\n",
        "        distillation_type: str = \"soft\",\n",
        "        alpha: float = 0.5,\n",
        "        tau: float = 2.0,\n",
        "        device: Optional[torch.device] = None,\n",
        "        use_adapter: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = distillation_type\n",
        "        self.alpha = float(alpha)\n",
        "        self.tau = float(tau)\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, device=self.device)\n",
        "        self.teacher_order = self.teachers.teacher_order\n",
        "\n",
        "        self.adapter = TeacherLogitAdapter(self.teacher_order, student_num_classes).to(self.device) if use_adapter else None\n",
        "        self.hdtse = HDTSEConfidence(temp=1.0)\n",
        "\n",
        "        # Optional: store last batch lambdas for debugging/logging\n",
        "        self.last_lambdas: Optional[torch.Tensor] = None  # (B,T)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        # Base loss (handles hard or soft labels depending on what base_criterion is)\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        # Teacher forward (frozen)\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)  # dict of (B,1000)\n",
        "\n",
        "        # Adapt to student class space\n",
        "        if self.adapter is not None:\n",
        "            t_logits = self.adapter(t_logits)  # dict of (B,C_student)\n",
        "\n",
        "        # Dynamic weights (mixup-safe)\n",
        "        order = self.teacher_order\n",
        "        with torch.no_grad():\n",
        "            lambdas = self.hdtse(outputs.detach(), t_logits, order, targets)  # (B,T)\n",
        "        self.last_lambdas = lambdas\n",
        "\n",
        "        # Fuse teacher logits\n",
        "        fused = fuse_logits(t_logits, order, lambdas)  # (B,C_student)\n",
        "\n",
        "        # KD loss\n",
        "        if self.distillation_type == \"hard\":\n",
        "            distill_loss = kd_hard(outputs, fused)\n",
        "        else:\n",
        "            distill_loss = kd_soft(outputs, fused, self.tau)\n",
        "\n",
        "        # Final loss\n",
        "        return (1.0 - self.alpha) * base_loss + self.alpha * distill_loss\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4jzkzbMHD-",
        "outputId": "e6da3d99-c80d-4f47-b7df-b10cda4e717a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "File written: multiteacher_loss.py\n",
            "File size (bytes): 7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, shutil\n",
        "from datetime import datetime\n",
        "import py_compile\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "assert MAIN.exists(), f\"main.py not found at {MAIN}\"\n",
        "\n",
        "# -----------------------\n",
        "# Backup\n",
        "# -----------------------\n",
        "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "backup = MAIN.with_name(f\"main.py.bak_allpatches_{ts}\")\n",
        "shutil.copy2(MAIN, backup)\n",
        "print(\"✅ Backup created:\", backup)\n",
        "\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# -----------------------\n",
        "# Patch 1: Ensure import exists\n",
        "# -----------------------\n",
        "if \"from multiteacher_loss import MultiTeacherDistillationLoss\" not in txt:\n",
        "    if \"from losses import DistillationLoss\" in txt:\n",
        "        txt = txt.replace(\n",
        "            \"from losses import DistillationLoss\",\n",
        "            \"from losses import DistillationLoss\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\",\n",
        "            1\n",
        "        )\n",
        "        print(\"✅ Added MultiTeacherDistillationLoss import\")\n",
        "    else:\n",
        "        # fallback\n",
        "        txt = txt.replace(\n",
        "            \"import utils\",\n",
        "            \"import utils\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\",\n",
        "            1\n",
        "        )\n",
        "        print(\"✅ Added MultiTeacherDistillationLoss import (fallback)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch 2: Add --teacher-models CLI arg (after --teacher-path)\n",
        "# -----------------------\n",
        "if \"--teacher-models\" not in txt:\n",
        "    anchor = \"parser.add_argument('--teacher-path', type=str, default='')\"\n",
        "    addition = (\n",
        "        \"    parser.add_argument('--teacher-models', type=str, default='',\\n\"\n",
        "        \"                        help='Comma-separated timm model names for multi-teacher distillation')\\n\"\n",
        "    )\n",
        "    if anchor in txt:\n",
        "        txt = txt.replace(anchor, anchor + \"\\n\" + addition, 1)\n",
        "        print(\"✅ Added --teacher-models argument\")\n",
        "    else:\n",
        "        raise RuntimeError(\"❌ Could not find --teacher-path line to insert --teacher-models after.\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch 3: Relax teacher-path assertion\n",
        "# -----------------------\n",
        "# Replace strict assertion if present\n",
        "txt, n_assert = re.subn(\n",
        "    r\"assert\\s+args\\.teacher_path\\s*,\\s*['\\\"]need to specify teacher-path when using distillation['\\\"]\",\n",
        "    \"assert (args.teacher_path or args.teacher_models), 'need to specify teacher-path OR teacher-models when using distillation'\",\n",
        "    txt\n",
        ")\n",
        "if n_assert:\n",
        "    print(f\"✅ Relaxed teacher-path assertion ({n_assert} replacements)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch 4: Allow finetune + MULTI-teacher distillation\n",
        "# (only block finetune + SINGLE teacher distillation)\n",
        "# -----------------------\n",
        "# Original:\n",
        "# if args.distillation_type != 'none' and args.finetune and not args.eval:\n",
        "#     raise NotImplementedError(\"Finetuning with distillation not yet supported\")\n",
        "finetune_pattern = re.compile(\n",
        "    r\"if\\s+args\\.distillation_type\\s*!=\\s*'none'\\s+and\\s+args\\.finetune\\s+and\\s+not\\s+args\\.eval:\\s*\\n\\s*raise\\s+NotImplementedError\\([\\\"']Finetuning with distillation not yet supported[\\\"']\\)\",\n",
        "    re.MULTILINE\n",
        ")\n",
        "if finetune_pattern.search(txt):\n",
        "    txt = finetune_pattern.sub(\n",
        "        \"if args.distillation_type != 'none' and args.finetune and not args.eval and not args.teacher_models:\\n\"\n",
        "        \"        raise NotImplementedError(\\\"Finetuning with single-teacher distillation not yet supported\\\")\",\n",
        "        txt,\n",
        "        count=1\n",
        "    )\n",
        "    print(\"✅ Patched finetune+distill guard (multi-teacher allowed)\")\n",
        "\n",
        "# -----------------------\n",
        "# Patch 5: Insert adapter->optimizer hook after MultiTeacherDistillationLoss(...)\n",
        "# -----------------------\n",
        "if \"Added adapter parameters to optimizer\" not in txt:\n",
        "    # Insert immediately after the first occurrence of the criterion creation call\n",
        "    mt_pattern = re.compile(\n",
        "        r\"(criterion\\s*=\\s*MultiTeacherDistillationLoss\\([\\s\\S]*?\\)\\n)\",\n",
        "        re.MULTILINE\n",
        "    )\n",
        "    adapter_block = (\n",
        "        \"\\n\"\n",
        "        \"            # ✅ IMPORTANT: train the adapter (otherwise KD hurts accuracy)\\n\"\n",
        "        \"            if hasattr(criterion, \\\"adapter\\\") and criterion.adapter is not None:\\n\"\n",
        "        \"                optimizer.add_param_group({\\n\"\n",
        "        \"                    \\\"params\\\": criterion.adapter.parameters(),\\n\"\n",
        "        \"                    \\\"lr\\\": args.lr,\\n\"\n",
        "        \"                    \\\"weight_decay\\\": 0.0,\\n\"\n",
        "        \"                })\\n\"\n",
        "        \"                print(\\\"✅ Added adapter parameters to optimizer\\\")\\n\"\n",
        "    )\n",
        "    txt2, n_ins = mt_pattern.subn(r\"\\1\" + adapter_block, txt, count=1)\n",
        "    if n_ins == 1:\n",
        "        txt = txt2\n",
        "        print(\"✅ Inserted adapter->optimizer hook after MultiTeacherDistillationLoss(...)\")\n",
        "    else:\n",
        "        print(\"ℹ️ Could not insert adapter hook automatically (MultiTeacherDistillationLoss block not found uniquely).\")\n",
        "\n",
        "# -----------------------\n",
        "# Write + compile check\n",
        "# -----------------------\n",
        "MAIN.write_text(txt)\n",
        "print(\"✅ Wrote patched main.py\")\n",
        "\n",
        "py_compile.compile(str(MAIN), doraise=True)\n",
        "print(\"✅ main.py compiles successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdJZ4F-NoE-",
        "outputId": "e79afe3e-f72d-4ccd-e400-48941359b161"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Backup created: /content/deit/main.py.bak_allpatches_20260125_232047\n",
            "✅ Patched finetune+distill guard (multi-teacher allowed)\n",
            "✅ Wrote patched main.py\n",
            "✅ main.py compiles successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "d6367923-4e8c-4d5f-a674-bb0489bdd224"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"pretrained_cfg\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxOmdCb90Ymg",
        "outputId": "95b562c5-c09a-43b0-ece5-c13c0f139cf9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('pretrained_cfg', None)\n",
            "66:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "67:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "84:    kwargs.pop('pretrained_cfg', None)\n",
            "85:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "86:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "103:    kwargs.pop('pretrained_cfg', None)\n",
            "104:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "105:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "122:    kwargs.pop('pretrained_cfg', None)\n",
            "123:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "124:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "141:    kwargs.pop('pretrained_cfg', None)\n",
            "142:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "143:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "160:    kwargs.pop('pretrained_cfg', None)\n",
            "161:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "162:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "179:    kwargs.pop('pretrained_cfg', None)\n",
            "180:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "181:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "198:    kwargs.pop('pretrained_cfg', None)\n",
            "199:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "200:    kwargs.pop('pretrained_cfg_priority', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "5672f22f-f92f-4e4c-a072-096ceee7bb6c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "V409XjDO1cdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"cache_dir\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFoOP5c1dbu",
        "outputId": "774b5c59-08b4-4853-a532-182217cf8162"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('cache_dir', None)\n",
            "88:    kwargs.pop('cache_dir', None)\n",
            "111:    kwargs.pop('cache_dir', None)\n",
            "134:    kwargs.pop('cache_dir', None)\n",
            "157:    kwargs.pop('cache_dir', None)\n",
            "180:    kwargs.pop('cache_dir', None)\n",
            "203:    kwargs.pop('cache_dir', None)\n",
            "226:    kwargs.pop('cache_dir', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 10 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 2 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.2 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b0,tf_efficientnet_b1,mobilenetv3_large_100\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "5c4a944a-6e45-46de-dc97-e10dd6a82310"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=10, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.00025, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=2, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m6-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.1, remode='pixel', recount=1, resplit=False, mixup=0.2, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', teacher_models='tf_efficientnet_b0,tf_efficientnet_b1,mobilenetv3_large_100', distillation_type='soft', distillation_alpha=0.2, distillation_tau=2.0, cosub=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100% 21.9M/21.9M [00:00<00:00, 242MB/s]\n",
            "number of params: 5717416\n",
            "/usr/local/lib/python3.12/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_aa-827b6e33.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b1_aa-ea7a6ee0.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_large_100_ra-f55367f5.pth\n",
            "✅ Added adapter parameters to optimizer\n",
            "Start training for 10 epochs\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch: [0]  [  0/781]  eta: 3:09:15  lr: 0.000001  loss: 7.0836 (7.0836)  time: 14.5394  data: 0.9821  max mem: 4873\n",
            "Epoch: [0]  [ 10/781]  eta: 0:18:25  lr: 0.000001  loss: 7.0113 (7.0219)  time: 1.4345  data: 0.0896  max mem: 4873\n",
            "Epoch: [0]  [ 20/781]  eta: 0:10:16  lr: 0.000001  loss: 6.9845 (6.9971)  time: 0.1238  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 30/781]  eta: 0:07:22  lr: 0.000001  loss: 6.9426 (6.9433)  time: 0.1234  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 40/781]  eta: 0:05:52  lr: 0.000001  loss: 6.8393 (6.8905)  time: 0.1234  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 50/781]  eta: 0:04:56  lr: 0.000001  loss: 6.6588 (6.8380)  time: 0.1234  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 60/781]  eta: 0:04:19  lr: 0.000001  loss: 6.5975 (6.7961)  time: 0.1226  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 70/781]  eta: 0:03:51  lr: 0.000001  loss: 6.5038 (6.7435)  time: 0.1223  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 80/781]  eta: 0:03:31  lr: 0.000001  loss: 6.3458 (6.6854)  time: 0.1235  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [ 90/781]  eta: 0:03:14  lr: 0.000001  loss: 6.2202 (6.6296)  time: 0.1232  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [100/781]  eta: 0:03:02  lr: 0.000001  loss: 6.1251 (6.5758)  time: 0.1296  data: 0.0069  max mem: 4873\n",
            "Epoch: [0]  [110/781]  eta: 0:02:51  lr: 0.000001  loss: 6.0390 (6.5226)  time: 0.1380  data: 0.0134  max mem: 4873\n",
            "Epoch: [0]  [120/781]  eta: 0:02:42  lr: 0.000001  loss: 5.8990 (6.4698)  time: 0.1411  data: 0.0158  max mem: 4873\n",
            "Epoch: [0]  [130/781]  eta: 0:02:34  lr: 0.000001  loss: 5.8741 (6.4228)  time: 0.1385  data: 0.0144  max mem: 4873\n",
            "Epoch: [0]  [140/781]  eta: 0:02:27  lr: 0.000001  loss: 5.8205 (6.3785)  time: 0.1343  data: 0.0114  max mem: 4873\n",
            "Epoch: [0]  [150/781]  eta: 0:02:20  lr: 0.000001  loss: 5.7336 (6.3322)  time: 0.1293  data: 0.0063  max mem: 4873\n",
            "Epoch: [0]  [160/781]  eta: 0:02:15  lr: 0.000001  loss: 5.6295 (6.2891)  time: 0.1309  data: 0.0077  max mem: 4873\n",
            "Epoch: [0]  [170/781]  eta: 0:02:10  lr: 0.000001  loss: 5.6038 (6.2480)  time: 0.1389  data: 0.0159  max mem: 4873\n",
            "Epoch: [0]  [180/781]  eta: 0:02:05  lr: 0.000001  loss: 5.5610 (6.2092)  time: 0.1373  data: 0.0146  max mem: 4873\n",
            "Epoch: [0]  [190/781]  eta: 0:02:01  lr: 0.000001  loss: 5.5400 (6.1717)  time: 0.1331  data: 0.0089  max mem: 4873\n",
            "Epoch: [0]  [200/781]  eta: 0:01:57  lr: 0.000001  loss: 5.4686 (6.1364)  time: 0.1308  data: 0.0055  max mem: 4873\n",
            "Epoch: [0]  [210/781]  eta: 0:01:53  lr: 0.000001  loss: 5.4528 (6.1035)  time: 0.1292  data: 0.0049  max mem: 4873\n",
            "Epoch: [0]  [220/781]  eta: 0:01:49  lr: 0.000001  loss: 5.4215 (6.0716)  time: 0.1309  data: 0.0077  max mem: 4873\n",
            "Epoch: [0]  [230/781]  eta: 0:01:45  lr: 0.000001  loss: 5.3763 (6.0417)  time: 0.1297  data: 0.0070  max mem: 4873\n",
            "Epoch: [0]  [240/781]  eta: 0:01:42  lr: 0.000001  loss: 5.3598 (6.0127)  time: 0.1299  data: 0.0070  max mem: 4873\n",
            "Epoch: [0]  [250/781]  eta: 0:01:39  lr: 0.000001  loss: 5.3598 (5.9869)  time: 0.1358  data: 0.0130  max mem: 4873\n",
            "Epoch: [0]  [260/781]  eta: 0:01:36  lr: 0.000001  loss: 5.3316 (5.9609)  time: 0.1390  data: 0.0154  max mem: 4873\n",
            "Epoch: [0]  [270/781]  eta: 0:01:33  lr: 0.000001  loss: 5.2723 (5.9346)  time: 0.1377  data: 0.0145  max mem: 4873\n",
            "Epoch: [0]  [280/781]  eta: 0:01:31  lr: 0.000001  loss: 5.2427 (5.9102)  time: 0.1294  data: 0.0072  max mem: 4873\n",
            "Epoch: [0]  [290/781]  eta: 0:01:28  lr: 0.000001  loss: 5.2558 (5.8880)  time: 0.1265  data: 0.0025  max mem: 4873\n",
            "Epoch: [0]  [300/781]  eta: 0:01:25  lr: 0.000001  loss: 5.2544 (5.8657)  time: 0.1334  data: 0.0076  max mem: 4873\n",
            "Epoch: [0]  [310/781]  eta: 0:01:23  lr: 0.000001  loss: 5.1963 (5.8450)  time: 0.1347  data: 0.0109  max mem: 4873\n",
            "Epoch: [0]  [320/781]  eta: 0:01:20  lr: 0.000001  loss: 5.1857 (5.8241)  time: 0.1276  data: 0.0053  max mem: 4873\n",
            "Epoch: [0]  [330/781]  eta: 0:01:18  lr: 0.000001  loss: 5.1903 (5.8057)  time: 0.1288  data: 0.0054  max mem: 4873\n",
            "Epoch: [0]  [340/781]  eta: 0:01:16  lr: 0.000001  loss: 5.2013 (5.7874)  time: 0.1304  data: 0.0073  max mem: 4873\n",
            "Epoch: [0]  [350/781]  eta: 0:01:14  lr: 0.000001  loss: 5.1595 (5.7694)  time: 0.1326  data: 0.0102  max mem: 4873\n",
            "Epoch: [0]  [360/781]  eta: 0:01:11  lr: 0.000001  loss: 5.1666 (5.7532)  time: 0.1355  data: 0.0128  max mem: 4873\n",
            "Epoch: [0]  [370/781]  eta: 0:01:09  lr: 0.000001  loss: 5.1511 (5.7362)  time: 0.1338  data: 0.0106  max mem: 4873\n",
            "Epoch: [0]  [380/781]  eta: 0:01:07  lr: 0.000001  loss: 5.1244 (5.7203)  time: 0.1372  data: 0.0141  max mem: 4873\n",
            "Epoch: [0]  [390/781]  eta: 0:01:05  lr: 0.000001  loss: 5.1164 (5.7046)  time: 0.1318  data: 0.0083  max mem: 4873\n",
            "Epoch: [0]  [400/781]  eta: 0:01:03  lr: 0.000001  loss: 5.1005 (5.6892)  time: 0.1231  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [410/781]  eta: 0:01:01  lr: 0.000001  loss: 5.1005 (5.6752)  time: 0.1228  data: 0.0006  max mem: 4873\n",
            "Epoch: [0]  [420/781]  eta: 0:00:59  lr: 0.000001  loss: 5.1004 (5.6614)  time: 0.1285  data: 0.0057  max mem: 4873\n",
            "Epoch: [0]  [430/781]  eta: 0:00:57  lr: 0.000001  loss: 5.0789 (5.6481)  time: 0.1363  data: 0.0133  max mem: 4873\n",
            "Epoch: [0]  [440/781]  eta: 0:00:55  lr: 0.000001  loss: 5.0742 (5.6347)  time: 0.1341  data: 0.0111  max mem: 4873\n",
            "Epoch: [0]  [450/781]  eta: 0:00:53  lr: 0.000001  loss: 5.0564 (5.6219)  time: 0.1340  data: 0.0114  max mem: 4873\n",
            "Epoch: [0]  [460/781]  eta: 0:00:52  lr: 0.000001  loss: 5.0636 (5.6104)  time: 0.1380  data: 0.0154  max mem: 4873\n",
            "Epoch: [0]  [470/781]  eta: 0:00:50  lr: 0.000001  loss: 5.0636 (5.5986)  time: 0.1313  data: 0.0080  max mem: 4873\n",
            "Epoch: [0]  [480/781]  eta: 0:00:48  lr: 0.000001  loss: 5.0634 (5.5877)  time: 0.1309  data: 0.0071  max mem: 4873\n",
            "Epoch: [0]  [490/781]  eta: 0:00:46  lr: 0.000001  loss: 5.0495 (5.5768)  time: 0.1352  data: 0.0103  max mem: 4873\n",
            "Epoch: [0]  [500/781]  eta: 0:00:44  lr: 0.000001  loss: 5.0439 (5.5663)  time: 0.1288  data: 0.0043  max mem: 4873\n",
            "Epoch: [0]  [510/781]  eta: 0:00:43  lr: 0.000001  loss: 5.0326 (5.5556)  time: 0.1252  data: 0.0019  max mem: 4873\n",
            "Epoch: [0]  [520/781]  eta: 0:00:41  lr: 0.000001  loss: 5.0184 (5.5456)  time: 0.1258  data: 0.0019  max mem: 4873\n",
            "Epoch: [0]  [530/781]  eta: 0:00:39  lr: 0.000001  loss: 5.0307 (5.5362)  time: 0.1235  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [540/781]  eta: 0:00:37  lr: 0.000001  loss: 5.0286 (5.5264)  time: 0.1228  data: 0.0003  max mem: 4873\n",
            "Epoch: [0]  [550/781]  eta: 0:00:36  lr: 0.000001  loss: 5.0023 (5.5170)  time: 0.1260  data: 0.0028  max mem: 4873\n",
            "Epoch: [0]  [560/781]  eta: 0:00:34  lr: 0.000001  loss: 4.9938 (5.5075)  time: 0.1272  data: 0.0040  max mem: 4873\n",
            "Epoch: [0]  [570/781]  eta: 0:00:32  lr: 0.000001  loss: 4.9891 (5.4983)  time: 0.1253  data: 0.0020  max mem: 4873\n",
            "Epoch: [0]  [580/781]  eta: 0:00:31  lr: 0.000001  loss: 4.9844 (5.4896)  time: 0.1275  data: 0.0044  max mem: 4873\n",
            "Epoch: [0]  [590/781]  eta: 0:00:29  lr: 0.000001  loss: 4.9844 (5.4811)  time: 0.1306  data: 0.0071  max mem: 4873\n",
            "Epoch: [0]  [600/781]  eta: 0:00:27  lr: 0.000001  loss: 4.9758 (5.4730)  time: 0.1362  data: 0.0126  max mem: 4873\n",
            "Epoch: [0]  [610/781]  eta: 0:00:26  lr: 0.000001  loss: 4.9737 (5.4650)  time: 0.1341  data: 0.0114  max mem: 4873\n",
            "Epoch: [0]  [620/781]  eta: 0:00:24  lr: 0.000001  loss: 4.9687 (5.4566)  time: 0.1286  data: 0.0059  max mem: 4873\n",
            "Epoch: [0]  [630/781]  eta: 0:00:23  lr: 0.000001  loss: 4.9574 (5.4487)  time: 0.1276  data: 0.0053  max mem: 4873\n",
            "Epoch: [0]  [640/781]  eta: 0:00:21  lr: 0.000001  loss: 4.9563 (5.4409)  time: 0.1327  data: 0.0107  max mem: 4873\n",
            "Epoch: [0]  [650/781]  eta: 0:00:19  lr: 0.000001  loss: 4.9448 (5.4335)  time: 0.1351  data: 0.0125  max mem: 4873\n",
            "Epoch: [0]  [660/781]  eta: 0:00:18  lr: 0.000001  loss: 4.9619 (5.4264)  time: 0.1297  data: 0.0067  max mem: 4873\n",
            "Epoch: [0]  [670/781]  eta: 0:00:16  lr: 0.000001  loss: 4.9628 (5.4193)  time: 0.1270  data: 0.0033  max mem: 4873\n",
            "Epoch: [0]  [680/781]  eta: 0:00:15  lr: 0.000001  loss: 4.9586 (5.4125)  time: 0.1298  data: 0.0047  max mem: 4873\n",
            "Epoch: [0]  [690/781]  eta: 0:00:13  lr: 0.000001  loss: 4.9278 (5.4055)  time: 0.1315  data: 0.0075  max mem: 4873\n",
            "Epoch: [0]  [700/781]  eta: 0:00:12  lr: 0.000001  loss: 4.9251 (5.3985)  time: 0.1249  data: 0.0031  max mem: 4873\n",
            "Epoch: [0]  [710/781]  eta: 0:00:10  lr: 0.000001  loss: 4.9255 (5.3921)  time: 0.1239  data: 0.0023  max mem: 4873\n",
            "Epoch: [0]  [720/781]  eta: 0:00:09  lr: 0.000001  loss: 4.9247 (5.3857)  time: 0.1237  data: 0.0023  max mem: 4873\n",
            "Epoch: [0]  [730/781]  eta: 0:00:07  lr: 0.000001  loss: 4.9173 (5.3793)  time: 0.1230  data: 0.0016  max mem: 4873\n",
            "Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000001  loss: 4.9245 (5.3735)  time: 0.1270  data: 0.0054  max mem: 4873\n",
            "Epoch: [0]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 4.9194 (5.3672)  time: 0.1295  data: 0.0080  max mem: 4873\n",
            "Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000001  loss: 4.9194 (5.3615)  time: 0.1326  data: 0.0106  max mem: 4873\n",
            "Epoch: [0]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 4.9256 (5.3557)  time: 0.1312  data: 0.0087  max mem: 4873\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 4.9116 (5.3499)  time: 0.1290  data: 0.0061  max mem: 4873\n",
            "Epoch: [0] Total time: 0:01:56 (0.1486 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.9116 (5.3499)\n",
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Test:  [ 0/53]  eta: 0:00:50  loss: 6.2132 (6.2132)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.9618  data: 0.8072  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 5.6325 (5.6860)  acc1: 0.0000 (0.1894)  acc5: 2.6042 (2.9356)  time: 0.1699  data: 0.1280  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 5.6909 (5.7339)  acc1: 0.0000 (0.5456)  acc5: 2.6042 (3.4970)  time: 0.1175  data: 0.0869  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.7901 (5.7972)  acc1: 0.0000 (0.4704)  acc5: 2.0833 (3.2258)  time: 0.1258  data: 0.0951  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.7674 (5.8046)  acc1: 0.0000 (0.8892)  acc5: 1.0417 (3.9888)  time: 0.1274  data: 0.0967  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 6.0209 (5.8778)  acc1: 0.0000 (0.7149)  acc5: 0.0000 (3.2169)  time: 0.1272  data: 0.0965  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 6.1029 (5.8952)  acc1: 0.0000 (0.7000)  acc5: 0.0000 (3.1500)  time: 0.1069  data: 0.0767  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1326 s / it)\n",
            "* Acc@1 0.700 Acc@5 3.150 loss 5.895\n",
            "Accuracy of the network on the 10000 test images: 0.7%\n",
            "Max accuracy: 0.70%\n",
            "Epoch: [1]  [  0/781]  eta: 0:11:46  lr: 0.000001  loss: 4.9984 (4.9984)  time: 0.9040  data: 0.7587  max mem: 4873\n",
            "Epoch: [1]  [ 10/781]  eta: 0:02:31  lr: 0.000001  loss: 4.8963 (4.8879)  time: 0.1962  data: 0.0719  max mem: 4873\n",
            "Epoch: [1]  [ 20/781]  eta: 0:02:07  lr: 0.000001  loss: 4.8963 (4.8920)  time: 0.1304  data: 0.0057  max mem: 4873\n",
            "Epoch: [1]  [ 30/781]  eta: 0:02:00  lr: 0.000001  loss: 4.8987 (4.8925)  time: 0.1400  data: 0.0042  max mem: 4873\n",
            "Epoch: [1]  [ 40/781]  eta: 0:01:51  lr: 0.000001  loss: 4.8972 (4.8923)  time: 0.1335  data: 0.0003  max mem: 4873\n",
            "Epoch: [1]  [ 50/781]  eta: 0:01:46  lr: 0.000001  loss: 4.8879 (4.8891)  time: 0.1226  data: 0.0003  max mem: 4873\n",
            "Epoch: [1]  [ 60/781]  eta: 0:01:43  lr: 0.000001  loss: 4.8723 (4.8867)  time: 0.1267  data: 0.0042  max mem: 4873\n",
            "Epoch: [1]  [ 70/781]  eta: 0:01:40  lr: 0.000001  loss: 4.8693 (4.8823)  time: 0.1310  data: 0.0083  max mem: 4873\n",
            "Epoch: [1]  [ 80/781]  eta: 0:01:38  lr: 0.000001  loss: 4.8693 (4.8812)  time: 0.1322  data: 0.0100  max mem: 4873\n",
            "Epoch: [1]  [ 90/781]  eta: 0:01:36  lr: 0.000001  loss: 4.8608 (4.8781)  time: 0.1316  data: 0.0100  max mem: 4873\n",
            "Epoch: [1]  [100/781]  eta: 0:01:34  lr: 0.000001  loss: 4.8649 (4.8775)  time: 0.1340  data: 0.0117  max mem: 4873\n",
            "Epoch: [1]  [110/781]  eta: 0:01:32  lr: 0.000001  loss: 4.8659 (4.8764)  time: 0.1346  data: 0.0116  max mem: 4873\n",
            "Epoch: [1]  [120/781]  eta: 0:01:31  lr: 0.000001  loss: 4.8494 (4.8729)  time: 0.1385  data: 0.0145  max mem: 4873\n",
            "Epoch: [1]  [130/781]  eta: 0:01:30  lr: 0.000001  loss: 4.8422 (4.8713)  time: 0.1393  data: 0.0154  max mem: 4873\n",
            "Epoch: [1]  [140/781]  eta: 0:01:28  lr: 0.000001  loss: 4.8422 (4.8703)  time: 0.1291  data: 0.0069  max mem: 4873\n",
            "Epoch: [1]  [150/781]  eta: 0:01:26  lr: 0.000001  loss: 4.8276 (4.8685)  time: 0.1319  data: 0.0092  max mem: 4873\n",
            "Epoch: [1]  [160/781]  eta: 0:01:25  lr: 0.000001  loss: 4.8276 (4.8666)  time: 0.1352  data: 0.0114  max mem: 4873\n",
            "Epoch: [1]  [170/781]  eta: 0:01:23  lr: 0.000001  loss: 4.8292 (4.8653)  time: 0.1335  data: 0.0086  max mem: 4873\n",
            "Epoch: [1]  [180/781]  eta: 0:01:22  lr: 0.000001  loss: 4.8507 (4.8644)  time: 0.1312  data: 0.0072  max mem: 4873\n",
            "Epoch: [1]  [190/781]  eta: 0:01:20  lr: 0.000001  loss: 4.8436 (4.8630)  time: 0.1339  data: 0.0122  max mem: 4873\n",
            "Epoch: [1]  [200/781]  eta: 0:01:19  lr: 0.000001  loss: 4.8303 (4.8616)  time: 0.1394  data: 0.0168  max mem: 4873\n",
            "Epoch: [1]  [210/781]  eta: 0:01:17  lr: 0.000001  loss: 4.8254 (4.8602)  time: 0.1343  data: 0.0099  max mem: 4873\n",
            "Epoch: [1]  [220/781]  eta: 0:01:16  lr: 0.000001  loss: 4.8140 (4.8573)  time: 0.1329  data: 0.0080  max mem: 4873\n",
            "Epoch: [1]  [230/781]  eta: 0:01:15  lr: 0.000001  loss: 4.8013 (4.8553)  time: 0.1367  data: 0.0129  max mem: 4873\n",
            "Epoch: [1]  [240/781]  eta: 0:01:13  lr: 0.000001  loss: 4.8074 (4.8537)  time: 0.1364  data: 0.0142  max mem: 4873\n",
            "Epoch: [1]  [250/781]  eta: 0:01:12  lr: 0.000001  loss: 4.8071 (4.8523)  time: 0.1397  data: 0.0169  max mem: 4873\n",
            "Epoch: [1]  [260/781]  eta: 0:01:11  lr: 0.000001  loss: 4.8161 (4.8509)  time: 0.1359  data: 0.0122  max mem: 4873\n",
            "Epoch: [1]  [270/781]  eta: 0:01:09  lr: 0.000001  loss: 4.8174 (4.8504)  time: 0.1291  data: 0.0063  max mem: 4873\n",
            "Epoch: [1]  [280/781]  eta: 0:01:08  lr: 0.000001  loss: 4.8102 (4.8487)  time: 0.1317  data: 0.0095  max mem: 4873\n",
            "Epoch: [1]  [290/781]  eta: 0:01:06  lr: 0.000001  loss: 4.8102 (4.8477)  time: 0.1318  data: 0.0098  max mem: 4873\n",
            "Epoch: [1]  [300/781]  eta: 0:01:05  lr: 0.000001  loss: 4.8082 (4.8462)  time: 0.1337  data: 0.0103  max mem: 4873\n",
            "Epoch: [1]  [310/781]  eta: 0:01:04  lr: 0.000001  loss: 4.7911 (4.8443)  time: 0.1393  data: 0.0137  max mem: 4873\n",
            "Epoch: [1]  [320/781]  eta: 0:01:02  lr: 0.000001  loss: 4.7937 (4.8428)  time: 0.1343  data: 0.0084  max mem: 4873\n",
            "Epoch: [1]  [330/781]  eta: 0:01:01  lr: 0.000001  loss: 4.7912 (4.8411)  time: 0.1290  data: 0.0056  max mem: 4873\n",
            "Epoch: [1]  [340/781]  eta: 0:00:59  lr: 0.000001  loss: 4.7765 (4.8393)  time: 0.1329  data: 0.0112  max mem: 4873\n",
            "Epoch: [1]  [350/781]  eta: 0:00:58  lr: 0.000001  loss: 4.7847 (4.8382)  time: 0.1328  data: 0.0112  max mem: 4873\n",
            "Epoch: [1]  [360/781]  eta: 0:00:56  lr: 0.000001  loss: 4.7857 (4.8367)  time: 0.1302  data: 0.0087  max mem: 4873\n",
            "Epoch: [1]  [370/781]  eta: 0:00:55  lr: 0.000001  loss: 4.7885 (4.8356)  time: 0.1324  data: 0.0109  max mem: 4873\n",
            "Epoch: [1]  [380/781]  eta: 0:00:54  lr: 0.000001  loss: 4.7932 (4.8344)  time: 0.1361  data: 0.0149  max mem: 4873\n",
            "Epoch: [1]  [390/781]  eta: 0:00:52  lr: 0.000001  loss: 4.7768 (4.8330)  time: 0.1348  data: 0.0124  max mem: 4873\n",
            "Epoch: [1]  [400/781]  eta: 0:00:51  lr: 0.000001  loss: 4.7672 (4.8317)  time: 0.1301  data: 0.0054  max mem: 4873\n",
            "Epoch: [1]  [410/781]  eta: 0:00:50  lr: 0.000001  loss: 4.7657 (4.8303)  time: 0.1387  data: 0.0142  max mem: 4873\n",
            "Epoch: [1]  [420/781]  eta: 0:00:48  lr: 0.000001  loss: 4.7765 (4.8293)  time: 0.1393  data: 0.0161  max mem: 4873\n",
            "Epoch: [1]  [430/781]  eta: 0:00:47  lr: 0.000001  loss: 4.7749 (4.8279)  time: 0.1290  data: 0.0061  max mem: 4873\n",
            "Epoch: [1]  [440/781]  eta: 0:00:46  lr: 0.000001  loss: 4.7680 (4.8266)  time: 0.1325  data: 0.0096  max mem: 4873\n",
            "Epoch: [1]  [450/781]  eta: 0:00:44  lr: 0.000001  loss: 4.7591 (4.8251)  time: 0.1348  data: 0.0118  max mem: 4873\n",
            "Epoch: [1]  [460/781]  eta: 0:00:43  lr: 0.000001  loss: 4.7764 (4.8243)  time: 0.1374  data: 0.0144  max mem: 4873\n",
            "Epoch: [1]  [470/781]  eta: 0:00:42  lr: 0.000001  loss: 4.7975 (4.8235)  time: 0.1446  data: 0.0218  max mem: 4873\n",
            "Epoch: [1]  [480/781]  eta: 0:00:40  lr: 0.000001  loss: 4.7778 (4.8225)  time: 0.1382  data: 0.0151  max mem: 4873\n",
            "Epoch: [1]  [490/781]  eta: 0:00:39  lr: 0.000001  loss: 4.7587 (4.8212)  time: 0.1391  data: 0.0157  max mem: 4873\n",
            "Epoch: [1]  [500/781]  eta: 0:00:38  lr: 0.000001  loss: 4.7447 (4.8198)  time: 0.1422  data: 0.0195  max mem: 4873\n",
            "Epoch: [1]  [510/781]  eta: 0:00:36  lr: 0.000001  loss: 4.7447 (4.8185)  time: 0.1337  data: 0.0113  max mem: 4873\n",
            "Epoch: [1]  [520/781]  eta: 0:00:35  lr: 0.000001  loss: 4.7427 (4.8172)  time: 0.1287  data: 0.0059  max mem: 4873\n",
            "Epoch: [1]  [530/781]  eta: 0:00:33  lr: 0.000001  loss: 4.7598 (4.8164)  time: 0.1279  data: 0.0057  max mem: 4873\n",
            "Epoch: [1]  [540/781]  eta: 0:00:32  lr: 0.000001  loss: 4.7575 (4.8152)  time: 0.1319  data: 0.0085  max mem: 4873\n",
            "Epoch: [1]  [550/781]  eta: 0:00:31  lr: 0.000001  loss: 4.7575 (4.8146)  time: 0.1292  data: 0.0048  max mem: 4873\n",
            "Epoch: [1]  [560/781]  eta: 0:00:29  lr: 0.000001  loss: 4.7529 (4.8131)  time: 0.1264  data: 0.0030  max mem: 4873\n",
            "Epoch: [1]  [570/781]  eta: 0:00:28  lr: 0.000001  loss: 4.7449 (4.8121)  time: 0.1277  data: 0.0038  max mem: 4873\n",
            "Epoch: [1]  [580/781]  eta: 0:00:27  lr: 0.000001  loss: 4.7557 (4.8112)  time: 0.1422  data: 0.0180  max mem: 4873\n",
            "Epoch: [1]  [590/781]  eta: 0:00:25  lr: 0.000001  loss: 4.7563 (4.8102)  time: 0.1448  data: 0.0215  max mem: 4873\n",
            "Epoch: [1]  [600/781]  eta: 0:00:24  lr: 0.000001  loss: 4.7515 (4.8090)  time: 0.1319  data: 0.0090  max mem: 4873\n",
            "Epoch: [1]  [610/781]  eta: 0:00:23  lr: 0.000001  loss: 4.7515 (4.8081)  time: 0.1284  data: 0.0044  max mem: 4873\n",
            "Epoch: [1]  [620/781]  eta: 0:00:21  lr: 0.000001  loss: 4.7550 (4.8072)  time: 0.1316  data: 0.0072  max mem: 4873\n",
            "Epoch: [1]  [630/781]  eta: 0:00:20  lr: 0.000001  loss: 4.7542 (4.8063)  time: 0.1385  data: 0.0152  max mem: 4873\n",
            "Epoch: [1]  [640/781]  eta: 0:00:19  lr: 0.000001  loss: 4.7303 (4.8051)  time: 0.1335  data: 0.0113  max mem: 4873\n",
            "Epoch: [1]  [650/781]  eta: 0:00:17  lr: 0.000001  loss: 4.7374 (4.8041)  time: 0.1307  data: 0.0085  max mem: 4873\n",
            "Epoch: [1]  [660/781]  eta: 0:00:16  lr: 0.000001  loss: 4.7437 (4.8033)  time: 0.1360  data: 0.0132  max mem: 4873\n",
            "Epoch: [1]  [670/781]  eta: 0:00:14  lr: 0.000001  loss: 4.7437 (4.8022)  time: 0.1370  data: 0.0134  max mem: 4873\n",
            "Epoch: [1]  [680/781]  eta: 0:00:13  lr: 0.000001  loss: 4.7259 (4.8011)  time: 0.1368  data: 0.0131  max mem: 4873\n",
            "Epoch: [1]  [690/781]  eta: 0:00:12  lr: 0.000001  loss: 4.7259 (4.8002)  time: 0.1326  data: 0.0097  max mem: 4873\n",
            "Epoch: [1]  [700/781]  eta: 0:00:10  lr: 0.000001  loss: 4.7255 (4.7991)  time: 0.1331  data: 0.0096  max mem: 4873\n",
            "Epoch: [1]  [710/781]  eta: 0:00:09  lr: 0.000001  loss: 4.7245 (4.7983)  time: 0.1364  data: 0.0133  max mem: 4873\n",
            "Epoch: [1]  [720/781]  eta: 0:00:08  lr: 0.000001  loss: 4.7349 (4.7975)  time: 0.1326  data: 0.0097  max mem: 4873\n",
            "Epoch: [1]  [730/781]  eta: 0:00:06  lr: 0.000001  loss: 4.7284 (4.7966)  time: 0.1338  data: 0.0105  max mem: 4873\n",
            "Epoch: [1]  [740/781]  eta: 0:00:05  lr: 0.000001  loss: 4.7229 (4.7957)  time: 0.1350  data: 0.0123  max mem: 4873\n",
            "Epoch: [1]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 4.7282 (4.7948)  time: 0.1328  data: 0.0099  max mem: 4873\n",
            "Epoch: [1]  [760/781]  eta: 0:00:02  lr: 0.000001  loss: 4.7270 (4.7940)  time: 0.1371  data: 0.0134  max mem: 4873\n",
            "Epoch: [1]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 4.7213 (4.7929)  time: 0.1437  data: 0.0200  max mem: 4873\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 4.7213 (4.7921)  time: 0.1397  data: 0.0163  max mem: 4873\n",
            "Epoch: [1] Total time: 0:01:45 (0.1352 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 4.7213 (4.7921)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 5.7768 (5.7768)  acc1: 0.0000 (0.0000)  acc5: 1.0417 (1.0417)  time: 0.8189  data: 0.7880  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 5.4261 (5.5071)  acc1: 0.0000 (0.5682)  acc5: 2.6042 (3.5038)  time: 0.1733  data: 0.1426  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 5.4497 (5.4963)  acc1: 0.0000 (0.8185)  acc5: 2.6042 (3.6706)  time: 0.1259  data: 0.0952  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.5076 (5.5646)  acc1: 0.0000 (0.8065)  acc5: 2.0833 (3.6290)  time: 0.1261  data: 0.0954  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.5076 (5.5664)  acc1: 0.0000 (1.0671)  acc5: 3.6458 (4.3445)  time: 0.1287  data: 0.0980  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 5.5240 (5.5812)  acc1: 0.0000 (0.8885)  acc5: 2.0833 (3.8705)  time: 0.1309  data: 0.1002  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 5.6093 (5.5937)  acc1: 0.0000 (0.8700)  acc5: 2.0833 (3.9300)  time: 0.1104  data: 0.0808  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1346 s / it)\n",
            "* Acc@1 0.870 Acc@5 3.930 loss 5.594\n",
            "Accuracy of the network on the 10000 test images: 0.9%\n",
            "Max accuracy: 0.87%\n",
            "Epoch: [2]  [  0/781]  eta: 0:11:34  lr: 0.000032  loss: 4.6445 (4.6445)  time: 0.8886  data: 0.7558  max mem: 4873\n",
            "Epoch: [2]  [ 10/781]  eta: 0:02:33  lr: 0.000032  loss: 4.6738 (4.6796)  time: 0.1997  data: 0.0722  max mem: 4873\n",
            "Epoch: [2]  [ 20/781]  eta: 0:02:08  lr: 0.000032  loss: 4.6789 (4.6829)  time: 0.1327  data: 0.0073  max mem: 4873\n",
            "Epoch: [2]  [ 30/781]  eta: 0:01:56  lr: 0.000032  loss: 4.6602 (4.6741)  time: 0.1310  data: 0.0072  max mem: 4873\n",
            "Epoch: [2]  [ 40/781]  eta: 0:01:49  lr: 0.000032  loss: 4.6286 (4.6595)  time: 0.1252  data: 0.0020  max mem: 4873\n",
            "Epoch: [2]  [ 50/781]  eta: 0:01:45  lr: 0.000032  loss: 4.5981 (4.6479)  time: 0.1263  data: 0.0041  max mem: 4873\n",
            "Epoch: [2]  [ 60/781]  eta: 0:01:41  lr: 0.000032  loss: 4.5927 (4.6360)  time: 0.1290  data: 0.0061  max mem: 4873\n",
            "Epoch: [2]  [ 70/781]  eta: 0:01:38  lr: 0.000032  loss: 4.5690 (4.6247)  time: 0.1263  data: 0.0029  max mem: 4873\n",
            "Epoch: [2]  [ 80/781]  eta: 0:01:36  lr: 0.000032  loss: 4.5201 (4.6085)  time: 0.1274  data: 0.0048  max mem: 4873\n",
            "Epoch: [2]  [ 90/781]  eta: 0:01:34  lr: 0.000032  loss: 4.4887 (4.5956)  time: 0.1268  data: 0.0042  max mem: 4873\n",
            "Epoch: [2]  [100/781]  eta: 0:01:32  lr: 0.000032  loss: 4.4674 (4.5815)  time: 0.1284  data: 0.0058  max mem: 4873\n",
            "Epoch: [2]  [110/781]  eta: 0:01:30  lr: 0.000032  loss: 4.4324 (4.5668)  time: 0.1310  data: 0.0077  max mem: 4873\n",
            "Epoch: [2]  [120/781]  eta: 0:01:29  lr: 0.000032  loss: 4.4082 (4.5529)  time: 0.1344  data: 0.0112  max mem: 4873\n",
            "Epoch: [2]  [130/781]  eta: 0:01:28  lr: 0.000032  loss: 4.3897 (4.5404)  time: 0.1364  data: 0.0134  max mem: 4873\n",
            "Epoch: [2]  [140/781]  eta: 0:01:26  lr: 0.000032  loss: 4.3426 (4.5273)  time: 0.1285  data: 0.0052  max mem: 4873\n",
            "Epoch: [2]  [150/781]  eta: 0:01:24  lr: 0.000032  loss: 4.3248 (4.5141)  time: 0.1264  data: 0.0042  max mem: 4873\n",
            "Epoch: [2]  [160/781]  eta: 0:01:22  lr: 0.000032  loss: 4.2960 (4.5031)  time: 0.1254  data: 0.0039  max mem: 4873\n",
            "Epoch: [2]  [170/781]  eta: 0:01:21  lr: 0.000032  loss: 4.2619 (4.4882)  time: 0.1250  data: 0.0033  max mem: 4873\n",
            "Epoch: [2]  [180/781]  eta: 0:01:19  lr: 0.000032  loss: 4.1956 (4.4718)  time: 0.1277  data: 0.0061  max mem: 4873\n",
            "Epoch: [2]  [190/781]  eta: 0:01:18  lr: 0.000032  loss: 4.1697 (4.4544)  time: 0.1286  data: 0.0053  max mem: 4873\n",
            "Epoch: [2]  [200/781]  eta: 0:01:17  lr: 0.000032  loss: 4.1389 (4.4384)  time: 0.1364  data: 0.0112  max mem: 4873\n",
            "Epoch: [2]  [210/781]  eta: 0:01:16  lr: 0.000032  loss: 4.1182 (4.4228)  time: 0.1368  data: 0.0120  max mem: 4873\n",
            "Epoch: [2]  [220/781]  eta: 0:01:14  lr: 0.000032  loss: 4.0789 (4.4067)  time: 0.1271  data: 0.0037  max mem: 4873\n",
            "Epoch: [2]  [230/781]  eta: 0:01:12  lr: 0.000032  loss: 4.0297 (4.3908)  time: 0.1238  data: 0.0011  max mem: 4873\n",
            "Epoch: [2]  [240/781]  eta: 0:01:11  lr: 0.000032  loss: 4.0280 (4.3766)  time: 0.1324  data: 0.0097  max mem: 4873\n",
            "Epoch: [2]  [250/781]  eta: 0:01:10  lr: 0.000032  loss: 3.9806 (4.3622)  time: 0.1372  data: 0.0144  max mem: 4873\n",
            "Epoch: [2]  [260/781]  eta: 0:01:09  lr: 0.000032  loss: 3.9378 (4.3456)  time: 0.1366  data: 0.0136  max mem: 4873\n",
            "Epoch: [2]  [270/781]  eta: 0:01:07  lr: 0.000032  loss: 3.9037 (4.3297)  time: 0.1352  data: 0.0118  max mem: 4873\n",
            "Epoch: [2]  [280/781]  eta: 0:01:06  lr: 0.000032  loss: 3.8820 (4.3168)  time: 0.1335  data: 0.0102  max mem: 4873\n",
            "Epoch: [2]  [290/781]  eta: 0:01:05  lr: 0.000032  loss: 3.8673 (4.3035)  time: 0.1334  data: 0.0105  max mem: 4873\n",
            "Epoch: [2]  [300/781]  eta: 0:01:03  lr: 0.000032  loss: 3.8465 (4.2866)  time: 0.1315  data: 0.0089  max mem: 4873\n",
            "Epoch: [2]  [310/781]  eta: 0:01:02  lr: 0.000032  loss: 3.8193 (4.2762)  time: 0.1306  data: 0.0086  max mem: 4873\n",
            "Epoch: [2]  [320/781]  eta: 0:01:01  lr: 0.000032  loss: 3.8333 (4.2621)  time: 0.1288  data: 0.0072  max mem: 4873\n",
            "Epoch: [2]  [330/781]  eta: 0:00:59  lr: 0.000032  loss: 3.7885 (4.2502)  time: 0.1269  data: 0.0047  max mem: 4873\n",
            "Epoch: [2]  [340/781]  eta: 0:00:58  lr: 0.000032  loss: 3.7233 (4.2347)  time: 0.1261  data: 0.0030  max mem: 4873\n",
            "Epoch: [2]  [350/781]  eta: 0:00:56  lr: 0.000032  loss: 3.7233 (4.2221)  time: 0.1264  data: 0.0034  max mem: 4873\n",
            "Epoch: [2]  [360/781]  eta: 0:00:55  lr: 0.000032  loss: 3.7282 (4.2089)  time: 0.1252  data: 0.0022  max mem: 4873\n",
            "Epoch: [2]  [370/781]  eta: 0:00:54  lr: 0.000032  loss: 3.6917 (4.1943)  time: 0.1244  data: 0.0014  max mem: 4873\n",
            "Epoch: [2]  [380/781]  eta: 0:00:52  lr: 0.000032  loss: 3.6742 (4.1821)  time: 0.1249  data: 0.0020  max mem: 4873\n",
            "Epoch: [2]  [390/781]  eta: 0:00:51  lr: 0.000032  loss: 3.6562 (4.1715)  time: 0.1299  data: 0.0060  max mem: 4873\n",
            "Epoch: [2]  [400/781]  eta: 0:00:50  lr: 0.000032  loss: 3.5566 (4.1617)  time: 0.1292  data: 0.0045  max mem: 4873\n",
            "Epoch: [2]  [410/781]  eta: 0:00:48  lr: 0.000032  loss: 3.5474 (4.1471)  time: 0.1257  data: 0.0018  max mem: 4873\n",
            "Epoch: [2]  [420/781]  eta: 0:00:47  lr: 0.000032  loss: 3.5778 (4.1386)  time: 0.1255  data: 0.0016  max mem: 4873\n",
            "Epoch: [2]  [430/781]  eta: 0:00:45  lr: 0.000032  loss: 3.6589 (4.1281)  time: 0.1248  data: 0.0017  max mem: 4873\n",
            "Epoch: [2]  [440/781]  eta: 0:00:44  lr: 0.000032  loss: 3.5228 (4.1160)  time: 0.1262  data: 0.0036  max mem: 4873\n",
            "Epoch: [2]  [450/781]  eta: 0:00:43  lr: 0.000032  loss: 3.5059 (4.1055)  time: 0.1296  data: 0.0058  max mem: 4873\n",
            "Epoch: [2]  [460/781]  eta: 0:00:42  lr: 0.000032  loss: 3.5059 (4.0961)  time: 0.1323  data: 0.0088  max mem: 4873\n",
            "Epoch: [2]  [470/781]  eta: 0:00:40  lr: 0.000032  loss: 3.5320 (4.0862)  time: 0.1332  data: 0.0107  max mem: 4873\n",
            "Epoch: [2]  [480/781]  eta: 0:00:39  lr: 0.000032  loss: 3.5073 (4.0751)  time: 0.1314  data: 0.0088  max mem: 4873\n",
            "Epoch: [2]  [490/781]  eta: 0:00:38  lr: 0.000032  loss: 3.4411 (4.0639)  time: 0.1273  data: 0.0037  max mem: 4873\n",
            "Epoch: [2]  [500/781]  eta: 0:00:36  lr: 0.000032  loss: 3.5055 (4.0536)  time: 0.1257  data: 0.0023  max mem: 4873\n",
            "Epoch: [2]  [510/781]  eta: 0:00:35  lr: 0.000032  loss: 3.4476 (4.0412)  time: 0.1264  data: 0.0045  max mem: 4873\n",
            "Epoch: [2]  [520/781]  eta: 0:00:34  lr: 0.000032  loss: 3.3781 (4.0284)  time: 0.1285  data: 0.0068  max mem: 4873\n",
            "Epoch: [2]  [530/781]  eta: 0:00:32  lr: 0.000032  loss: 3.3781 (4.0181)  time: 0.1287  data: 0.0065  max mem: 4873\n",
            "Epoch: [2]  [540/781]  eta: 0:00:31  lr: 0.000032  loss: 3.4357 (4.0073)  time: 0.1277  data: 0.0050  max mem: 4873\n",
            "Epoch: [2]  [550/781]  eta: 0:00:30  lr: 0.000032  loss: 3.3276 (3.9976)  time: 0.1269  data: 0.0029  max mem: 4873\n",
            "Epoch: [2]  [560/781]  eta: 0:00:28  lr: 0.000032  loss: 3.3182 (3.9874)  time: 0.1270  data: 0.0034  max mem: 4873\n",
            "Epoch: [2]  [570/781]  eta: 0:00:27  lr: 0.000032  loss: 3.3231 (3.9759)  time: 0.1265  data: 0.0043  max mem: 4873\n",
            "Epoch: [2]  [580/781]  eta: 0:00:26  lr: 0.000032  loss: 3.2905 (3.9643)  time: 0.1293  data: 0.0057  max mem: 4873\n",
            "Epoch: [2]  [590/781]  eta: 0:00:24  lr: 0.000032  loss: 3.2923 (3.9558)  time: 0.1363  data: 0.0100  max mem: 4873\n",
            "Epoch: [2]  [600/781]  eta: 0:00:23  lr: 0.000032  loss: 3.3341 (3.9455)  time: 0.1370  data: 0.0126  max mem: 4873\n",
            "Epoch: [2]  [610/781]  eta: 0:00:22  lr: 0.000032  loss: 3.3216 (3.9393)  time: 0.1338  data: 0.0118  max mem: 4873\n",
            "Epoch: [2]  [620/781]  eta: 0:00:21  lr: 0.000032  loss: 3.3279 (3.9306)  time: 0.1313  data: 0.0089  max mem: 4873\n",
            "Epoch: [2]  [630/781]  eta: 0:00:19  lr: 0.000032  loss: 3.2899 (3.9220)  time: 0.1283  data: 0.0063  max mem: 4873\n",
            "Epoch: [2]  [640/781]  eta: 0:00:18  lr: 0.000032  loss: 3.2507 (3.9146)  time: 0.1256  data: 0.0036  max mem: 4873\n",
            "Epoch: [2]  [650/781]  eta: 0:00:17  lr: 0.000032  loss: 3.2507 (3.9057)  time: 0.1255  data: 0.0031  max mem: 4873\n",
            "Epoch: [2]  [660/781]  eta: 0:00:15  lr: 0.000032  loss: 3.1803 (3.8943)  time: 0.1260  data: 0.0019  max mem: 4873\n",
            "Epoch: [2]  [670/781]  eta: 0:00:14  lr: 0.000032  loss: 3.1658 (3.8843)  time: 0.1296  data: 0.0037  max mem: 4873\n",
            "Epoch: [2]  [680/781]  eta: 0:00:13  lr: 0.000032  loss: 3.1856 (3.8766)  time: 0.1336  data: 0.0077  max mem: 4873\n",
            "Epoch: [2]  [690/781]  eta: 0:00:11  lr: 0.000032  loss: 3.2093 (3.8689)  time: 0.1325  data: 0.0074  max mem: 4873\n",
            "Epoch: [2]  [700/781]  eta: 0:00:10  lr: 0.000032  loss: 3.2093 (3.8605)  time: 0.1286  data: 0.0047  max mem: 4873\n",
            "Epoch: [2]  [710/781]  eta: 0:00:09  lr: 0.000032  loss: 3.1867 (3.8517)  time: 0.1322  data: 0.0095  max mem: 4873\n",
            "Epoch: [2]  [720/781]  eta: 0:00:07  lr: 0.000032  loss: 3.1941 (3.8430)  time: 0.1388  data: 0.0164  max mem: 4873\n",
            "Epoch: [2]  [730/781]  eta: 0:00:06  lr: 0.000032  loss: 3.1941 (3.8369)  time: 0.1363  data: 0.0140  max mem: 4873\n",
            "Epoch: [2]  [740/781]  eta: 0:00:05  lr: 0.000032  loss: 3.1087 (3.8277)  time: 0.1312  data: 0.0080  max mem: 4873\n",
            "Epoch: [2]  [750/781]  eta: 0:00:04  lr: 0.000032  loss: 3.0719 (3.8175)  time: 0.1285  data: 0.0055  max mem: 4873\n",
            "Epoch: [2]  [760/781]  eta: 0:00:02  lr: 0.000032  loss: 3.0567 (3.8097)  time: 0.1277  data: 0.0059  max mem: 4873\n",
            "Epoch: [2]  [770/781]  eta: 0:00:01  lr: 0.000032  loss: 3.1319 (3.8029)  time: 0.1278  data: 0.0057  max mem: 4873\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000032  loss: 3.1319 (3.7975)  time: 0.1259  data: 0.0032  max mem: 4873\n",
            "Epoch: [2] Total time: 0:01:41 (0.1306 s / it)\n",
            "Averaged stats: lr: 0.000032  loss: 3.1319 (3.7975)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 1.3181 (1.3181)  acc1: 74.4792 (74.4792)  acc5: 90.6250 (90.6250)  time: 0.8203  data: 0.7895  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 2.0340 (1.9779)  acc1: 56.7708 (56.3920)  acc5: 85.4167 (83.2386)  time: 0.1653  data: 0.1346  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 2.0651 (2.2237)  acc1: 45.8333 (52.0833)  acc5: 78.1250 (78.4970)  time: 0.1187  data: 0.0880  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 2.4893 (2.2925)  acc1: 47.3958 (51.8985)  acc5: 73.4375 (77.9402)  time: 0.1172  data: 0.0865  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 2.5140 (2.4180)  acc1: 44.2708 (49.2505)  acc5: 73.4375 (75.4446)  time: 0.1125  data: 0.0818  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 2.5768 (2.4075)  acc1: 45.3125 (49.7038)  acc5: 70.8333 (75.4800)  time: 0.1134  data: 0.0827  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 2.5768 (2.4030)  acc1: 45.3125 (49.8200)  acc5: 74.4792 (75.7300)  time: 0.0962  data: 0.0665  max mem: 4873\n",
            "Test: Total time: 0:00:06 (0.1233 s / it)\n",
            "* Acc@1 49.820 Acc@5 75.730 loss 2.403\n",
            "Accuracy of the network on the 10000 test images: 49.8%\n",
            "Max accuracy: 49.82%\n",
            "Epoch: [3]  [  0/781]  eta: 0:11:52  lr: 0.000057  loss: 2.9966 (2.9966)  time: 0.9120  data: 0.7849  max mem: 4873\n",
            "Epoch: [3]  [ 10/781]  eta: 0:02:31  lr: 0.000057  loss: 3.1714 (3.3440)  time: 0.1970  data: 0.0738  max mem: 4873\n",
            "Epoch: [3]  [ 20/781]  eta: 0:02:07  lr: 0.000057  loss: 3.1411 (3.2744)  time: 0.1304  data: 0.0070  max mem: 4873\n",
            "Epoch: [3]  [ 30/781]  eta: 0:02:01  lr: 0.000057  loss: 3.0825 (3.2624)  time: 0.1429  data: 0.0183  max mem: 4873\n",
            "Epoch: [3]  [ 40/781]  eta: 0:01:54  lr: 0.000057  loss: 3.0972 (3.2482)  time: 0.1417  data: 0.0168  max mem: 4873\n",
            "Epoch: [3]  [ 50/781]  eta: 0:01:50  lr: 0.000057  loss: 3.1221 (3.2609)  time: 0.1348  data: 0.0117  max mem: 4873\n",
            "Epoch: [3]  [ 60/781]  eta: 0:01:47  lr: 0.000057  loss: 3.0818 (3.2449)  time: 0.1352  data: 0.0126  max mem: 4873\n",
            "Epoch: [3]  [ 70/781]  eta: 0:01:44  lr: 0.000057  loss: 3.0574 (3.2341)  time: 0.1339  data: 0.0112  max mem: 4873\n",
            "Epoch: [3]  [ 80/781]  eta: 0:01:41  lr: 0.000057  loss: 3.0916 (3.2314)  time: 0.1344  data: 0.0113  max mem: 4873\n",
            "Epoch: [3]  [ 90/781]  eta: 0:01:40  lr: 0.000057  loss: 3.0916 (3.2286)  time: 0.1401  data: 0.0161  max mem: 4873\n",
            "Epoch: [3]  [100/781]  eta: 0:01:38  lr: 0.000057  loss: 3.0329 (3.2033)  time: 0.1402  data: 0.0172  max mem: 4873\n",
            "Epoch: [3]  [110/781]  eta: 0:01:36  lr: 0.000057  loss: 2.9710 (3.1864)  time: 0.1395  data: 0.0164  max mem: 4873\n",
            "Epoch: [3]  [120/781]  eta: 0:01:35  lr: 0.000057  loss: 2.9563 (3.1764)  time: 0.1425  data: 0.0181  max mem: 4873\n",
            "Epoch: [3]  [130/781]  eta: 0:01:33  lr: 0.000057  loss: 2.9563 (3.1672)  time: 0.1402  data: 0.0171  max mem: 4873\n",
            "Epoch: [3]  [140/781]  eta: 0:01:31  lr: 0.000057  loss: 3.0182 (3.1673)  time: 0.1384  data: 0.0170  max mem: 4873\n",
            "Epoch: [3]  [150/781]  eta: 0:01:30  lr: 0.000057  loss: 2.9438 (3.1594)  time: 0.1416  data: 0.0198  max mem: 4873\n",
            "Epoch: [3]  [160/781]  eta: 0:01:28  lr: 0.000057  loss: 2.9721 (3.1610)  time: 0.1416  data: 0.0198  max mem: 4873\n",
            "Epoch: [3]  [170/781]  eta: 0:01:27  lr: 0.000057  loss: 3.0351 (3.1590)  time: 0.1364  data: 0.0145  max mem: 4873\n",
            "Epoch: [3]  [180/781]  eta: 0:01:25  lr: 0.000057  loss: 3.0301 (3.1607)  time: 0.1323  data: 0.0097  max mem: 4873\n",
            "Epoch: [3]  [190/781]  eta: 0:01:23  lr: 0.000057  loss: 2.9958 (3.1578)  time: 0.1348  data: 0.0126  max mem: 4873\n",
            "Epoch: [3]  [200/781]  eta: 0:01:22  lr: 0.000057  loss: 2.9898 (3.1619)  time: 0.1374  data: 0.0159  max mem: 4873\n",
            "Epoch: [3]  [210/781]  eta: 0:01:20  lr: 0.000057  loss: 2.8613 (3.1548)  time: 0.1398  data: 0.0171  max mem: 4873\n",
            "Epoch: [3]  [220/781]  eta: 0:01:19  lr: 0.000057  loss: 2.9064 (3.1466)  time: 0.1405  data: 0.0163  max mem: 4873\n",
            "Epoch: [3]  [230/781]  eta: 0:01:17  lr: 0.000057  loss: 2.9187 (3.1485)  time: 0.1381  data: 0.0147  max mem: 4873\n",
            "Epoch: [3]  [240/781]  eta: 0:01:16  lr: 0.000057  loss: 2.9097 (3.1495)  time: 0.1419  data: 0.0201  max mem: 4873\n",
            "Epoch: [3]  [250/781]  eta: 0:01:15  lr: 0.000057  loss: 2.8965 (3.1470)  time: 0.1468  data: 0.0252  max mem: 4873\n",
            "Epoch: [3]  [260/781]  eta: 0:01:13  lr: 0.000057  loss: 2.9092 (3.1389)  time: 0.1413  data: 0.0194  max mem: 4873\n",
            "Epoch: [3]  [270/781]  eta: 0:01:12  lr: 0.000057  loss: 2.9210 (3.1325)  time: 0.1358  data: 0.0139  max mem: 4873\n",
            "Epoch: [3]  [280/781]  eta: 0:01:10  lr: 0.000057  loss: 2.9204 (3.1286)  time: 0.1367  data: 0.0138  max mem: 4873\n",
            "Epoch: [3]  [290/781]  eta: 0:01:09  lr: 0.000057  loss: 2.8209 (3.1232)  time: 0.1369  data: 0.0125  max mem: 4873\n",
            "Epoch: [3]  [300/781]  eta: 0:01:07  lr: 0.000057  loss: 2.8547 (3.1197)  time: 0.1385  data: 0.0135  max mem: 4873\n",
            "Epoch: [3]  [310/781]  eta: 0:01:06  lr: 0.000057  loss: 2.8586 (3.1120)  time: 0.1404  data: 0.0170  max mem: 4873\n",
            "Epoch: [3]  [320/781]  eta: 0:01:04  lr: 0.000057  loss: 2.8422 (3.1065)  time: 0.1388  data: 0.0175  max mem: 4873\n",
            "Epoch: [3]  [330/781]  eta: 0:01:03  lr: 0.000057  loss: 2.8578 (3.1012)  time: 0.1368  data: 0.0154  max mem: 4873\n",
            "Epoch: [3]  [340/781]  eta: 0:01:01  lr: 0.000057  loss: 2.8332 (3.0964)  time: 0.1361  data: 0.0146  max mem: 4873\n",
            "Epoch: [3]  [350/781]  eta: 0:01:00  lr: 0.000057  loss: 2.8178 (3.0929)  time: 0.1388  data: 0.0065  max mem: 4873\n",
            "Epoch: [3]  [360/781]  eta: 0:00:58  lr: 0.000057  loss: 2.8178 (3.0891)  time: 0.1338  data: 0.0015  max mem: 4873\n",
            "Epoch: [3]  [370/781]  eta: 0:00:57  lr: 0.000057  loss: 2.8610 (3.0906)  time: 0.1286  data: 0.0071  max mem: 4873\n",
            "Epoch: [3]  [380/781]  eta: 0:00:56  lr: 0.000057  loss: 2.8717 (3.0839)  time: 0.1393  data: 0.0177  max mem: 4873\n",
            "Epoch: [3]  [390/781]  eta: 0:00:54  lr: 0.000057  loss: 2.9099 (3.0888)  time: 0.1410  data: 0.0183  max mem: 4873\n",
            "Epoch: [3]  [400/781]  eta: 0:00:53  lr: 0.000057  loss: 2.9099 (3.0892)  time: 0.1389  data: 0.0150  max mem: 4873\n",
            "Epoch: [3]  [410/781]  eta: 0:00:51  lr: 0.000057  loss: 2.7953 (3.0812)  time: 0.1375  data: 0.0136  max mem: 4873\n",
            "Epoch: [3]  [420/781]  eta: 0:00:50  lr: 0.000057  loss: 2.7513 (3.0749)  time: 0.1377  data: 0.0144  max mem: 4873\n",
            "Epoch: [3]  [430/781]  eta: 0:00:48  lr: 0.000057  loss: 2.7912 (3.0702)  time: 0.1351  data: 0.0130  max mem: 4873\n",
            "Epoch: [3]  [440/781]  eta: 0:00:47  lr: 0.000057  loss: 2.7918 (3.0664)  time: 0.1342  data: 0.0121  max mem: 4873\n",
            "Epoch: [3]  [450/781]  eta: 0:00:46  lr: 0.000057  loss: 2.8290 (3.0623)  time: 0.1351  data: 0.0129  max mem: 4873\n",
            "Epoch: [3]  [460/781]  eta: 0:00:44  lr: 0.000057  loss: 2.8290 (3.0585)  time: 0.1344  data: 0.0124  max mem: 4873\n",
            "Epoch: [3]  [470/781]  eta: 0:00:43  lr: 0.000057  loss: 2.7674 (3.0573)  time: 0.1371  data: 0.0145  max mem: 4873\n",
            "Epoch: [3]  [480/781]  eta: 0:00:41  lr: 0.000057  loss: 2.7728 (3.0518)  time: 0.1389  data: 0.0147  max mem: 4873\n",
            "Epoch: [3]  [490/781]  eta: 0:00:40  lr: 0.000057  loss: 2.7728 (3.0467)  time: 0.1358  data: 0.0123  max mem: 4873\n",
            "Epoch: [3]  [500/781]  eta: 0:00:39  lr: 0.000057  loss: 2.7963 (3.0451)  time: 0.1368  data: 0.0144  max mem: 4873\n",
            "Epoch: [3]  [510/781]  eta: 0:00:37  lr: 0.000057  loss: 2.7864 (3.0409)  time: 0.1440  data: 0.0212  max mem: 4873\n",
            "Epoch: [3]  [520/781]  eta: 0:00:36  lr: 0.000057  loss: 2.7135 (3.0386)  time: 0.1422  data: 0.0180  max mem: 4873\n",
            "Epoch: [3]  [530/781]  eta: 0:00:34  lr: 0.000057  loss: 2.7281 (3.0332)  time: 0.1382  data: 0.0133  max mem: 4873\n",
            "Epoch: [3]  [540/781]  eta: 0:00:33  lr: 0.000057  loss: 2.7597 (3.0280)  time: 0.1353  data: 0.0123  max mem: 4873\n",
            "Epoch: [3]  [550/781]  eta: 0:00:32  lr: 0.000057  loss: 2.7981 (3.0319)  time: 0.1324  data: 0.0101  max mem: 4873\n",
            "Epoch: [3]  [560/781]  eta: 0:00:30  lr: 0.000057  loss: 2.8128 (3.0278)  time: 0.1373  data: 0.0142  max mem: 4873\n",
            "Epoch: [3]  [570/781]  eta: 0:00:29  lr: 0.000057  loss: 2.7710 (3.0250)  time: 0.1442  data: 0.0206  max mem: 4873\n",
            "Epoch: [3]  [580/781]  eta: 0:00:27  lr: 0.000057  loss: 2.7710 (3.0238)  time: 0.1401  data: 0.0174  max mem: 4873\n",
            "Epoch: [3]  [590/781]  eta: 0:00:26  lr: 0.000057  loss: 2.6697 (3.0203)  time: 0.1354  data: 0.0138  max mem: 4873\n",
            "Epoch: [3]  [600/781]  eta: 0:00:25  lr: 0.000057  loss: 2.6843 (3.0183)  time: 0.1361  data: 0.0145  max mem: 4873\n",
            "Epoch: [3]  [610/781]  eta: 0:00:23  lr: 0.000057  loss: 2.7766 (3.0179)  time: 0.1348  data: 0.0124  max mem: 4873\n",
            "Epoch: [3]  [620/781]  eta: 0:00:22  lr: 0.000057  loss: 2.7820 (3.0141)  time: 0.1338  data: 0.0106  max mem: 4873\n",
            "Epoch: [3]  [630/781]  eta: 0:00:20  lr: 0.000057  loss: 2.7650 (3.0118)  time: 0.1356  data: 0.0124  max mem: 4873\n",
            "Epoch: [3]  [640/781]  eta: 0:00:19  lr: 0.000057  loss: 2.7572 (3.0074)  time: 0.1406  data: 0.0180  max mem: 4873\n",
            "Epoch: [3]  [650/781]  eta: 0:00:18  lr: 0.000057  loss: 2.7779 (3.0073)  time: 0.1437  data: 0.0207  max mem: 4873\n",
            "Epoch: [3]  [660/781]  eta: 0:00:16  lr: 0.000057  loss: 2.7779 (3.0061)  time: 0.1372  data: 0.0143  max mem: 4873\n",
            "Epoch: [3]  [670/781]  eta: 0:00:15  lr: 0.000057  loss: 2.7474 (3.0057)  time: 0.1365  data: 0.0147  max mem: 4873\n",
            "Epoch: [3]  [680/781]  eta: 0:00:14  lr: 0.000057  loss: 2.6902 (3.0010)  time: 0.1441  data: 0.0229  max mem: 4873\n",
            "Epoch: [3]  [690/781]  eta: 0:00:12  lr: 0.000057  loss: 2.6328 (2.9966)  time: 0.1417  data: 0.0205  max mem: 4873\n",
            "Epoch: [3]  [700/781]  eta: 0:00:11  lr: 0.000057  loss: 2.6583 (2.9925)  time: 0.1412  data: 0.0201  max mem: 4873\n",
            "Epoch: [3]  [710/781]  eta: 0:00:09  lr: 0.000057  loss: 2.7101 (2.9931)  time: 0.1374  data: 0.0161  max mem: 4873\n",
            "Epoch: [3]  [720/781]  eta: 0:00:08  lr: 0.000057  loss: 2.7649 (2.9921)  time: 0.1323  data: 0.0104  max mem: 4873\n",
            "Epoch: [3]  [730/781]  eta: 0:00:07  lr: 0.000057  loss: 2.6741 (2.9874)  time: 0.1350  data: 0.0124  max mem: 4873\n",
            "Epoch: [3]  [740/781]  eta: 0:00:05  lr: 0.000057  loss: 2.6505 (2.9848)  time: 0.1421  data: 0.0191  max mem: 4873\n",
            "Epoch: [3]  [750/781]  eta: 0:00:04  lr: 0.000057  loss: 2.6610 (2.9826)  time: 0.1464  data: 0.0231  max mem: 4873\n",
            "Epoch: [3]  [760/781]  eta: 0:00:02  lr: 0.000057  loss: 2.6990 (2.9806)  time: 0.1418  data: 0.0186  max mem: 4873\n",
            "Epoch: [3]  [770/781]  eta: 0:00:01  lr: 0.000057  loss: 2.6999 (2.9769)  time: 0.1365  data: 0.0137  max mem: 4873\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000057  loss: 2.6474 (2.9741)  time: 0.1335  data: 0.0111  max mem: 4873\n",
            "Epoch: [3] Total time: 0:01:48 (0.1391 s / it)\n",
            "Averaged stats: lr: 0.000057  loss: 2.6474 (2.9741)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.1429 (1.1429)  acc1: 73.9583 (73.9583)  acc5: 91.6667 (91.6667)  time: 0.8341  data: 0.8031  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.4886 (1.4744)  acc1: 67.7083 (66.0511)  acc5: 91.6667 (88.7311)  time: 0.1670  data: 0.1363  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.4886 (1.5396)  acc1: 63.5417 (65.4514)  acc5: 85.9375 (86.7808)  time: 0.1215  data: 0.0908  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.5923 (1.5595)  acc1: 66.1458 (65.3562)  acc5: 84.8958 (86.6767)  time: 0.1271  data: 0.0965  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.6528 (1.6456)  acc1: 61.9792 (63.2622)  acc5: 84.8958 (85.2007)  time: 0.1298  data: 0.0991  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.6528 (1.6429)  acc1: 58.3333 (63.1332)  acc5: 83.3333 (85.1103)  time: 0.1284  data: 0.0977  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.7541 (1.6498)  acc1: 58.3333 (62.9800)  acc5: 83.8542 (85.1400)  time: 0.1082  data: 0.0785  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1327 s / it)\n",
            "* Acc@1 62.980 Acc@5 85.140 loss 1.650\n",
            "Accuracy of the network on the 10000 test images: 63.0%\n",
            "Max accuracy: 62.98%\n",
            "Epoch: [4]  [  0/781]  eta: 0:11:58  lr: 0.000052  loss: 2.6219 (2.6219)  time: 0.9200  data: 0.7922  max mem: 4873\n",
            "Epoch: [4]  [ 10/781]  eta: 0:02:34  lr: 0.000052  loss: 2.6219 (2.6956)  time: 0.2001  data: 0.0754  max mem: 4873\n",
            "Epoch: [4]  [ 20/781]  eta: 0:02:05  lr: 0.000052  loss: 2.6498 (2.7559)  time: 0.1278  data: 0.0047  max mem: 4873\n",
            "Epoch: [4]  [ 30/781]  eta: 0:01:59  lr: 0.000052  loss: 2.6804 (2.7531)  time: 0.1362  data: 0.0139  max mem: 4873\n",
            "Epoch: [4]  [ 40/781]  eta: 0:01:52  lr: 0.000052  loss: 2.6699 (2.7265)  time: 0.1380  data: 0.0143  max mem: 4873\n",
            "Epoch: [4]  [ 50/781]  eta: 0:01:49  lr: 0.000052  loss: 2.6870 (2.7371)  time: 0.1363  data: 0.0136  max mem: 4873\n",
            "Epoch: [4]  [ 60/781]  eta: 0:01:46  lr: 0.000052  loss: 2.6820 (2.7575)  time: 0.1388  data: 0.0171  max mem: 4873\n",
            "Epoch: [4]  [ 70/781]  eta: 0:01:44  lr: 0.000052  loss: 2.6081 (2.7583)  time: 0.1412  data: 0.0188  max mem: 4873\n",
            "Epoch: [4]  [ 80/781]  eta: 0:01:42  lr: 0.000052  loss: 2.6163 (2.7589)  time: 0.1437  data: 0.0204  max mem: 4873\n",
            "Epoch: [4]  [ 90/781]  eta: 0:01:41  lr: 0.000052  loss: 2.6040 (2.7510)  time: 0.1442  data: 0.0211  max mem: 4873\n",
            "Epoch: [4]  [100/781]  eta: 0:01:39  lr: 0.000052  loss: 2.5410 (2.7257)  time: 0.1413  data: 0.0197  max mem: 4873\n",
            "Epoch: [4]  [110/781]  eta: 0:01:37  lr: 0.000052  loss: 2.5492 (2.7252)  time: 0.1380  data: 0.0156  max mem: 4873\n",
            "Epoch: [4]  [120/781]  eta: 0:01:35  lr: 0.000052  loss: 2.5702 (2.7152)  time: 0.1375  data: 0.0143  max mem: 4873\n",
            "Epoch: [4]  [130/781]  eta: 0:01:33  lr: 0.000052  loss: 2.5615 (2.7056)  time: 0.1386  data: 0.0160  max mem: 4873\n",
            "Epoch: [4]  [140/781]  eta: 0:01:32  lr: 0.000052  loss: 2.6173 (2.7070)  time: 0.1421  data: 0.0199  max mem: 4873\n",
            "Epoch: [4]  [150/781]  eta: 0:01:30  lr: 0.000052  loss: 2.6649 (2.7173)  time: 0.1423  data: 0.0205  max mem: 4873\n",
            "Epoch: [4]  [160/781]  eta: 0:01:29  lr: 0.000052  loss: 2.5849 (2.7082)  time: 0.1408  data: 0.0187  max mem: 4873\n",
            "Epoch: [4]  [170/781]  eta: 0:01:27  lr: 0.000052  loss: 2.5804 (2.7121)  time: 0.1395  data: 0.0155  max mem: 4873\n",
            "Epoch: [4]  [180/781]  eta: 0:01:26  lr: 0.000052  loss: 2.5443 (2.7059)  time: 0.1407  data: 0.0161  max mem: 4873\n",
            "Epoch: [4]  [190/781]  eta: 0:01:24  lr: 0.000052  loss: 2.5373 (2.6967)  time: 0.1446  data: 0.0217  max mem: 4873\n",
            "Epoch: [4]  [200/781]  eta: 0:01:23  lr: 0.000052  loss: 2.5485 (2.6993)  time: 0.1395  data: 0.0171  max mem: 4873\n",
            "Epoch: [4]  [210/781]  eta: 0:01:21  lr: 0.000052  loss: 2.5922 (2.7038)  time: 0.1337  data: 0.0119  max mem: 4873\n",
            "Epoch: [4]  [220/781]  eta: 0:01:19  lr: 0.000052  loss: 2.6450 (2.7149)  time: 0.1357  data: 0.0140  max mem: 4873\n",
            "Epoch: [4]  [230/781]  eta: 0:01:18  lr: 0.000052  loss: 2.6627 (2.7242)  time: 0.1330  data: 0.0104  max mem: 4873\n",
            "Epoch: [4]  [240/781]  eta: 0:01:16  lr: 0.000052  loss: 2.6627 (2.7213)  time: 0.1355  data: 0.0125  max mem: 4873\n",
            "Epoch: [4]  [250/781]  eta: 0:01:15  lr: 0.000052  loss: 2.5480 (2.7174)  time: 0.1465  data: 0.0238  max mem: 4873\n",
            "Epoch: [4]  [260/781]  eta: 0:01:14  lr: 0.000052  loss: 2.6099 (2.7191)  time: 0.1483  data: 0.0253  max mem: 4873\n",
            "Epoch: [4]  [270/781]  eta: 0:01:12  lr: 0.000052  loss: 2.6616 (2.7138)  time: 0.1447  data: 0.0209  max mem: 4873\n",
            "Epoch: [4]  [280/781]  eta: 0:01:11  lr: 0.000052  loss: 2.5696 (2.7099)  time: 0.1371  data: 0.0144  max mem: 4873\n",
            "Epoch: [4]  [290/781]  eta: 0:01:09  lr: 0.000052  loss: 2.5696 (2.7088)  time: 0.1424  data: 0.0199  max mem: 4873\n",
            "Epoch: [4]  [300/781]  eta: 0:01:08  lr: 0.000052  loss: 2.4953 (2.7070)  time: 0.1505  data: 0.0270  max mem: 4873\n",
            "Epoch: [4]  [310/781]  eta: 0:01:07  lr: 0.000052  loss: 2.5371 (2.7075)  time: 0.1405  data: 0.0175  max mem: 4873\n",
            "Epoch: [4]  [320/781]  eta: 0:01:05  lr: 0.000052  loss: 2.6350 (2.7102)  time: 0.1343  data: 0.0116  max mem: 4873\n",
            "Epoch: [4]  [330/781]  eta: 0:01:03  lr: 0.000052  loss: 2.5740 (2.7074)  time: 0.1351  data: 0.0120  max mem: 4873\n",
            "Epoch: [4]  [340/781]  eta: 0:01:02  lr: 0.000052  loss: 2.5648 (2.7070)  time: 0.1354  data: 0.0119  max mem: 4873\n",
            "Epoch: [4]  [350/781]  eta: 0:01:01  lr: 0.000052  loss: 2.5654 (2.7028)  time: 0.1377  data: 0.0128  max mem: 4873\n",
            "Epoch: [4]  [360/781]  eta: 0:00:59  lr: 0.000052  loss: 2.5481 (2.7031)  time: 0.1379  data: 0.0135  max mem: 4873\n",
            "Epoch: [4]  [370/781]  eta: 0:00:58  lr: 0.000052  loss: 2.5651 (2.7030)  time: 0.1352  data: 0.0126  max mem: 4873\n",
            "Epoch: [4]  [380/781]  eta: 0:00:56  lr: 0.000052  loss: 2.5894 (2.7124)  time: 0.1346  data: 0.0121  max mem: 4873\n",
            "Epoch: [4]  [390/781]  eta: 0:00:55  lr: 0.000052  loss: 2.6180 (2.7097)  time: 0.1346  data: 0.0127  max mem: 4873\n",
            "Epoch: [4]  [400/781]  eta: 0:00:53  lr: 0.000052  loss: 2.5540 (2.7091)  time: 0.1358  data: 0.0140  max mem: 4873\n",
            "Epoch: [4]  [410/781]  eta: 0:00:52  lr: 0.000052  loss: 2.5124 (2.7097)  time: 0.1350  data: 0.0130  max mem: 4873\n",
            "Epoch: [4]  [420/781]  eta: 0:00:50  lr: 0.000052  loss: 2.6523 (2.7149)  time: 0.1317  data: 0.0080  max mem: 4873\n",
            "Epoch: [4]  [430/781]  eta: 0:00:49  lr: 0.000052  loss: 2.6264 (2.7117)  time: 0.1310  data: 0.0062  max mem: 4873\n",
            "Epoch: [4]  [440/781]  eta: 0:00:47  lr: 0.000052  loss: 2.5651 (2.7112)  time: 0.1408  data: 0.0172  max mem: 4873\n",
            "Epoch: [4]  [450/781]  eta: 0:00:46  lr: 0.000052  loss: 2.5651 (2.7178)  time: 0.1407  data: 0.0178  max mem: 4873\n",
            "Epoch: [4]  [460/781]  eta: 0:00:44  lr: 0.000052  loss: 2.5546 (2.7161)  time: 0.1321  data: 0.0096  max mem: 4873\n",
            "Epoch: [4]  [470/781]  eta: 0:00:43  lr: 0.000052  loss: 2.5030 (2.7149)  time: 0.1326  data: 0.0099  max mem: 4873\n",
            "Epoch: [4]  [480/781]  eta: 0:00:42  lr: 0.000052  loss: 2.5236 (2.7132)  time: 0.1339  data: 0.0116  max mem: 4873\n",
            "Epoch: [4]  [490/781]  eta: 0:00:40  lr: 0.000052  loss: 2.5449 (2.7114)  time: 0.1341  data: 0.0122  max mem: 4873\n",
            "Epoch: [4]  [500/781]  eta: 0:00:39  lr: 0.000052  loss: 2.5372 (2.7083)  time: 0.1338  data: 0.0114  max mem: 4873\n",
            "Epoch: [4]  [510/781]  eta: 0:00:37  lr: 0.000052  loss: 2.5952 (2.7103)  time: 0.1363  data: 0.0138  max mem: 4873\n",
            "Epoch: [4]  [520/781]  eta: 0:00:36  lr: 0.000052  loss: 2.4981 (2.7061)  time: 0.1372  data: 0.0144  max mem: 4873\n",
            "Epoch: [4]  [530/781]  eta: 0:00:34  lr: 0.000052  loss: 2.4821 (2.7069)  time: 0.1363  data: 0.0126  max mem: 4873\n",
            "Epoch: [4]  [540/781]  eta: 0:00:33  lr: 0.000052  loss: 2.4997 (2.7043)  time: 0.1426  data: 0.0188  max mem: 4873\n",
            "Epoch: [4]  [550/781]  eta: 0:00:32  lr: 0.000052  loss: 2.4997 (2.7008)  time: 0.1460  data: 0.0236  max mem: 4873\n",
            "Epoch: [4]  [560/781]  eta: 0:00:30  lr: 0.000052  loss: 2.5402 (2.7032)  time: 0.1445  data: 0.0230  max mem: 4873\n",
            "Epoch: [4]  [570/781]  eta: 0:00:29  lr: 0.000052  loss: 2.5513 (2.7020)  time: 0.1360  data: 0.0140  max mem: 4873\n",
            "Epoch: [4]  [580/781]  eta: 0:00:28  lr: 0.000052  loss: 2.5213 (2.7014)  time: 0.1327  data: 0.0101  max mem: 4873\n",
            "Epoch: [4]  [590/781]  eta: 0:00:26  lr: 0.000052  loss: 2.5132 (2.7046)  time: 0.1325  data: 0.0102  max mem: 4873\n",
            "Epoch: [4]  [600/781]  eta: 0:00:25  lr: 0.000052  loss: 2.5537 (2.7034)  time: 0.1318  data: 0.0102  max mem: 4873\n",
            "Epoch: [4]  [610/781]  eta: 0:00:23  lr: 0.000052  loss: 2.5537 (2.7005)  time: 0.1379  data: 0.0162  max mem: 4873\n",
            "Epoch: [4]  [620/781]  eta: 0:00:22  lr: 0.000052  loss: 2.5062 (2.6983)  time: 0.1374  data: 0.0141  max mem: 4873\n",
            "Epoch: [4]  [630/781]  eta: 0:00:21  lr: 0.000052  loss: 2.4712 (2.7008)  time: 0.1353  data: 0.0104  max mem: 4873\n",
            "Epoch: [4]  [640/781]  eta: 0:00:19  lr: 0.000052  loss: 2.4932 (2.7019)  time: 0.1335  data: 0.0095  max mem: 4873\n",
            "Epoch: [4]  [650/781]  eta: 0:00:18  lr: 0.000052  loss: 2.4932 (2.7012)  time: 0.1307  data: 0.0084  max mem: 4873\n",
            "Epoch: [4]  [660/781]  eta: 0:00:16  lr: 0.000052  loss: 2.4921 (2.7002)  time: 0.1332  data: 0.0102  max mem: 4873\n",
            "Epoch: [4]  [670/781]  eta: 0:00:15  lr: 0.000052  loss: 2.5191 (2.6994)  time: 0.1372  data: 0.0121  max mem: 4873\n",
            "Epoch: [4]  [680/781]  eta: 0:00:14  lr: 0.000052  loss: 2.5860 (2.6999)  time: 0.1347  data: 0.0100  max mem: 4873\n",
            "Epoch: [4]  [690/781]  eta: 0:00:12  lr: 0.000052  loss: 2.5540 (2.6977)  time: 0.1339  data: 0.0101  max mem: 4873\n",
            "Epoch: [4]  [700/781]  eta: 0:00:11  lr: 0.000052  loss: 2.4812 (2.6986)  time: 0.1345  data: 0.0112  max mem: 4873\n",
            "Epoch: [4]  [710/781]  eta: 0:00:09  lr: 0.000052  loss: 2.4679 (2.6972)  time: 0.1376  data: 0.0151  max mem: 4873\n",
            "Epoch: [4]  [720/781]  eta: 0:00:08  lr: 0.000052  loss: 2.4679 (2.6949)  time: 0.1408  data: 0.0171  max mem: 4873\n",
            "Epoch: [4]  [730/781]  eta: 0:00:07  lr: 0.000052  loss: 2.4693 (2.6930)  time: 0.1405  data: 0.0172  max mem: 4873\n",
            "Epoch: [4]  [740/781]  eta: 0:00:05  lr: 0.000052  loss: 2.4721 (2.6945)  time: 0.1401  data: 0.0180  max mem: 4873\n",
            "Epoch: [4]  [750/781]  eta: 0:00:04  lr: 0.000052  loss: 2.4721 (2.6941)  time: 0.1381  data: 0.0162  max mem: 4873\n",
            "Epoch: [4]  [760/781]  eta: 0:00:02  lr: 0.000052  loss: 2.5384 (2.6958)  time: 0.1319  data: 0.0094  max mem: 4873\n",
            "Epoch: [4]  [770/781]  eta: 0:00:01  lr: 0.000052  loss: 2.5642 (2.6940)  time: 0.1276  data: 0.0052  max mem: 4873\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000052  loss: 2.6130 (2.7001)  time: 0.1295  data: 0.0083  max mem: 4873\n",
            "Epoch: [4] Total time: 0:01:48 (0.1384 s / it)\n",
            "Averaged stats: lr: 0.000052  loss: 2.6130 (2.7001)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.9805 (0.9805)  acc1: 79.1667 (79.1667)  acc5: 92.7083 (92.7083)  time: 0.8465  data: 0.8156  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.3718 (1.2694)  acc1: 69.7917 (71.2595)  acc5: 92.1875 (90.9091)  time: 0.1701  data: 0.1394  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.4371 (1.3602)  acc1: 66.6667 (69.4692)  acc5: 89.0625 (89.1617)  time: 0.1262  data: 0.0955  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.5244 (1.4092)  acc1: 66.6667 (68.6828)  acc5: 87.5000 (88.4913)  time: 0.1298  data: 0.0991  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.5647 (1.4633)  acc1: 65.6250 (67.3272)  acc5: 85.9375 (87.6270)  time: 0.1322  data: 0.1015  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4371 (1.4574)  acc1: 67.1875 (67.5551)  acc5: 86.4583 (87.8166)  time: 0.1302  data: 0.0996  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4086 (1.4496)  acc1: 67.1875 (67.4700)  acc5: 88.0208 (87.9300)  time: 0.1113  data: 0.0817  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1351 s / it)\n",
            "* Acc@1 67.470 Acc@5 87.930 loss 1.450\n",
            "Accuracy of the network on the 10000 test images: 67.5%\n",
            "Max accuracy: 67.47%\n",
            "Epoch: [5]  [  0/781]  eta: 0:11:11  lr: 0.000044  loss: 2.5977 (2.5977)  time: 0.8597  data: 0.7322  max mem: 4873\n",
            "Epoch: [5]  [ 10/781]  eta: 0:02:40  lr: 0.000044  loss: 2.5344 (2.7867)  time: 0.2085  data: 0.0857  max mem: 4873\n",
            "Epoch: [5]  [ 20/781]  eta: 0:02:16  lr: 0.000044  loss: 2.4817 (2.6795)  time: 0.1455  data: 0.0229  max mem: 4873\n",
            "Epoch: [5]  [ 30/781]  eta: 0:02:03  lr: 0.000044  loss: 2.5331 (2.6333)  time: 0.1400  data: 0.0173  max mem: 4873\n",
            "Epoch: [5]  [ 40/781]  eta: 0:01:54  lr: 0.000044  loss: 2.3928 (2.5746)  time: 0.1287  data: 0.0058  max mem: 4873\n",
            "Epoch: [5]  [ 50/781]  eta: 0:01:50  lr: 0.000044  loss: 2.3846 (2.5918)  time: 0.1294  data: 0.0051  max mem: 4873\n",
            "Epoch: [5]  [ 60/781]  eta: 0:01:47  lr: 0.000044  loss: 2.4658 (2.6277)  time: 0.1390  data: 0.0152  max mem: 4873\n",
            "Epoch: [5]  [ 70/781]  eta: 0:01:43  lr: 0.000044  loss: 2.6442 (2.6878)  time: 0.1340  data: 0.0112  max mem: 4873\n",
            "Epoch: [5]  [ 80/781]  eta: 0:01:40  lr: 0.000044  loss: 2.6288 (2.6695)  time: 0.1240  data: 0.0010  max mem: 4873\n",
            "Epoch: [5]  [ 90/781]  eta: 0:01:37  lr: 0.000044  loss: 2.5283 (2.6874)  time: 0.1242  data: 0.0012  max mem: 4873\n",
            "Epoch: [5]  [100/781]  eta: 0:01:34  lr: 0.000044  loss: 2.4261 (2.6672)  time: 0.1233  data: 0.0005  max mem: 4873\n",
            "Epoch: [5]  [110/781]  eta: 0:01:32  lr: 0.000044  loss: 2.4261 (2.6800)  time: 0.1265  data: 0.0037  max mem: 4873\n",
            "Epoch: [5]  [120/781]  eta: 0:01:30  lr: 0.000044  loss: 2.4951 (2.6762)  time: 0.1289  data: 0.0059  max mem: 4873\n",
            "Epoch: [5]  [130/781]  eta: 0:01:29  lr: 0.000044  loss: 2.4635 (2.6907)  time: 0.1269  data: 0.0043  max mem: 4873\n",
            "Epoch: [5]  [140/781]  eta: 0:01:27  lr: 0.000044  loss: 2.5144 (2.6914)  time: 0.1256  data: 0.0023  max mem: 4873\n",
            "Epoch: [5]  [150/781]  eta: 0:01:26  lr: 0.000044  loss: 2.5144 (2.6984)  time: 0.1338  data: 0.0086  max mem: 4873\n",
            "Epoch: [5]  [160/781]  eta: 0:01:24  lr: 0.000044  loss: 2.4195 (2.6815)  time: 0.1343  data: 0.0093  max mem: 4873\n",
            "Epoch: [5]  [170/781]  eta: 0:01:22  lr: 0.000044  loss: 2.4712 (2.6761)  time: 0.1294  data: 0.0062  max mem: 4873\n",
            "Epoch: [5]  [180/781]  eta: 0:01:21  lr: 0.000044  loss: 2.5211 (2.6753)  time: 0.1330  data: 0.0101  max mem: 4873\n",
            "Epoch: [5]  [190/781]  eta: 0:01:19  lr: 0.000044  loss: 2.5008 (2.6761)  time: 0.1295  data: 0.0063  max mem: 4873\n",
            "Epoch: [5]  [200/781]  eta: 0:01:18  lr: 0.000044  loss: 2.5008 (2.6713)  time: 0.1255  data: 0.0015  max mem: 4873\n",
            "Epoch: [5]  [210/781]  eta: 0:01:16  lr: 0.000044  loss: 2.4113 (2.6732)  time: 0.1284  data: 0.0047  max mem: 4873\n",
            "Epoch: [5]  [220/781]  eta: 0:01:15  lr: 0.000044  loss: 2.4509 (2.6678)  time: 0.1313  data: 0.0084  max mem: 4873\n",
            "Epoch: [5]  [230/781]  eta: 0:01:14  lr: 0.000044  loss: 2.4977 (2.6711)  time: 0.1366  data: 0.0138  max mem: 4873\n",
            "Epoch: [5]  [240/781]  eta: 0:01:12  lr: 0.000044  loss: 2.4893 (2.6677)  time: 0.1398  data: 0.0169  max mem: 4873\n",
            "Epoch: [5]  [250/781]  eta: 0:01:11  lr: 0.000044  loss: 2.3754 (2.6558)  time: 0.1388  data: 0.0155  max mem: 4873\n",
            "Epoch: [5]  [260/781]  eta: 0:01:10  lr: 0.000044  loss: 2.4116 (2.6594)  time: 0.1326  data: 0.0096  max mem: 4873\n",
            "Epoch: [5]  [270/781]  eta: 0:01:08  lr: 0.000044  loss: 2.4582 (2.6561)  time: 0.1243  data: 0.0019  max mem: 4873\n",
            "Epoch: [5]  [280/781]  eta: 0:01:07  lr: 0.000044  loss: 2.4515 (2.6508)  time: 0.1264  data: 0.0016  max mem: 4873\n",
            "Epoch: [5]  [290/781]  eta: 0:01:05  lr: 0.000044  loss: 2.4415 (2.6512)  time: 0.1273  data: 0.0013  max mem: 4873\n",
            "Epoch: [5]  [300/781]  eta: 0:01:04  lr: 0.000044  loss: 2.3596 (2.6509)  time: 0.1268  data: 0.0034  max mem: 4873\n",
            "Epoch: [5]  [310/781]  eta: 0:01:02  lr: 0.000044  loss: 2.4041 (2.6496)  time: 0.1312  data: 0.0086  max mem: 4873\n",
            "Epoch: [5]  [320/781]  eta: 0:01:01  lr: 0.000044  loss: 2.4830 (2.6505)  time: 0.1314  data: 0.0086  max mem: 4873\n",
            "Epoch: [5]  [330/781]  eta: 0:01:00  lr: 0.000044  loss: 2.4250 (2.6461)  time: 0.1272  data: 0.0032  max mem: 4873\n",
            "Epoch: [5]  [340/781]  eta: 0:00:58  lr: 0.000044  loss: 2.4158 (2.6519)  time: 0.1251  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [350/781]  eta: 0:00:57  lr: 0.000044  loss: 2.4158 (2.6452)  time: 0.1276  data: 0.0040  max mem: 4873\n",
            "Epoch: [5]  [360/781]  eta: 0:00:55  lr: 0.000044  loss: 2.4033 (2.6441)  time: 0.1332  data: 0.0103  max mem: 4873\n",
            "Epoch: [5]  [370/781]  eta: 0:00:54  lr: 0.000044  loss: 2.4432 (2.6434)  time: 0.1396  data: 0.0170  max mem: 4873\n",
            "Epoch: [5]  [380/781]  eta: 0:00:53  lr: 0.000044  loss: 2.4456 (2.6389)  time: 0.1379  data: 0.0134  max mem: 4873\n",
            "Epoch: [5]  [390/781]  eta: 0:00:51  lr: 0.000044  loss: 2.4383 (2.6431)  time: 0.1293  data: 0.0033  max mem: 4873\n",
            "Epoch: [5]  [400/781]  eta: 0:00:50  lr: 0.000044  loss: 2.4778 (2.6395)  time: 0.1242  data: 0.0006  max mem: 4873\n",
            "Epoch: [5]  [410/781]  eta: 0:00:49  lr: 0.000044  loss: 2.4484 (2.6366)  time: 0.1237  data: 0.0015  max mem: 4873\n",
            "Epoch: [5]  [420/781]  eta: 0:00:47  lr: 0.000044  loss: 2.4162 (2.6350)  time: 0.1293  data: 0.0060  max mem: 4873\n",
            "Epoch: [5]  [430/781]  eta: 0:00:46  lr: 0.000044  loss: 2.4162 (2.6328)  time: 0.1379  data: 0.0145  max mem: 4873\n",
            "Epoch: [5]  [440/781]  eta: 0:00:45  lr: 0.000044  loss: 2.3539 (2.6340)  time: 0.1360  data: 0.0116  max mem: 4873\n",
            "Epoch: [5]  [450/781]  eta: 0:00:43  lr: 0.000044  loss: 2.3817 (2.6286)  time: 0.1301  data: 0.0062  max mem: 4873\n",
            "Epoch: [5]  [460/781]  eta: 0:00:42  lr: 0.000044  loss: 2.4289 (2.6260)  time: 0.1293  data: 0.0074  max mem: 4873\n",
            "Epoch: [5]  [470/781]  eta: 0:00:41  lr: 0.000044  loss: 2.4590 (2.6267)  time: 0.1258  data: 0.0032  max mem: 4873\n",
            "Epoch: [5]  [480/781]  eta: 0:00:39  lr: 0.000044  loss: 2.4257 (2.6265)  time: 0.1240  data: 0.0006  max mem: 4873\n",
            "Epoch: [5]  [490/781]  eta: 0:00:38  lr: 0.000044  loss: 2.4187 (2.6263)  time: 0.1272  data: 0.0038  max mem: 4873\n",
            "Epoch: [5]  [500/781]  eta: 0:00:37  lr: 0.000044  loss: 2.3951 (2.6223)  time: 0.1290  data: 0.0037  max mem: 4873\n",
            "Epoch: [5]  [510/781]  eta: 0:00:35  lr: 0.000044  loss: 2.3951 (2.6184)  time: 0.1259  data: 0.0004  max mem: 4873\n",
            "Epoch: [5]  [520/781]  eta: 0:00:34  lr: 0.000044  loss: 2.4123 (2.6161)  time: 0.1244  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [530/781]  eta: 0:00:33  lr: 0.000044  loss: 2.4548 (2.6171)  time: 0.1283  data: 0.0033  max mem: 4873\n",
            "Epoch: [5]  [540/781]  eta: 0:00:31  lr: 0.000044  loss: 2.4597 (2.6171)  time: 0.1283  data: 0.0040  max mem: 4873\n",
            "Epoch: [5]  [550/781]  eta: 0:00:30  lr: 0.000044  loss: 2.4633 (2.6168)  time: 0.1251  data: 0.0022  max mem: 4873\n",
            "Epoch: [5]  [560/781]  eta: 0:00:29  lr: 0.000044  loss: 2.4499 (2.6155)  time: 0.1295  data: 0.0067  max mem: 4873\n",
            "Epoch: [5]  [570/781]  eta: 0:00:27  lr: 0.000044  loss: 2.4378 (2.6132)  time: 0.1345  data: 0.0120  max mem: 4873\n",
            "Epoch: [5]  [580/781]  eta: 0:00:26  lr: 0.000044  loss: 2.4709 (2.6148)  time: 0.1426  data: 0.0201  max mem: 4873\n",
            "Epoch: [5]  [590/781]  eta: 0:00:25  lr: 0.000044  loss: 2.4416 (2.6145)  time: 0.1393  data: 0.0167  max mem: 4873\n",
            "Epoch: [5]  [600/781]  eta: 0:00:23  lr: 0.000044  loss: 2.4416 (2.6155)  time: 0.1282  data: 0.0062  max mem: 4873\n",
            "Epoch: [5]  [610/781]  eta: 0:00:22  lr: 0.000044  loss: 2.4107 (2.6155)  time: 0.1254  data: 0.0036  max mem: 4873\n",
            "Epoch: [5]  [620/781]  eta: 0:00:21  lr: 0.000044  loss: 2.4150 (2.6172)  time: 0.1276  data: 0.0038  max mem: 4873\n",
            "Epoch: [5]  [630/781]  eta: 0:00:19  lr: 0.000044  loss: 2.4467 (2.6148)  time: 0.1329  data: 0.0069  max mem: 4873\n",
            "Epoch: [5]  [640/781]  eta: 0:00:18  lr: 0.000044  loss: 2.3538 (2.6115)  time: 0.1353  data: 0.0113  max mem: 4873\n",
            "Epoch: [5]  [650/781]  eta: 0:00:17  lr: 0.000044  loss: 2.3287 (2.6147)  time: 0.1355  data: 0.0133  max mem: 4873\n",
            "Epoch: [5]  [660/781]  eta: 0:00:15  lr: 0.000044  loss: 2.3626 (2.6109)  time: 0.1388  data: 0.0059  max mem: 4873\n",
            "Epoch: [5]  [670/781]  eta: 0:00:14  lr: 0.000044  loss: 2.4012 (2.6082)  time: 0.1332  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [680/781]  eta: 0:00:13  lr: 0.000044  loss: 2.4074 (2.6061)  time: 0.1240  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [690/781]  eta: 0:00:11  lr: 0.000044  loss: 2.4012 (2.6069)  time: 0.1248  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [700/781]  eta: 0:00:10  lr: 0.000044  loss: 2.3925 (2.6053)  time: 0.1235  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [710/781]  eta: 0:00:09  lr: 0.000044  loss: 2.4224 (2.6047)  time: 0.1228  data: 0.0003  max mem: 4873\n",
            "Epoch: [5]  [720/781]  eta: 0:00:08  lr: 0.000044  loss: 2.4423 (2.6059)  time: 0.1250  data: 0.0019  max mem: 4873\n",
            "Epoch: [5]  [730/781]  eta: 0:00:06  lr: 0.000044  loss: 2.4423 (2.6072)  time: 0.1294  data: 0.0050  max mem: 4873\n",
            "Epoch: [5]  [740/781]  eta: 0:00:05  lr: 0.000044  loss: 2.4662 (2.6076)  time: 0.1298  data: 0.0050  max mem: 4873\n",
            "Epoch: [5]  [750/781]  eta: 0:00:04  lr: 0.000044  loss: 2.4221 (2.6092)  time: 0.1304  data: 0.0075  max mem: 4873\n",
            "Epoch: [5]  [760/781]  eta: 0:00:02  lr: 0.000044  loss: 2.3993 (2.6093)  time: 0.1338  data: 0.0120  max mem: 4873\n",
            "Epoch: [5]  [770/781]  eta: 0:00:01  lr: 0.000044  loss: 2.4264 (2.6123)  time: 0.1302  data: 0.0083  max mem: 4873\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000044  loss: 2.4335 (2.6109)  time: 0.1249  data: 0.0028  max mem: 4873\n",
            "Epoch: [5] Total time: 0:01:42 (0.1313 s / it)\n",
            "Averaged stats: lr: 0.000044  loss: 2.4335 (2.6109)\n",
            "Test:  [ 0/53]  eta: 0:00:42  loss: 0.9582 (0.9582)  acc1: 77.0833 (77.0833)  acc5: 93.2292 (93.2292)  time: 0.7953  data: 0.7643  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2156 (1.1447)  acc1: 72.9167 (72.4905)  acc5: 92.1875 (91.6667)  time: 0.1696  data: 0.1389  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2534 (1.2155)  acc1: 71.8750 (71.8254)  acc5: 90.6250 (90.2530)  time: 0.1246  data: 0.0939  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3536 (1.2634)  acc1: 68.7500 (71.0349)  acc5: 89.0625 (89.8858)  time: 0.1267  data: 0.0960  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4445 (1.3447)  acc1: 66.1458 (69.1311)  acc5: 88.0208 (88.8847)  time: 0.1303  data: 0.0996  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3283 (1.3323)  acc1: 67.1875 (69.4036)  acc5: 89.0625 (88.9502)  time: 0.1297  data: 0.0990  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4055 (1.3397)  acc1: 66.1458 (69.2300)  acc5: 89.0625 (89.0200)  time: 0.1080  data: 0.0783  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1335 s / it)\n",
            "* Acc@1 69.230 Acc@5 89.020 loss 1.340\n",
            "Accuracy of the network on the 10000 test images: 69.2%\n",
            "Max accuracy: 69.23%\n",
            "Epoch: [6]  [  0/781]  eta: 0:11:09  lr: 0.000036  loss: 2.4959 (2.4959)  time: 0.8567  data: 0.7196  max mem: 4873\n",
            "Epoch: [6]  [ 10/781]  eta: 0:02:30  lr: 0.000036  loss: 2.2863 (2.2881)  time: 0.1950  data: 0.0708  max mem: 4873\n",
            "Epoch: [6]  [ 20/781]  eta: 0:02:09  lr: 0.000036  loss: 2.2948 (2.3957)  time: 0.1361  data: 0.0128  max mem: 4873\n",
            "Epoch: [6]  [ 30/781]  eta: 0:01:59  lr: 0.000036  loss: 2.4329 (2.4174)  time: 0.1399  data: 0.0169  max mem: 4873\n",
            "Epoch: [6]  [ 40/781]  eta: 0:01:56  lr: 0.000036  loss: 2.4504 (2.5520)  time: 0.1422  data: 0.0198  max mem: 4873\n",
            "Epoch: [6]  [ 50/781]  eta: 0:01:50  lr: 0.000036  loss: 2.4504 (2.5660)  time: 0.1393  data: 0.0161  max mem: 4873\n",
            "Epoch: [6]  [ 60/781]  eta: 0:01:49  lr: 0.000036  loss: 2.3890 (2.5413)  time: 0.1406  data: 0.0170  max mem: 4873\n",
            "Epoch: [6]  [ 70/781]  eta: 0:01:46  lr: 0.000036  loss: 2.3890 (2.5440)  time: 0.1430  data: 0.0201  max mem: 4873\n",
            "Epoch: [6]  [ 80/781]  eta: 0:01:43  lr: 0.000036  loss: 2.4039 (2.5366)  time: 0.1383  data: 0.0153  max mem: 4873\n",
            "Epoch: [6]  [ 90/781]  eta: 0:01:41  lr: 0.000036  loss: 2.3676 (2.5473)  time: 0.1396  data: 0.0164  max mem: 4873\n",
            "Epoch: [6]  [100/781]  eta: 0:01:39  lr: 0.000036  loss: 2.3302 (2.5440)  time: 0.1414  data: 0.0189  max mem: 4873\n",
            "Epoch: [6]  [110/781]  eta: 0:01:37  lr: 0.000036  loss: 2.3302 (2.5362)  time: 0.1365  data: 0.0141  max mem: 4873\n",
            "Epoch: [6]  [120/781]  eta: 0:01:35  lr: 0.000036  loss: 2.3895 (2.5287)  time: 0.1330  data: 0.0102  max mem: 4873\n",
            "Epoch: [6]  [130/781]  eta: 0:01:33  lr: 0.000036  loss: 2.3730 (2.5166)  time: 0.1371  data: 0.0139  max mem: 4873\n",
            "Epoch: [6]  [140/781]  eta: 0:01:31  lr: 0.000036  loss: 2.3690 (2.5206)  time: 0.1374  data: 0.0138  max mem: 4873\n",
            "Epoch: [6]  [150/781]  eta: 0:01:30  lr: 0.000036  loss: 2.4654 (2.5309)  time: 0.1368  data: 0.0127  max mem: 4873\n",
            "Epoch: [6]  [160/781]  eta: 0:01:28  lr: 0.000036  loss: 2.4449 (2.5308)  time: 0.1397  data: 0.0155  max mem: 4873\n",
            "Epoch: [6]  [170/781]  eta: 0:01:27  lr: 0.000036  loss: 2.3220 (2.5197)  time: 0.1405  data: 0.0171  max mem: 4873\n",
            "Epoch: [6]  [180/781]  eta: 0:01:25  lr: 0.000036  loss: 2.3179 (2.5268)  time: 0.1398  data: 0.0163  max mem: 4873\n",
            "Epoch: [6]  [190/781]  eta: 0:01:24  lr: 0.000036  loss: 2.4017 (2.5262)  time: 0.1403  data: 0.0169  max mem: 4873\n",
            "Epoch: [6]  [200/781]  eta: 0:01:22  lr: 0.000036  loss: 2.4402 (2.5379)  time: 0.1422  data: 0.0197  max mem: 4873\n",
            "Epoch: [6]  [210/781]  eta: 0:01:21  lr: 0.000036  loss: 2.4538 (2.5381)  time: 0.1431  data: 0.0210  max mem: 4873\n",
            "Epoch: [6]  [220/781]  eta: 0:01:19  lr: 0.000036  loss: 2.4665 (2.5446)  time: 0.1396  data: 0.0177  max mem: 4873\n",
            "Epoch: [6]  [230/781]  eta: 0:01:18  lr: 0.000036  loss: 2.3640 (2.5417)  time: 0.1384  data: 0.0154  max mem: 4873\n",
            "Epoch: [6]  [240/781]  eta: 0:01:16  lr: 0.000036  loss: 2.3782 (2.5428)  time: 0.1419  data: 0.0179  max mem: 4873\n",
            "Epoch: [6]  [250/781]  eta: 0:01:15  lr: 0.000036  loss: 2.3753 (2.5388)  time: 0.1464  data: 0.0237  max mem: 4873\n",
            "Epoch: [6]  [260/781]  eta: 0:01:14  lr: 0.000036  loss: 2.3705 (2.5460)  time: 0.1448  data: 0.0217  max mem: 4873\n",
            "Epoch: [6]  [270/781]  eta: 0:01:12  lr: 0.000036  loss: 2.3705 (2.5404)  time: 0.1413  data: 0.0178  max mem: 4873\n",
            "Epoch: [6]  [280/781]  eta: 0:01:11  lr: 0.000036  loss: 2.3195 (2.5326)  time: 0.1380  data: 0.0139  max mem: 4873\n",
            "Epoch: [6]  [290/781]  eta: 0:01:09  lr: 0.000036  loss: 2.3609 (2.5386)  time: 0.1363  data: 0.0121  max mem: 4873\n",
            "Epoch: [6]  [300/781]  eta: 0:01:08  lr: 0.000036  loss: 2.4138 (2.5436)  time: 0.1407  data: 0.0181  max mem: 4873\n",
            "Epoch: [6]  [310/781]  eta: 0:01:06  lr: 0.000036  loss: 2.2640 (2.5403)  time: 0.1389  data: 0.0164  max mem: 4873\n",
            "Epoch: [6]  [320/781]  eta: 0:01:05  lr: 0.000036  loss: 2.3350 (2.5394)  time: 0.1355  data: 0.0130  max mem: 4873\n",
            "Epoch: [6]  [330/781]  eta: 0:01:03  lr: 0.000036  loss: 2.4267 (2.5375)  time: 0.1398  data: 0.0169  max mem: 4873\n",
            "Epoch: [6]  [340/781]  eta: 0:01:02  lr: 0.000036  loss: 2.4703 (2.5372)  time: 0.1406  data: 0.0171  max mem: 4873\n",
            "Epoch: [6]  [350/781]  eta: 0:01:00  lr: 0.000036  loss: 2.4703 (2.5383)  time: 0.1384  data: 0.0152  max mem: 4873\n",
            "Epoch: [6]  [360/781]  eta: 0:00:59  lr: 0.000036  loss: 2.3840 (2.5409)  time: 0.1368  data: 0.0148  max mem: 4873\n",
            "Epoch: [6]  [370/781]  eta: 0:00:57  lr: 0.000036  loss: 2.3688 (2.5395)  time: 0.1337  data: 0.0109  max mem: 4873\n",
            "Epoch: [6]  [380/781]  eta: 0:00:56  lr: 0.000036  loss: 2.3232 (2.5349)  time: 0.1369  data: 0.0143  max mem: 4873\n",
            "Epoch: [6]  [390/781]  eta: 0:00:55  lr: 0.000036  loss: 2.3491 (2.5364)  time: 0.1421  data: 0.0208  max mem: 4873\n",
            "Epoch: [6]  [400/781]  eta: 0:00:53  lr: 0.000036  loss: 2.4478 (2.5380)  time: 0.1399  data: 0.0186  max mem: 4873\n",
            "Epoch: [6]  [410/781]  eta: 0:00:52  lr: 0.000036  loss: 2.3960 (2.5358)  time: 0.1382  data: 0.0164  max mem: 4873\n",
            "Epoch: [6]  [420/781]  eta: 0:00:50  lr: 0.000036  loss: 2.4199 (2.5410)  time: 0.1373  data: 0.0150  max mem: 4873\n",
            "Epoch: [6]  [430/781]  eta: 0:00:49  lr: 0.000036  loss: 2.4344 (2.5450)  time: 0.1366  data: 0.0141  max mem: 4873\n",
            "Epoch: [6]  [440/781]  eta: 0:00:48  lr: 0.000036  loss: 2.4046 (2.5476)  time: 0.1419  data: 0.0193  max mem: 4873\n",
            "Epoch: [6]  [450/781]  eta: 0:00:46  lr: 0.000036  loss: 2.3188 (2.5436)  time: 0.1437  data: 0.0215  max mem: 4873\n",
            "Epoch: [6]  [460/781]  eta: 0:00:45  lr: 0.000036  loss: 2.2561 (2.5400)  time: 0.1394  data: 0.0172  max mem: 4873\n",
            "Epoch: [6]  [470/781]  eta: 0:00:43  lr: 0.000036  loss: 2.3462 (2.5434)  time: 0.1365  data: 0.0144  max mem: 4873\n",
            "Epoch: [6]  [480/781]  eta: 0:00:42  lr: 0.000036  loss: 2.4366 (2.5453)  time: 0.1361  data: 0.0140  max mem: 4873\n",
            "Epoch: [6]  [490/781]  eta: 0:00:40  lr: 0.000036  loss: 2.4683 (2.5475)  time: 0.1375  data: 0.0150  max mem: 4873\n",
            "Epoch: [6]  [500/781]  eta: 0:00:39  lr: 0.000036  loss: 2.3288 (2.5439)  time: 0.1436  data: 0.0202  max mem: 4873\n",
            "Epoch: [6]  [510/781]  eta: 0:00:38  lr: 0.000036  loss: 2.3109 (2.5409)  time: 0.1459  data: 0.0202  max mem: 4873\n",
            "Epoch: [6]  [520/781]  eta: 0:00:36  lr: 0.000036  loss: 2.3896 (2.5468)  time: 0.1421  data: 0.0172  max mem: 4873\n",
            "Epoch: [6]  [530/781]  eta: 0:00:35  lr: 0.000036  loss: 2.3909 (2.5462)  time: 0.1380  data: 0.0144  max mem: 4873\n",
            "Epoch: [6]  [540/781]  eta: 0:00:33  lr: 0.000036  loss: 2.3171 (2.5437)  time: 0.1343  data: 0.0107  max mem: 4873\n",
            "Epoch: [6]  [550/781]  eta: 0:00:32  lr: 0.000036  loss: 2.3230 (2.5404)  time: 0.1340  data: 0.0113  max mem: 4873\n",
            "Epoch: [6]  [560/781]  eta: 0:00:31  lr: 0.000036  loss: 2.3792 (2.5379)  time: 0.1353  data: 0.0118  max mem: 4873\n",
            "Epoch: [6]  [570/781]  eta: 0:00:29  lr: 0.000036  loss: 2.3451 (2.5381)  time: 0.1308  data: 0.0078  max mem: 4873\n",
            "Epoch: [6]  [580/781]  eta: 0:00:28  lr: 0.000036  loss: 2.3443 (2.5390)  time: 0.1296  data: 0.0078  max mem: 4873\n",
            "Epoch: [6]  [590/781]  eta: 0:00:26  lr: 0.000036  loss: 2.3881 (2.5459)  time: 0.1380  data: 0.0158  max mem: 4873\n",
            "Epoch: [6]  [600/781]  eta: 0:00:25  lr: 0.000036  loss: 2.3387 (2.5430)  time: 0.1445  data: 0.0198  max mem: 4873\n",
            "Epoch: [6]  [610/781]  eta: 0:00:23  lr: 0.000036  loss: 2.3224 (2.5478)  time: 0.1452  data: 0.0206  max mem: 4873\n",
            "Epoch: [6]  [620/781]  eta: 0:00:22  lr: 0.000036  loss: 2.3020 (2.5435)  time: 0.1455  data: 0.0235  max mem: 4873\n",
            "Epoch: [6]  [630/781]  eta: 0:00:21  lr: 0.000036  loss: 2.3020 (2.5445)  time: 0.1408  data: 0.0187  max mem: 4873\n",
            "Epoch: [6]  [640/781]  eta: 0:00:19  lr: 0.000036  loss: 2.3108 (2.5409)  time: 0.1418  data: 0.0188  max mem: 4873\n",
            "Epoch: [6]  [650/781]  eta: 0:00:18  lr: 0.000036  loss: 2.2677 (2.5435)  time: 0.1446  data: 0.0202  max mem: 4873\n",
            "Epoch: [6]  [660/781]  eta: 0:00:16  lr: 0.000036  loss: 2.2601 (2.5385)  time: 0.1398  data: 0.0162  max mem: 4873\n",
            "Epoch: [6]  [670/781]  eta: 0:00:15  lr: 0.000036  loss: 2.2521 (2.5377)  time: 0.1382  data: 0.0161  max mem: 4873\n",
            "Epoch: [6]  [680/781]  eta: 0:00:14  lr: 0.000036  loss: 2.3414 (2.5368)  time: 0.1338  data: 0.0106  max mem: 4873\n",
            "Epoch: [6]  [690/781]  eta: 0:00:12  lr: 0.000036  loss: 2.3656 (2.5389)  time: 0.1383  data: 0.0139  max mem: 4873\n",
            "Epoch: [6]  [700/781]  eta: 0:00:11  lr: 0.000036  loss: 2.3873 (2.5388)  time: 0.1399  data: 0.0163  max mem: 4873\n",
            "Epoch: [6]  [710/781]  eta: 0:00:09  lr: 0.000036  loss: 2.3473 (2.5375)  time: 0.1366  data: 0.0141  max mem: 4873\n",
            "Epoch: [6]  [720/781]  eta: 0:00:08  lr: 0.000036  loss: 2.3377 (2.5348)  time: 0.1371  data: 0.0145  max mem: 4873\n",
            "Epoch: [6]  [730/781]  eta: 0:00:07  lr: 0.000036  loss: 2.2554 (2.5343)  time: 0.1363  data: 0.0135  max mem: 4873\n",
            "Epoch: [6]  [740/781]  eta: 0:00:05  lr: 0.000036  loss: 2.3800 (2.5343)  time: 0.1368  data: 0.0143  max mem: 4873\n",
            "Epoch: [6]  [750/781]  eta: 0:00:04  lr: 0.000036  loss: 2.3962 (2.5352)  time: 0.1368  data: 0.0144  max mem: 4873\n",
            "Epoch: [6]  [760/781]  eta: 0:00:02  lr: 0.000036  loss: 2.3548 (2.5333)  time: 0.1375  data: 0.0151  max mem: 4873\n",
            "Epoch: [6]  [770/781]  eta: 0:00:01  lr: 0.000036  loss: 2.3003 (2.5310)  time: 0.1364  data: 0.0132  max mem: 4873\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000036  loss: 2.3353 (2.5289)  time: 0.1349  data: 0.0117  max mem: 4873\n",
            "Epoch: [6] Total time: 0:01:49 (0.1399 s / it)\n",
            "Averaged stats: lr: 0.000036  loss: 2.3353 (2.5289)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.8190 (0.8190)  acc1: 82.8125 (82.8125)  acc5: 94.2708 (94.2708)  time: 0.8313  data: 0.8005  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1808 (1.0502)  acc1: 75.0000 (74.9527)  acc5: 93.7500 (92.5663)  time: 0.1673  data: 0.1366  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1849 (1.1378)  acc1: 70.8333 (73.0903)  acc5: 90.6250 (91.0714)  time: 0.1255  data: 0.0948  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2762 (1.1868)  acc1: 69.7917 (72.4798)  acc5: 89.5833 (90.6082)  time: 0.1317  data: 0.1010  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3685 (1.2468)  acc1: 69.7917 (71.2525)  acc5: 88.5417 (89.8628)  time: 0.1316  data: 0.1009  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3145 (1.2474)  acc1: 69.2708 (71.0989)  acc5: 88.5417 (89.8182)  time: 0.1283  data: 0.0976  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3421 (1.2570)  acc1: 69.2708 (70.9200)  acc5: 89.5833 (89.8800)  time: 0.1078  data: 0.0781  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1345 s / it)\n",
            "* Acc@1 70.920 Acc@5 89.880 loss 1.257\n",
            "Accuracy of the network on the 10000 test images: 70.9%\n",
            "Max accuracy: 70.92%\n",
            "Epoch: [7]  [  0/781]  eta: 0:11:28  lr: 0.000028  loss: 2.1261 (2.1261)  time: 0.8818  data: 0.7440  max mem: 4873\n",
            "Epoch: [7]  [ 10/781]  eta: 0:02:30  lr: 0.000028  loss: 2.2749 (2.4149)  time: 0.1955  data: 0.0712  max mem: 4873\n",
            "Epoch: [7]  [ 20/781]  eta: 0:02:08  lr: 0.000028  loss: 2.2934 (2.4573)  time: 0.1326  data: 0.0087  max mem: 4873\n",
            "Epoch: [7]  [ 30/781]  eta: 0:02:00  lr: 0.000028  loss: 2.3506 (2.4899)  time: 0.1414  data: 0.0167  max mem: 4873\n",
            "Epoch: [7]  [ 40/781]  eta: 0:01:52  lr: 0.000028  loss: 2.3817 (2.4879)  time: 0.1337  data: 0.0107  max mem: 4873\n",
            "Epoch: [7]  [ 50/781]  eta: 0:01:46  lr: 0.000028  loss: 2.3874 (2.5269)  time: 0.1234  data: 0.0017  max mem: 4873\n",
            "Epoch: [7]  [ 60/781]  eta: 0:01:43  lr: 0.000028  loss: 2.4002 (2.5360)  time: 0.1256  data: 0.0039  max mem: 4873\n",
            "Epoch: [7]  [ 70/781]  eta: 0:01:40  lr: 0.000028  loss: 2.3093 (2.5149)  time: 0.1311  data: 0.0096  max mem: 4873\n",
            "Epoch: [7]  [ 80/781]  eta: 0:01:38  lr: 0.000028  loss: 2.2736 (2.5340)  time: 0.1346  data: 0.0133  max mem: 4873\n",
            "Epoch: [7]  [ 90/781]  eta: 0:01:37  lr: 0.000028  loss: 2.3747 (2.5486)  time: 0.1359  data: 0.0144  max mem: 4873\n",
            "Epoch: [7]  [100/781]  eta: 0:01:34  lr: 0.000028  loss: 2.2995 (2.5416)  time: 0.1297  data: 0.0082  max mem: 4873\n",
            "Epoch: [7]  [110/781]  eta: 0:01:33  lr: 0.000028  loss: 2.2995 (2.5464)  time: 0.1319  data: 0.0096  max mem: 4873\n",
            "Epoch: [7]  [120/781]  eta: 0:01:31  lr: 0.000028  loss: 2.2946 (2.5319)  time: 0.1336  data: 0.0094  max mem: 4873\n",
            "Epoch: [7]  [130/781]  eta: 0:01:29  lr: 0.000028  loss: 2.2494 (2.5193)  time: 0.1270  data: 0.0016  max mem: 4873\n",
            "Epoch: [7]  [140/781]  eta: 0:01:27  lr: 0.000028  loss: 2.2476 (2.5035)  time: 0.1271  data: 0.0031  max mem: 4873\n",
            "Epoch: [7]  [150/781]  eta: 0:01:25  lr: 0.000028  loss: 2.2476 (2.5014)  time: 0.1240  data: 0.0021  max mem: 4873\n",
            "Epoch: [7]  [160/781]  eta: 0:01:23  lr: 0.000028  loss: 2.2728 (2.5106)  time: 0.1234  data: 0.0015  max mem: 4873\n",
            "Epoch: [7]  [170/781]  eta: 0:01:22  lr: 0.000028  loss: 2.3044 (2.5201)  time: 0.1274  data: 0.0044  max mem: 4873\n",
            "Epoch: [7]  [180/781]  eta: 0:01:20  lr: 0.000028  loss: 2.3466 (2.5240)  time: 0.1273  data: 0.0039  max mem: 4873\n",
            "Epoch: [7]  [190/781]  eta: 0:01:19  lr: 0.000028  loss: 2.2815 (2.5154)  time: 0.1289  data: 0.0063  max mem: 4873\n",
            "Epoch: [7]  [200/781]  eta: 0:01:17  lr: 0.000028  loss: 2.2506 (2.5102)  time: 0.1292  data: 0.0058  max mem: 4873\n",
            "Epoch: [7]  [210/781]  eta: 0:01:16  lr: 0.000028  loss: 2.2446 (2.4973)  time: 0.1267  data: 0.0018  max mem: 4873\n",
            "Epoch: [7]  [220/781]  eta: 0:01:14  lr: 0.000028  loss: 2.2356 (2.4859)  time: 0.1294  data: 0.0039  max mem: 4873\n",
            "Epoch: [7]  [230/781]  eta: 0:01:13  lr: 0.000028  loss: 2.2462 (2.4835)  time: 0.1281  data: 0.0043  max mem: 4873\n",
            "Epoch: [7]  [240/781]  eta: 0:01:11  lr: 0.000028  loss: 2.2487 (2.4756)  time: 0.1258  data: 0.0040  max mem: 4873\n",
            "Epoch: [7]  [250/781]  eta: 0:01:10  lr: 0.000028  loss: 2.2466 (2.4685)  time: 0.1263  data: 0.0042  max mem: 4873\n",
            "Epoch: [7]  [260/781]  eta: 0:01:08  lr: 0.000028  loss: 2.2639 (2.4621)  time: 0.1281  data: 0.0060  max mem: 4873\n",
            "Epoch: [7]  [270/781]  eta: 0:01:07  lr: 0.000028  loss: 2.2943 (2.4633)  time: 0.1284  data: 0.0066  max mem: 4873\n",
            "Epoch: [7]  [280/781]  eta: 0:01:06  lr: 0.000028  loss: 2.2997 (2.4701)  time: 0.1267  data: 0.0046  max mem: 4873\n",
            "Epoch: [7]  [290/781]  eta: 0:01:04  lr: 0.000028  loss: 2.2997 (2.4673)  time: 0.1242  data: 0.0020  max mem: 4873\n",
            "Epoch: [7]  [300/781]  eta: 0:01:03  lr: 0.000028  loss: 2.2640 (2.4650)  time: 0.1292  data: 0.0069  max mem: 4873\n",
            "Epoch: [7]  [310/781]  eta: 0:01:01  lr: 0.000028  loss: 2.2482 (2.4624)  time: 0.1325  data: 0.0083  max mem: 4873\n",
            "Epoch: [7]  [320/781]  eta: 0:01:00  lr: 0.000028  loss: 2.2949 (2.4590)  time: 0.1288  data: 0.0032  max mem: 4873\n",
            "Epoch: [7]  [330/781]  eta: 0:00:59  lr: 0.000028  loss: 2.3282 (2.4592)  time: 0.1327  data: 0.0086  max mem: 4873\n",
            "Epoch: [7]  [340/781]  eta: 0:00:57  lr: 0.000028  loss: 2.2919 (2.4556)  time: 0.1324  data: 0.0102  max mem: 4873\n",
            "Epoch: [7]  [350/781]  eta: 0:00:56  lr: 0.000028  loss: 2.2610 (2.4486)  time: 0.1315  data: 0.0099  max mem: 4873\n",
            "Epoch: [7]  [360/781]  eta: 0:00:55  lr: 0.000028  loss: 2.2426 (2.4442)  time: 0.1317  data: 0.0101  max mem: 4873\n",
            "Epoch: [7]  [370/781]  eta: 0:00:54  lr: 0.000028  loss: 2.2927 (2.4411)  time: 0.1283  data: 0.0067  max mem: 4873\n",
            "Epoch: [7]  [380/781]  eta: 0:00:52  lr: 0.000028  loss: 2.3488 (2.4427)  time: 0.1279  data: 0.0063  max mem: 4873\n",
            "Epoch: [7]  [390/781]  eta: 0:00:51  lr: 0.000028  loss: 2.3191 (2.4389)  time: 0.1256  data: 0.0042  max mem: 4873\n",
            "Epoch: [7]  [400/781]  eta: 0:00:49  lr: 0.000028  loss: 2.3367 (2.4404)  time: 0.1261  data: 0.0035  max mem: 4873\n",
            "Epoch: [7]  [410/781]  eta: 0:00:48  lr: 0.000028  loss: 2.3367 (2.4428)  time: 0.1376  data: 0.0111  max mem: 4873\n",
            "Epoch: [7]  [420/781]  eta: 0:00:47  lr: 0.000028  loss: 2.3107 (2.4428)  time: 0.1388  data: 0.0135  max mem: 4873\n",
            "Epoch: [7]  [430/781]  eta: 0:00:46  lr: 0.000028  loss: 2.2926 (2.4429)  time: 0.1288  data: 0.0072  max mem: 4873\n",
            "Epoch: [7]  [440/781]  eta: 0:00:44  lr: 0.000028  loss: 2.3317 (2.4472)  time: 0.1242  data: 0.0027  max mem: 4873\n",
            "Epoch: [7]  [450/781]  eta: 0:00:43  lr: 0.000028  loss: 2.3585 (2.4493)  time: 0.1254  data: 0.0020  max mem: 4873\n",
            "Epoch: [7]  [460/781]  eta: 0:00:42  lr: 0.000028  loss: 2.3138 (2.4510)  time: 0.1265  data: 0.0019  max mem: 4873\n",
            "Epoch: [7]  [470/781]  eta: 0:00:40  lr: 0.000028  loss: 2.3232 (2.4547)  time: 0.1282  data: 0.0040  max mem: 4873\n",
            "Epoch: [7]  [480/781]  eta: 0:00:39  lr: 0.000028  loss: 2.2955 (2.4511)  time: 0.1285  data: 0.0054  max mem: 4873\n",
            "Epoch: [7]  [490/781]  eta: 0:00:38  lr: 0.000028  loss: 2.3250 (2.4533)  time: 0.1273  data: 0.0040  max mem: 4873\n",
            "Epoch: [7]  [500/781]  eta: 0:00:36  lr: 0.000028  loss: 2.4177 (2.4560)  time: 0.1275  data: 0.0031  max mem: 4873\n",
            "Epoch: [7]  [510/781]  eta: 0:00:35  lr: 0.000028  loss: 2.4141 (2.4552)  time: 0.1264  data: 0.0018  max mem: 4873\n",
            "Epoch: [7]  [520/781]  eta: 0:00:34  lr: 0.000028  loss: 2.2020 (2.4504)  time: 0.1320  data: 0.0082  max mem: 4873\n",
            "Epoch: [7]  [530/781]  eta: 0:00:32  lr: 0.000028  loss: 2.2558 (2.4534)  time: 0.1361  data: 0.0130  max mem: 4873\n",
            "Epoch: [7]  [540/781]  eta: 0:00:31  lr: 0.000028  loss: 2.3242 (2.4547)  time: 0.1339  data: 0.0107  max mem: 4873\n",
            "Epoch: [7]  [550/781]  eta: 0:00:30  lr: 0.000028  loss: 2.2834 (2.4505)  time: 0.1340  data: 0.0099  max mem: 4873\n",
            "Epoch: [7]  [560/781]  eta: 0:00:28  lr: 0.000028  loss: 2.2834 (2.4526)  time: 0.1307  data: 0.0071  max mem: 4873\n",
            "Epoch: [7]  [570/781]  eta: 0:00:27  lr: 0.000028  loss: 2.3212 (2.4527)  time: 0.1281  data: 0.0053  max mem: 4873\n",
            "Epoch: [7]  [580/781]  eta: 0:00:26  lr: 0.000028  loss: 2.3336 (2.4566)  time: 0.1297  data: 0.0062  max mem: 4873\n",
            "Epoch: [7]  [590/781]  eta: 0:00:25  lr: 0.000028  loss: 2.3336 (2.4581)  time: 0.1341  data: 0.0109  max mem: 4873\n",
            "Epoch: [7]  [600/781]  eta: 0:00:23  lr: 0.000028  loss: 2.2600 (2.4559)  time: 0.1336  data: 0.0103  max mem: 4873\n",
            "Epoch: [7]  [610/781]  eta: 0:00:22  lr: 0.000028  loss: 2.2452 (2.4572)  time: 0.1350  data: 0.0118  max mem: 4873\n",
            "Epoch: [7]  [620/781]  eta: 0:00:21  lr: 0.000028  loss: 2.2913 (2.4551)  time: 0.1357  data: 0.0129  max mem: 4873\n",
            "Epoch: [7]  [630/781]  eta: 0:00:19  lr: 0.000028  loss: 2.2228 (2.4538)  time: 0.1282  data: 0.0059  max mem: 4873\n",
            "Epoch: [7]  [640/781]  eta: 0:00:18  lr: 0.000028  loss: 2.2228 (2.4525)  time: 0.1252  data: 0.0029  max mem: 4873\n",
            "Epoch: [7]  [650/781]  eta: 0:00:17  lr: 0.000028  loss: 2.2803 (2.4497)  time: 0.1230  data: 0.0005  max mem: 4873\n",
            "Epoch: [7]  [660/781]  eta: 0:00:15  lr: 0.000028  loss: 2.2803 (2.4485)  time: 0.1247  data: 0.0026  max mem: 4873\n",
            "Epoch: [7]  [670/781]  eta: 0:00:14  lr: 0.000028  loss: 2.2762 (2.4478)  time: 0.1285  data: 0.0065  max mem: 4873\n",
            "Epoch: [7]  [680/781]  eta: 0:00:13  lr: 0.000028  loss: 2.2939 (2.4475)  time: 0.1267  data: 0.0042  max mem: 4873\n",
            "Epoch: [7]  [690/781]  eta: 0:00:11  lr: 0.000028  loss: 2.2968 (2.4443)  time: 0.1267  data: 0.0032  max mem: 4873\n",
            "Epoch: [7]  [700/781]  eta: 0:00:10  lr: 0.000028  loss: 2.2357 (2.4422)  time: 0.1373  data: 0.0129  max mem: 4873\n",
            "Epoch: [7]  [710/781]  eta: 0:00:09  lr: 0.000028  loss: 2.2760 (2.4448)  time: 0.1348  data: 0.0110  max mem: 4873\n",
            "Epoch: [7]  [720/781]  eta: 0:00:07  lr: 0.000028  loss: 2.2384 (2.4438)  time: 0.1239  data: 0.0013  max mem: 4873\n",
            "Epoch: [7]  [730/781]  eta: 0:00:06  lr: 0.000028  loss: 2.2083 (2.4416)  time: 0.1266  data: 0.0035  max mem: 4873\n",
            "Epoch: [7]  [740/781]  eta: 0:00:05  lr: 0.000028  loss: 2.2779 (2.4399)  time: 0.1274  data: 0.0035  max mem: 4873\n",
            "Epoch: [7]  [750/781]  eta: 0:00:04  lr: 0.000028  loss: 2.2805 (2.4399)  time: 0.1229  data: 0.0003  max mem: 4873\n",
            "Epoch: [7]  [760/781]  eta: 0:00:02  lr: 0.000028  loss: 2.2596 (2.4390)  time: 0.1223  data: 0.0008  max mem: 4873\n",
            "Epoch: [7]  [770/781]  eta: 0:00:01  lr: 0.000028  loss: 2.3054 (2.4385)  time: 0.1246  data: 0.0008  max mem: 4873\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000028  loss: 2.2746 (2.4377)  time: 0.1243  data: 0.0006  max mem: 4873\n",
            "Epoch: [7] Total time: 0:01:41 (0.1302 s / it)\n",
            "Averaged stats: lr: 0.000028  loss: 2.2746 (2.4377)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.9265 (0.9265)  acc1: 77.0833 (77.0833)  acc5: 94.2708 (94.2708)  time: 0.8370  data: 0.8060  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1849 (1.0793)  acc1: 73.9583 (74.4792)  acc5: 92.1875 (91.9981)  time: 0.1828  data: 0.1520  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2011 (1.1303)  acc1: 71.8750 (73.5119)  acc5: 91.1458 (91.2946)  time: 0.1321  data: 0.1014  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2669 (1.1858)  acc1: 69.2708 (72.6142)  acc5: 90.6250 (90.9106)  time: 0.1270  data: 0.0963  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3060 (1.2267)  acc1: 70.8333 (71.7099)  acc5: 89.5833 (90.4218)  time: 0.1249  data: 0.0942  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1918 (1.2134)  acc1: 71.8750 (72.0180)  acc5: 89.5833 (90.4820)  time: 0.1274  data: 0.0968  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2486 (1.2163)  acc1: 70.8333 (71.8900)  acc5: 90.1042 (90.5300)  time: 0.1094  data: 0.0797  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1356 s / it)\n",
            "* Acc@1 71.890 Acc@5 90.530 loss 1.216\n",
            "Accuracy of the network on the 10000 test images: 71.9%\n",
            "Max accuracy: 71.89%\n",
            "Epoch: [8]  [  0/781]  eta: 0:11:22  lr: 0.000021  loss: 2.1386 (2.1386)  time: 0.8736  data: 0.7280  max mem: 4873\n",
            "Epoch: [8]  [ 10/781]  eta: 0:02:38  lr: 0.000021  loss: 2.4295 (2.5680)  time: 0.2056  data: 0.0804  max mem: 4873\n",
            "Epoch: [8]  [ 20/781]  eta: 0:02:13  lr: 0.000021  loss: 2.3277 (2.5344)  time: 0.1406  data: 0.0172  max mem: 4873\n",
            "Epoch: [8]  [ 30/781]  eta: 0:02:01  lr: 0.000021  loss: 2.2665 (2.4565)  time: 0.1377  data: 0.0143  max mem: 4873\n",
            "Epoch: [8]  [ 40/781]  eta: 0:01:56  lr: 0.000021  loss: 2.1599 (2.4272)  time: 0.1385  data: 0.0146  max mem: 4873\n",
            "Epoch: [8]  [ 50/781]  eta: 0:01:53  lr: 0.000021  loss: 2.1913 (2.4245)  time: 0.1459  data: 0.0212  max mem: 4873\n",
            "Epoch: [8]  [ 60/781]  eta: 0:01:50  lr: 0.000021  loss: 2.3441 (2.4784)  time: 0.1460  data: 0.0219  max mem: 4873\n",
            "Epoch: [8]  [ 70/781]  eta: 0:01:47  lr: 0.000021  loss: 2.3638 (2.5010)  time: 0.1393  data: 0.0165  max mem: 4873\n",
            "Epoch: [8]  [ 80/781]  eta: 0:01:44  lr: 0.000021  loss: 2.3356 (2.5197)  time: 0.1358  data: 0.0137  max mem: 4873\n",
            "Epoch: [8]  [ 90/781]  eta: 0:01:42  lr: 0.000021  loss: 2.3356 (2.5165)  time: 0.1380  data: 0.0159  max mem: 4873\n",
            "Epoch: [8]  [100/781]  eta: 0:01:40  lr: 0.000021  loss: 2.3025 (2.4982)  time: 0.1377  data: 0.0160  max mem: 4873\n",
            "Epoch: [8]  [110/781]  eta: 0:01:38  lr: 0.000021  loss: 2.1779 (2.4796)  time: 0.1384  data: 0.0164  max mem: 4873\n",
            "Epoch: [8]  [120/781]  eta: 0:01:36  lr: 0.000021  loss: 2.1779 (2.4666)  time: 0.1383  data: 0.0154  max mem: 4873\n",
            "Epoch: [8]  [130/781]  eta: 0:01:35  lr: 0.000021  loss: 2.2802 (2.4525)  time: 0.1469  data: 0.0231  max mem: 4873\n",
            "Epoch: [8]  [140/781]  eta: 0:01:34  lr: 0.000021  loss: 2.2802 (2.4507)  time: 0.1558  data: 0.0220  max mem: 4873\n",
            "Epoch: [8]  [150/781]  eta: 0:01:31  lr: 0.000021  loss: 2.2701 (2.4549)  time: 0.1393  data: 0.0056  max mem: 4873\n",
            "Epoch: [8]  [160/781]  eta: 0:01:29  lr: 0.000021  loss: 2.2482 (2.4456)  time: 0.1279  data: 0.0047  max mem: 4873\n",
            "Epoch: [8]  [170/781]  eta: 0:01:28  lr: 0.000021  loss: 2.2660 (2.4463)  time: 0.1338  data: 0.0108  max mem: 4873\n",
            "Epoch: [8]  [180/781]  eta: 0:01:26  lr: 0.000021  loss: 2.2269 (2.4312)  time: 0.1337  data: 0.0113  max mem: 4873\n",
            "Epoch: [8]  [190/781]  eta: 0:01:24  lr: 0.000021  loss: 2.1923 (2.4308)  time: 0.1344  data: 0.0123  max mem: 4873\n",
            "Epoch: [8]  [200/781]  eta: 0:01:22  lr: 0.000021  loss: 2.2108 (2.4277)  time: 0.1357  data: 0.0129  max mem: 4873\n",
            "Epoch: [8]  [210/781]  eta: 0:01:21  lr: 0.000021  loss: 2.2006 (2.4220)  time: 0.1377  data: 0.0143  max mem: 4873\n",
            "Epoch: [8]  [220/781]  eta: 0:01:19  lr: 0.000021  loss: 2.2094 (2.4126)  time: 0.1372  data: 0.0120  max mem: 4873\n",
            "Epoch: [8]  [230/781]  eta: 0:01:18  lr: 0.000021  loss: 2.2094 (2.4104)  time: 0.1419  data: 0.0157  max mem: 4873\n",
            "Epoch: [8]  [240/781]  eta: 0:01:17  lr: 0.000021  loss: 2.1996 (2.4071)  time: 0.1447  data: 0.0211  max mem: 4873\n",
            "Epoch: [8]  [250/781]  eta: 0:01:15  lr: 0.000021  loss: 2.1947 (2.4083)  time: 0.1377  data: 0.0164  max mem: 4873\n",
            "Epoch: [8]  [260/781]  eta: 0:01:13  lr: 0.000021  loss: 2.1962 (2.4116)  time: 0.1339  data: 0.0125  max mem: 4873\n",
            "Epoch: [8]  [270/781]  eta: 0:01:12  lr: 0.000021  loss: 2.2265 (2.4109)  time: 0.1367  data: 0.0153  max mem: 4873\n",
            "Epoch: [8]  [280/781]  eta: 0:01:10  lr: 0.000021  loss: 2.2775 (2.4219)  time: 0.1387  data: 0.0167  max mem: 4873\n",
            "Epoch: [8]  [290/781]  eta: 0:01:09  lr: 0.000021  loss: 2.2611 (2.4149)  time: 0.1405  data: 0.0180  max mem: 4873\n",
            "Epoch: [8]  [300/781]  eta: 0:01:08  lr: 0.000021  loss: 2.2283 (2.4139)  time: 0.1421  data: 0.0178  max mem: 4873\n",
            "Epoch: [8]  [310/781]  eta: 0:01:06  lr: 0.000021  loss: 2.2801 (2.4202)  time: 0.1377  data: 0.0125  max mem: 4873\n",
            "Epoch: [8]  [320/781]  eta: 0:01:05  lr: 0.000021  loss: 2.2657 (2.4166)  time: 0.1445  data: 0.0216  max mem: 4873\n",
            "Epoch: [8]  [330/781]  eta: 0:01:03  lr: 0.000021  loss: 2.1848 (2.4126)  time: 0.1491  data: 0.0271  max mem: 4873\n",
            "Epoch: [8]  [340/781]  eta: 0:01:02  lr: 0.000021  loss: 2.1623 (2.4115)  time: 0.1396  data: 0.0175  max mem: 4873\n",
            "Epoch: [8]  [350/781]  eta: 0:01:00  lr: 0.000021  loss: 2.1619 (2.4054)  time: 0.1352  data: 0.0134  max mem: 4873\n",
            "Epoch: [8]  [360/781]  eta: 0:00:59  lr: 0.000021  loss: 2.1862 (2.4055)  time: 0.1368  data: 0.0146  max mem: 4873\n",
            "Epoch: [8]  [370/781]  eta: 0:00:58  lr: 0.000021  loss: 2.2199 (2.4005)  time: 0.1381  data: 0.0153  max mem: 4873\n",
            "Epoch: [8]  [380/781]  eta: 0:00:56  lr: 0.000021  loss: 2.1820 (2.4009)  time: 0.1386  data: 0.0162  max mem: 4873\n",
            "Epoch: [8]  [390/781]  eta: 0:00:55  lr: 0.000021  loss: 2.3656 (2.4155)  time: 0.1394  data: 0.0157  max mem: 4873\n",
            "Epoch: [8]  [400/781]  eta: 0:00:53  lr: 0.000021  loss: 2.2750 (2.4111)  time: 0.1427  data: 0.0173  max mem: 4873\n",
            "Epoch: [8]  [410/781]  eta: 0:00:52  lr: 0.000021  loss: 2.2750 (2.4167)  time: 0.1380  data: 0.0142  max mem: 4873\n",
            "Epoch: [8]  [420/781]  eta: 0:00:50  lr: 0.000021  loss: 2.3054 (2.4134)  time: 0.1305  data: 0.0083  max mem: 4873\n",
            "Epoch: [8]  [430/781]  eta: 0:00:49  lr: 0.000021  loss: 2.1998 (2.4097)  time: 0.1377  data: 0.0156  max mem: 4873\n",
            "Epoch: [8]  [440/781]  eta: 0:00:48  lr: 0.000021  loss: 2.2824 (2.4073)  time: 0.1467  data: 0.0245  max mem: 4873\n",
            "Epoch: [8]  [450/781]  eta: 0:00:46  lr: 0.000021  loss: 2.2143 (2.4061)  time: 0.1449  data: 0.0227  max mem: 4873\n",
            "Epoch: [8]  [460/781]  eta: 0:00:45  lr: 0.000021  loss: 2.2402 (2.4034)  time: 0.1403  data: 0.0188  max mem: 4873\n",
            "Epoch: [8]  [470/781]  eta: 0:00:43  lr: 0.000021  loss: 2.2625 (2.4082)  time: 0.1401  data: 0.0186  max mem: 4873\n",
            "Epoch: [8]  [480/781]  eta: 0:00:42  lr: 0.000021  loss: 2.2623 (2.4146)  time: 0.1370  data: 0.0144  max mem: 4873\n",
            "Epoch: [8]  [490/781]  eta: 0:00:41  lr: 0.000021  loss: 2.2063 (2.4126)  time: 0.1386  data: 0.0138  max mem: 4873\n",
            "Epoch: [8]  [500/781]  eta: 0:00:39  lr: 0.000021  loss: 2.1897 (2.4131)  time: 0.1414  data: 0.0175  max mem: 4873\n",
            "Epoch: [8]  [510/781]  eta: 0:00:38  lr: 0.000021  loss: 2.2557 (2.4145)  time: 0.1386  data: 0.0162  max mem: 4873\n",
            "Epoch: [8]  [520/781]  eta: 0:00:36  lr: 0.000021  loss: 2.2557 (2.4145)  time: 0.1350  data: 0.0126  max mem: 4873\n",
            "Epoch: [8]  [530/781]  eta: 0:00:35  lr: 0.000021  loss: 2.2015 (2.4136)  time: 0.1338  data: 0.0111  max mem: 4873\n",
            "Epoch: [8]  [540/781]  eta: 0:00:33  lr: 0.000021  loss: 2.2269 (2.4118)  time: 0.1340  data: 0.0114  max mem: 4873\n",
            "Epoch: [8]  [550/781]  eta: 0:00:32  lr: 0.000021  loss: 2.2597 (2.4089)  time: 0.1358  data: 0.0137  max mem: 4873\n",
            "Epoch: [8]  [560/781]  eta: 0:00:31  lr: 0.000021  loss: 2.2394 (2.4124)  time: 0.1381  data: 0.0160  max mem: 4873\n",
            "Epoch: [8]  [570/781]  eta: 0:00:29  lr: 0.000021  loss: 2.2424 (2.4132)  time: 0.1342  data: 0.0103  max mem: 4873\n",
            "Epoch: [8]  [580/781]  eta: 0:00:28  lr: 0.000021  loss: 2.1639 (2.4099)  time: 0.1367  data: 0.0111  max mem: 4873\n",
            "Epoch: [8]  [590/781]  eta: 0:00:26  lr: 0.000021  loss: 2.2004 (2.4157)  time: 0.1403  data: 0.0166  max mem: 4873\n",
            "Epoch: [8]  [600/781]  eta: 0:00:25  lr: 0.000021  loss: 2.3503 (2.4136)  time: 0.1443  data: 0.0217  max mem: 4873\n",
            "Epoch: [8]  [610/781]  eta: 0:00:24  lr: 0.000021  loss: 2.2797 (2.4109)  time: 0.1470  data: 0.0235  max mem: 4873\n",
            "Epoch: [8]  [620/781]  eta: 0:00:22  lr: 0.000021  loss: 2.2830 (2.4139)  time: 0.1430  data: 0.0199  max mem: 4873\n",
            "Epoch: [8]  [630/781]  eta: 0:00:21  lr: 0.000021  loss: 2.2771 (2.4114)  time: 0.1410  data: 0.0186  max mem: 4873\n",
            "Epoch: [8]  [640/781]  eta: 0:00:19  lr: 0.000021  loss: 2.1805 (2.4115)  time: 0.1432  data: 0.0209  max mem: 4873\n",
            "Epoch: [8]  [650/781]  eta: 0:00:18  lr: 0.000021  loss: 2.1679 (2.4106)  time: 0.1469  data: 0.0249  max mem: 4873\n",
            "Epoch: [8]  [660/781]  eta: 0:00:17  lr: 0.000021  loss: 2.2916 (2.4102)  time: 0.1459  data: 0.0228  max mem: 4873\n",
            "Epoch: [8]  [670/781]  eta: 0:00:15  lr: 0.000021  loss: 2.2665 (2.4109)  time: 0.1501  data: 0.0260  max mem: 4873\n",
            "Epoch: [8]  [680/781]  eta: 0:00:14  lr: 0.000021  loss: 2.2956 (2.4152)  time: 0.1478  data: 0.0247  max mem: 4873\n",
            "Epoch: [8]  [690/781]  eta: 0:00:12  lr: 0.000021  loss: 2.2599 (2.4130)  time: 0.1394  data: 0.0171  max mem: 4873\n",
            "Epoch: [8]  [700/781]  eta: 0:00:11  lr: 0.000021  loss: 2.2572 (2.4136)  time: 0.1390  data: 0.0163  max mem: 4873\n",
            "Epoch: [8]  [710/781]  eta: 0:00:09  lr: 0.000021  loss: 2.2572 (2.4128)  time: 0.1366  data: 0.0143  max mem: 4873\n",
            "Epoch: [8]  [720/781]  eta: 0:00:08  lr: 0.000021  loss: 2.2009 (2.4100)  time: 0.1368  data: 0.0153  max mem: 4873\n",
            "Epoch: [8]  [730/781]  eta: 0:00:07  lr: 0.000021  loss: 2.2535 (2.4106)  time: 0.1418  data: 0.0198  max mem: 4873\n",
            "Epoch: [8]  [740/781]  eta: 0:00:05  lr: 0.000021  loss: 2.2535 (2.4106)  time: 0.1443  data: 0.0207  max mem: 4873\n",
            "Epoch: [8]  [750/781]  eta: 0:00:04  lr: 0.000021  loss: 2.3062 (2.4141)  time: 0.1428  data: 0.0187  max mem: 4873\n",
            "Epoch: [8]  [760/781]  eta: 0:00:02  lr: 0.000021  loss: 2.2186 (2.4120)  time: 0.1391  data: 0.0155  max mem: 4873\n",
            "Epoch: [8]  [770/781]  eta: 0:00:01  lr: 0.000021  loss: 2.2461 (2.4144)  time: 0.1408  data: 0.0178  max mem: 4873\n",
            "Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000021  loss: 2.2399 (2.4110)  time: 0.1420  data: 0.0200  max mem: 4873\n",
            "Epoch: [8] Total time: 0:01:50 (0.1409 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 2.2399 (2.4110)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8276 (0.8276)  acc1: 79.1667 (79.1667)  acc5: 95.3125 (95.3125)  time: 0.8188  data: 0.7879  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1580 (1.0166)  acc1: 75.0000 (76.8939)  acc5: 93.7500 (93.1818)  time: 0.1690  data: 0.1383  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1610 (1.0951)  acc1: 73.9583 (75.4464)  acc5: 91.6667 (91.7163)  time: 0.1253  data: 0.0946  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2345 (1.1383)  acc1: 70.8333 (74.4288)  acc5: 91.1458 (91.3979)  time: 0.1263  data: 0.0957  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2399 (1.1881)  acc1: 71.3542 (72.8532)  acc5: 90.6250 (90.8918)  time: 0.1271  data: 0.0964  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2308 (1.1880)  acc1: 71.3542 (72.7533)  acc5: 90.6250 (90.9212)  time: 0.1304  data: 0.0998  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2357 (1.1948)  acc1: 70.8333 (72.6300)  acc5: 90.6250 (90.9500)  time: 0.1103  data: 0.0806  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1337 s / it)\n",
            "* Acc@1 72.630 Acc@5 90.950 loss 1.195\n",
            "Accuracy of the network on the 10000 test images: 72.6%\n",
            "Max accuracy: 72.63%\n",
            "Epoch: [9]  [  0/781]  eta: 0:12:27  lr: 0.000015  loss: 2.2680 (2.2680)  time: 0.9567  data: 0.8229  max mem: 4873\n",
            "Epoch: [9]  [ 10/781]  eta: 0:02:38  lr: 0.000015  loss: 2.2027 (2.2095)  time: 0.2055  data: 0.0805  max mem: 4873\n",
            "Epoch: [9]  [ 20/781]  eta: 0:02:10  lr: 0.000015  loss: 2.2027 (2.2699)  time: 0.1322  data: 0.0086  max mem: 4873\n",
            "Epoch: [9]  [ 30/781]  eta: 0:01:56  lr: 0.000015  loss: 2.1834 (2.3048)  time: 0.1280  data: 0.0057  max mem: 4873\n",
            "Epoch: [9]  [ 40/781]  eta: 0:01:49  lr: 0.000015  loss: 2.2223 (2.3703)  time: 0.1219  data: 0.0003  max mem: 4873\n",
            "Epoch: [9]  [ 50/781]  eta: 0:01:45  lr: 0.000015  loss: 2.2272 (2.3835)  time: 0.1263  data: 0.0048  max mem: 4873\n",
            "Epoch: [9]  [ 60/781]  eta: 0:01:42  lr: 0.000015  loss: 2.2367 (2.4105)  time: 0.1300  data: 0.0070  max mem: 4873\n",
            "Epoch: [9]  [ 70/781]  eta: 0:01:39  lr: 0.000015  loss: 2.3329 (2.4588)  time: 0.1271  data: 0.0034  max mem: 4873\n",
            "Epoch: [9]  [ 80/781]  eta: 0:01:36  lr: 0.000015  loss: 2.2433 (2.4735)  time: 0.1265  data: 0.0041  max mem: 4873\n",
            "Epoch: [9]  [ 90/781]  eta: 0:01:34  lr: 0.000015  loss: 2.4632 (2.5049)  time: 0.1305  data: 0.0071  max mem: 4873\n",
            "Epoch: [9]  [100/781]  eta: 0:01:33  lr: 0.000015  loss: 2.1845 (2.4687)  time: 0.1321  data: 0.0076  max mem: 4873\n",
            "Epoch: [9]  [110/781]  eta: 0:01:31  lr: 0.000015  loss: 2.1839 (2.4810)  time: 0.1317  data: 0.0086  max mem: 4873\n",
            "Epoch: [9]  [120/781]  eta: 0:01:29  lr: 0.000015  loss: 2.3030 (2.4803)  time: 0.1285  data: 0.0065  max mem: 4873\n",
            "Epoch: [9]  [130/781]  eta: 0:01:27  lr: 0.000015  loss: 2.2366 (2.4695)  time: 0.1271  data: 0.0054  max mem: 4873\n",
            "Epoch: [9]  [140/781]  eta: 0:01:25  lr: 0.000015  loss: 2.1810 (2.4827)  time: 0.1268  data: 0.0052  max mem: 4873\n",
            "Epoch: [9]  [150/781]  eta: 0:01:24  lr: 0.000015  loss: 2.2962 (2.4781)  time: 0.1297  data: 0.0079  max mem: 4873\n",
            "Epoch: [9]  [160/781]  eta: 0:01:23  lr: 0.000015  loss: 2.2475 (2.4688)  time: 0.1313  data: 0.0086  max mem: 4873\n",
            "Epoch: [9]  [170/781]  eta: 0:01:21  lr: 0.000015  loss: 2.2567 (2.4701)  time: 0.1262  data: 0.0022  max mem: 4873\n",
            "Epoch: [9]  [180/781]  eta: 0:01:20  lr: 0.000015  loss: 2.2567 (2.4613)  time: 0.1298  data: 0.0046  max mem: 4873\n",
            "Epoch: [9]  [190/781]  eta: 0:01:18  lr: 0.000015  loss: 2.1956 (2.4662)  time: 0.1364  data: 0.0119  max mem: 4873\n",
            "Epoch: [9]  [200/781]  eta: 0:01:17  lr: 0.000015  loss: 2.2504 (2.4559)  time: 0.1327  data: 0.0099  max mem: 4873\n",
            "Epoch: [9]  [210/781]  eta: 0:01:15  lr: 0.000015  loss: 2.2504 (2.4542)  time: 0.1254  data: 0.0029  max mem: 4873\n",
            "Epoch: [9]  [220/781]  eta: 0:01:14  lr: 0.000015  loss: 2.2240 (2.4527)  time: 0.1271  data: 0.0042  max mem: 4873\n",
            "Epoch: [9]  [230/781]  eta: 0:01:12  lr: 0.000015  loss: 2.2240 (2.4458)  time: 0.1281  data: 0.0057  max mem: 4873\n",
            "Epoch: [9]  [240/781]  eta: 0:01:11  lr: 0.000015  loss: 2.2133 (2.4358)  time: 0.1260  data: 0.0044  max mem: 4873\n",
            "Epoch: [9]  [250/781]  eta: 0:01:10  lr: 0.000015  loss: 2.2133 (2.4325)  time: 0.1261  data: 0.0046  max mem: 4873\n",
            "Epoch: [9]  [260/781]  eta: 0:01:08  lr: 0.000015  loss: 2.2181 (2.4359)  time: 0.1271  data: 0.0057  max mem: 4873\n",
            "Epoch: [9]  [270/781]  eta: 0:01:07  lr: 0.000015  loss: 2.2722 (2.4351)  time: 0.1296  data: 0.0075  max mem: 4873\n",
            "Epoch: [9]  [280/781]  eta: 0:01:05  lr: 0.000015  loss: 2.2698 (2.4286)  time: 0.1302  data: 0.0065  max mem: 4873\n",
            "Epoch: [9]  [290/781]  eta: 0:01:04  lr: 0.000015  loss: 2.2239 (2.4210)  time: 0.1268  data: 0.0027  max mem: 4873\n",
            "Epoch: [9]  [300/781]  eta: 0:01:03  lr: 0.000015  loss: 2.2357 (2.4209)  time: 0.1263  data: 0.0032  max mem: 4873\n",
            "Epoch: [9]  [310/781]  eta: 0:01:01  lr: 0.000015  loss: 2.2587 (2.4178)  time: 0.1256  data: 0.0035  max mem: 4873\n",
            "Epoch: [9]  [320/781]  eta: 0:01:00  lr: 0.000015  loss: 2.2310 (2.4160)  time: 0.1270  data: 0.0053  max mem: 4873\n",
            "Epoch: [9]  [330/781]  eta: 0:00:59  lr: 0.000015  loss: 2.2103 (2.4083)  time: 0.1283  data: 0.0067  max mem: 4873\n",
            "Epoch: [9]  [340/781]  eta: 0:00:57  lr: 0.000015  loss: 2.1718 (2.4063)  time: 0.1264  data: 0.0046  max mem: 4873\n",
            "Epoch: [9]  [350/781]  eta: 0:00:56  lr: 0.000015  loss: 2.1346 (2.4028)  time: 0.1248  data: 0.0030  max mem: 4873\n",
            "Epoch: [9]  [360/781]  eta: 0:00:54  lr: 0.000015  loss: 2.2118 (2.4067)  time: 0.1241  data: 0.0022  max mem: 4873\n",
            "Epoch: [9]  [370/781]  eta: 0:00:53  lr: 0.000015  loss: 2.2118 (2.4100)  time: 0.1266  data: 0.0045  max mem: 4873\n",
            "Epoch: [9]  [380/781]  eta: 0:00:52  lr: 0.000015  loss: 2.2089 (2.4094)  time: 0.1315  data: 0.0095  max mem: 4873\n",
            "Epoch: [9]  [390/781]  eta: 0:00:51  lr: 0.000015  loss: 2.1724 (2.4075)  time: 0.1344  data: 0.0124  max mem: 4873\n",
            "Epoch: [9]  [400/781]  eta: 0:00:49  lr: 0.000015  loss: 2.1382 (2.4026)  time: 0.1285  data: 0.0065  max mem: 4873\n",
            "Epoch: [9]  [410/781]  eta: 0:00:48  lr: 0.000015  loss: 2.1569 (2.4002)  time: 0.1280  data: 0.0055  max mem: 4873\n",
            "Epoch: [9]  [420/781]  eta: 0:00:47  lr: 0.000015  loss: 2.3019 (2.4077)  time: 0.1285  data: 0.0055  max mem: 4873\n",
            "Epoch: [9]  [430/781]  eta: 0:00:45  lr: 0.000015  loss: 2.2884 (2.4046)  time: 0.1239  data: 0.0013  max mem: 4873\n",
            "Epoch: [9]  [440/781]  eta: 0:00:44  lr: 0.000015  loss: 2.1642 (2.4014)  time: 0.1271  data: 0.0047  max mem: 4873\n",
            "Epoch: [9]  [450/781]  eta: 0:00:43  lr: 0.000015  loss: 2.2256 (2.3997)  time: 0.1287  data: 0.0059  max mem: 4873\n",
            "Epoch: [9]  [460/781]  eta: 0:00:41  lr: 0.000015  loss: 2.2228 (2.3985)  time: 0.1264  data: 0.0030  max mem: 4873\n",
            "Epoch: [9]  [470/781]  eta: 0:00:40  lr: 0.000015  loss: 2.1477 (2.3963)  time: 0.1258  data: 0.0018  max mem: 4873\n",
            "Epoch: [9]  [480/781]  eta: 0:00:39  lr: 0.000015  loss: 2.1813 (2.3949)  time: 0.1252  data: 0.0016  max mem: 4873\n",
            "Epoch: [9]  [490/781]  eta: 0:00:37  lr: 0.000015  loss: 2.1592 (2.3923)  time: 0.1335  data: 0.0101  max mem: 4873\n",
            "Epoch: [9]  [500/781]  eta: 0:00:36  lr: 0.000015  loss: 2.1198 (2.3936)  time: 0.1338  data: 0.0109  max mem: 4873\n",
            "Epoch: [9]  [510/781]  eta: 0:00:35  lr: 0.000015  loss: 2.1691 (2.3961)  time: 0.1262  data: 0.0042  max mem: 4873\n",
            "Epoch: [9]  [520/781]  eta: 0:00:33  lr: 0.000015  loss: 2.2778 (2.3968)  time: 0.1285  data: 0.0063  max mem: 4873\n",
            "Epoch: [9]  [530/781]  eta: 0:00:32  lr: 0.000015  loss: 2.2426 (2.3953)  time: 0.1284  data: 0.0063  max mem: 4873\n",
            "Epoch: [9]  [540/781]  eta: 0:00:31  lr: 0.000015  loss: 2.2284 (2.3949)  time: 0.1255  data: 0.0025  max mem: 4873\n",
            "Epoch: [9]  [550/781]  eta: 0:00:29  lr: 0.000015  loss: 2.2234 (2.3947)  time: 0.1264  data: 0.0036  max mem: 4873\n",
            "Epoch: [9]  [560/781]  eta: 0:00:28  lr: 0.000015  loss: 2.2918 (2.4007)  time: 0.1275  data: 0.0057  max mem: 4873\n",
            "Epoch: [9]  [570/781]  eta: 0:00:27  lr: 0.000015  loss: 2.3081 (2.4045)  time: 0.1287  data: 0.0068  max mem: 4873\n",
            "Epoch: [9]  [580/781]  eta: 0:00:26  lr: 0.000015  loss: 2.2189 (2.4059)  time: 0.1319  data: 0.0088  max mem: 4873\n",
            "Epoch: [9]  [590/781]  eta: 0:00:24  lr: 0.000015  loss: 2.2189 (2.4055)  time: 0.1364  data: 0.0126  max mem: 4873\n",
            "Epoch: [9]  [600/781]  eta: 0:00:23  lr: 0.000015  loss: 2.1847 (2.4031)  time: 0.1309  data: 0.0081  max mem: 4873\n",
            "Epoch: [9]  [610/781]  eta: 0:00:22  lr: 0.000015  loss: 2.1847 (2.3998)  time: 0.1303  data: 0.0081  max mem: 4873\n",
            "Epoch: [9]  [620/781]  eta: 0:00:20  lr: 0.000015  loss: 2.1888 (2.3976)  time: 0.1390  data: 0.0172  max mem: 4873\n",
            "Epoch: [9]  [630/781]  eta: 0:00:19  lr: 0.000015  loss: 2.1741 (2.3967)  time: 0.1350  data: 0.0130  max mem: 4873\n",
            "Epoch: [9]  [640/781]  eta: 0:00:18  lr: 0.000015  loss: 2.1257 (2.3932)  time: 0.1288  data: 0.0066  max mem: 4873\n",
            "Epoch: [9]  [650/781]  eta: 0:00:17  lr: 0.000015  loss: 2.1020 (2.3931)  time: 0.1279  data: 0.0045  max mem: 4873\n",
            "Epoch: [9]  [660/781]  eta: 0:00:15  lr: 0.000015  loss: 2.2179 (2.3925)  time: 0.1273  data: 0.0018  max mem: 4873\n",
            "Epoch: [9]  [670/781]  eta: 0:00:14  lr: 0.000015  loss: 2.1977 (2.3903)  time: 0.1269  data: 0.0021  max mem: 4873\n",
            "Epoch: [9]  [680/781]  eta: 0:00:13  lr: 0.000015  loss: 2.1303 (2.3894)  time: 0.1281  data: 0.0039  max mem: 4873\n",
            "Epoch: [9]  [690/781]  eta: 0:00:11  lr: 0.000015  loss: 2.1303 (2.3865)  time: 0.1255  data: 0.0022  max mem: 4873\n",
            "Epoch: [9]  [700/781]  eta: 0:00:10  lr: 0.000015  loss: 2.2034 (2.3890)  time: 0.1295  data: 0.0072  max mem: 4873\n",
            "Epoch: [9]  [710/781]  eta: 0:00:09  lr: 0.000015  loss: 2.2637 (2.3894)  time: 0.1365  data: 0.0134  max mem: 4873\n",
            "Epoch: [9]  [720/781]  eta: 0:00:07  lr: 0.000015  loss: 2.2808 (2.3916)  time: 0.1362  data: 0.0137  max mem: 4873\n",
            "Epoch: [9]  [730/781]  eta: 0:00:06  lr: 0.000015  loss: 2.2574 (2.3901)  time: 0.1374  data: 0.0150  max mem: 4873\n",
            "Epoch: [9]  [740/781]  eta: 0:00:05  lr: 0.000015  loss: 2.1386 (2.3866)  time: 0.1380  data: 0.0151  max mem: 4873\n",
            "Epoch: [9]  [750/781]  eta: 0:00:04  lr: 0.000015  loss: 2.1076 (2.3852)  time: 0.1407  data: 0.0180  max mem: 4873\n",
            "Epoch: [9]  [760/781]  eta: 0:00:02  lr: 0.000015  loss: 2.1610 (2.3839)  time: 0.1389  data: 0.0153  max mem: 4873\n",
            "Epoch: [9]  [770/781]  eta: 0:00:01  lr: 0.000015  loss: 2.2210 (2.3833)  time: 0.1324  data: 0.0080  max mem: 4873\n",
            "Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000015  loss: 2.1993 (2.3829)  time: 0.1281  data: 0.0054  max mem: 4873\n",
            "Epoch: [9] Total time: 0:01:41 (0.1306 s / it)\n",
            "Averaged stats: lr: 0.000015  loss: 2.1993 (2.3829)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8422 (0.8422)  acc1: 80.7292 (80.7292)  acc5: 94.7917 (94.7917)  time: 0.8259  data: 0.7949  max mem: 4873\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0568 (0.9989)  acc1: 76.5625 (77.3674)  acc5: 94.2708 (93.3239)  time: 0.1716  data: 0.1408  max mem: 4873\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0862 (1.0719)  acc1: 75.0000 (75.9425)  acc5: 91.6667 (91.8403)  time: 0.1230  data: 0.0923  max mem: 4873\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2024 (1.1106)  acc1: 72.3958 (75.1680)  acc5: 91.1458 (91.4987)  time: 0.1236  data: 0.0929  max mem: 4873\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2371 (1.1561)  acc1: 70.3125 (73.7551)  acc5: 89.5833 (91.0061)  time: 0.1291  data: 0.0984  max mem: 4873\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2371 (1.1634)  acc1: 68.7500 (73.4375)  acc5: 89.5833 (90.9926)  time: 0.1301  data: 0.0994  max mem: 4873\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2781 (1.1723)  acc1: 68.2292 (73.2300)  acc5: 90.1042 (91.0100)  time: 0.1106  data: 0.0809  max mem: 4873\n",
            "Test: Total time: 0:00:07 (0.1331 s / it)\n",
            "* Acc@1 73.230 Acc@5 91.010 loss 1.172\n",
            "Accuracy of the network on the 10000 test images: 73.2%\n",
            "Max accuracy: 73.23%\n",
            "Training time 0:19:01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}