{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1e450b-87e4-46ad-d608-ece08cfac138"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "08694784-a47c-4da7-b8b2-4fd4bbfd50ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "5054b0fa-d5c9-4513-f702-a738fc3da9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 23.20 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "7820625c-5403-45dd-8406-02b27fbe606e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "4baa49a7-d3fc-4f2b-aa55-572c773b6bd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "20c4d2b2-bb47-47a5-d5cd-3a1b219ac5a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "# !pip -q install timm==0.6.13 submitit\n",
        "!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\"\n",
        "#0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "095440b1-c1ac-4ece-9b48-fbe36dec6843"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "e7d0cf89-c885-416e-87a1-844dbcb9ea37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "4c74e1ba-e3a8-4bca-95b2-f03a6e96373d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "9dda4227-eb72-4b9b-d9c2-d74396d08bba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.4.12\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "443a8fa3-af93-4576-8b2a-ed0987e580cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "ef2f2561-c4e8-4906-858a-765d39aeb043",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "a7d17998-07ed-4850-f736-dfd186ab0cd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "0a3ee5c3-bd93-4855-a46b-ba0cf5d40c62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n02950826\n",
            "/content/tiny-imagenet-200/val/n02085620\n",
            "/content/tiny-imagenet-200/val/n01641577\n",
            "/content/tiny-imagenet-200/val/n04254777\n",
            "/content/tiny-imagenet-200/val/n02917067\n",
            "/content/tiny-imagenet-200/val/n03404251\n",
            "/content/tiny-imagenet-200/val/n03085013\n",
            "/content/tiny-imagenet-200/val/n02504458\n",
            "/content/tiny-imagenet-200/val/n03424325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "3d60b3f0-7415-4477-f89b-327068363604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Feb  9  2015 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Jan 30 19:31 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Jan 30 19:32 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "87a425f8-abc0-448a-c407-ade1aceb7c52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched augment.py for timm compatibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!sed -n '1,200p' models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Cm_gVMz1-_",
        "outputId": "bb4df83e-8d45-46bf-8297-54b950510c33",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "# Copyright (c) 2015-present, Facebook, Inc.\n",
            "# All rights reserved.\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from functools import partial\n",
            "\n",
            "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
            "from timm.models.registry import register_model\n",
            "from timm.models.layers import trunc_normal_\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n",
            "    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n",
            "    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n",
            "    'deit_base_distilled_patch16_384',\n",
            "]\n",
            "\n",
            "\n",
            "class DistilledVisionTransformer(VisionTransformer):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
            "        num_patches = self.patch_embed.num_patches\n",
            "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
            "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
            "\n",
            "        trunc_normal_(self.dist_token, std=.02)\n",
            "        trunc_normal_(self.pos_embed, std=.02)\n",
            "        self.head_dist.apply(self._init_weights)\n",
            "\n",
            "    def forward_features(self, x):\n",
            "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
            "        # with slight modifications to add the dist_token\n",
            "        B = x.shape[0]\n",
            "        x = self.patch_embed(x)\n",
            "\n",
            "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
            "        dist_token = self.dist_token.expand(B, -1, -1)\n",
            "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
            "\n",
            "        x = x + self.pos_embed\n",
            "        x = self.pos_drop(x)\n",
            "\n",
            "        for blk in self.blocks:\n",
            "            x = blk(x)\n",
            "\n",
            "        x = self.norm(x)\n",
            "        return x[:, 0], x[:, 1]\n",
            "\n",
            "    def forward(self, x):\n",
            "        x, x_dist = self.forward_features(x)\n",
            "        x = self.head(x)\n",
            "        x_dist = self.head_dist(x_dist)\n",
            "        if self.training:\n",
            "            return x, x_dist\n",
            "        else:\n",
            "            # during inference, return the average of both classifier predictions\n",
            "            return (x + x_dist) / 2\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_patch16_384(pretrained=False, **kwargs):\n",
            "    model = VisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n",
            "\n",
            "\n",
            "@register_model\n",
            "def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n",
            "    model = DistilledVisionTransformer(\n",
            "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
            "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
            "    model.default_cfg = _cfg()\n",
            "    if pretrained:\n",
            "        checkpoint = torch.hub.load_state_dict_from_url(\n",
            "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\",\n",
            "            map_location=\"cpu\", check_hash=True\n",
            "        )\n",
            "        model.load_state_dict(checkpoint[\"model\"])\n",
            "    return model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RizknqA6MBXb",
        "outputId": "d70931e8-de44-475f-b3dd-67d249f6ad0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "ls: cannot access 'multiteacher_loss.py': No such file or directory\n",
            "✅ old file removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(\n",
        "    teacher_logits: Dict[str, torch.Tensor],\n",
        "    teacher_order: List[str],\n",
        "    lambdas: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)  # (B, T, C)\n",
        "\n",
        "    lambdas = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lambdas.dim() == 1:\n",
        "        lambdas = lambdas.unsqueeze(0).expand(stacked.size(0), -1)  # (B, T)\n",
        "\n",
        "    return (stacked * lambdas.unsqueeze(-1)).sum(dim=1)  # (B, C)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    return F.cross_entropy(student_logits, teacher_logits.argmax(dim=-1))\n",
        "\n",
        "\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleDict({\n",
        "            name: timm.create_model(name, pretrained=True, num_classes=1000).eval().to(device)\n",
        "            for name in teacher_names\n",
        "        })\n",
        "        for m in self.models.values():\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "class TeacherLogitAdapter(nn.Module):\n",
        "    def __init__(self, teacher_keys: List[str], student_num_classes: int):\n",
        "        super().__init__()\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            k: nn.Linear(1000, student_num_classes, bias=False) for k in teacher_keys\n",
        "        })\n",
        "\n",
        "    def forward(self, teacher_logits: Dict[str, torch.Tensor]):\n",
        "        return {k: self.adapters[k](v) for k, v in teacher_logits.items()}\n",
        "\n",
        "\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    def __init__(self, temp: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.temp = temp\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, student_logits, teacher_logits, teacher_order, targets):\n",
        "        stacked = torch.stack([teacher_logits[k] for k in teacher_order], dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Hard labels: (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Soft labels (mixup/cutmix): (B,C)\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)  # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion,\n",
        "        student_num_classes: int,\n",
        "        teacher_names: List[str],\n",
        "        distillation_type: str = \"soft\",\n",
        "        alpha: float = 0.5,\n",
        "        tau: float = 2.0,\n",
        "        device=None,\n",
        "        use_adapter: bool = True,\n",
        "        hdtse_warmup_epochs: int = 0,\n",
        "        lambda_log: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = distillation_type\n",
        "        self.alpha = alpha\n",
        "        self.tau = tau\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, self.device)\n",
        "        self.teacher_order = list(self.teachers.teacher_order)\n",
        "\n",
        "        self.adapter = TeacherLogitAdapter(self.teachers.teacher_order, student_num_classes).to(self.device) if use_adapter else None\n",
        "        self.hdtse = HDTSEConfidence()\n",
        "\n",
        "        # ---- New controls ----\n",
        "        self.epoch: int = 0\n",
        "        self.hdtse_warmup_epochs = int(hdtse_warmup_epochs)\n",
        "        self.lambda_log = bool(lambda_log)\n",
        "\n",
        "        # ---- Logging state (epoch-level) ----\n",
        "        self._lambda_sum = torch.zeros(len(self.teacher_order), dtype=torch.float32)\n",
        "        self._lambda_count = 0\n",
        "        self.last_lambdas: Optional[torch.Tensor] = None  # (B,T) from last forward\n",
        "\n",
        "    def set_epoch(self, epoch: int):\n",
        "        self.epoch = int(epoch)\n",
        "\n",
        "    def _uniform_lambdas(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        t = len(self.teacher_order)\n",
        "        return torch.full((batch_size, t), 1.0 / t, device=device, dtype=torch.float32)\n",
        "\n",
        "    def pop_lambda_stats(self) -> Optional[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Returns mean λ per teacher over the epoch, then resets accumulators.\n",
        "        Call this once per epoch from main.py.\n",
        "        \"\"\"\n",
        "        if self._lambda_count <= 0:\n",
        "            return None\n",
        "\n",
        "        mean_lmb = (self._lambda_sum / float(self._lambda_count)).tolist()\n",
        "        out = {f\"lambda_{name}\": float(v) for name, v in zip(self.teacher_order, mean_lmb)}\n",
        "\n",
        "        # reset\n",
        "        self._lambda_sum.zero_()\n",
        "        self._lambda_count = 0\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs, outputs, targets):\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)\n",
        "        if self.adapter is not None:\n",
        "            t_logits = self.adapter(t_logits)\n",
        "\n",
        "        # ---- HDTSE delay ----\n",
        "        if self.epoch < self.hdtse_warmup_epochs:\n",
        "            lambdas = self._uniform_lambdas(outputs.size(0), outputs.device)  # (B,T)\n",
        "        else:\n",
        "            lambdas = self.hdtse(outputs, t_logits, list(t_logits.keys()), targets)  # (B,T)\n",
        "\n",
        "        self.last_lambdas = lambdas.detach()\n",
        "\n",
        "        # ---- λ logging ----\n",
        "        if self.lambda_log:\n",
        "            # accumulate batch mean λ, weighted by batch size\n",
        "            batch_mean = lambdas.detach().mean(dim=0).cpu()  # (T,)\n",
        "            self._lambda_sum += batch_mean * outputs.size(0)\n",
        "            self._lambda_count += outputs.size(0)\n",
        "\n",
        "        fused = fuse_logits(t_logits, self.teacher_order, lambdas)\n",
        "\n",
        "        kd = kd_soft(outputs, fused, self.tau) if self.distillation_type == \"soft\" else kd_hard(outputs, fused)\n",
        "        return (1 - self.alpha) * base_loss + self.alpha * kd\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4jzkzbMHD-",
        "outputId": "3e3544c0-4113-4bab-b778-403eaf4d23ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "File written: multiteacher_loss.py\n",
            "File size (bytes): 6517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import py_compile\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "assert MAIN.exists(), f\"Not found: {MAIN}\"\n",
        "\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Add import for MultiTeacherDistillationLoss\n",
        "# ------------------------------------------------------------\n",
        "if \"from multiteacher_loss import MultiTeacherDistillationLoss\" not in txt:\n",
        "    if \"from losses import DistillationLoss\" in txt:\n",
        "        txt = txt.replace(\n",
        "            \"from losses import DistillationLoss\",\n",
        "            \"from losses import DistillationLoss\\nfrom multiteacher_loss import MultiTeacherDistillationLoss\",\n",
        "            1\n",
        "        )\n",
        "    else:\n",
        "        raise RuntimeError(\"Could not find 'from losses import DistillationLoss' to insert MultiTeacher import.\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Add CLI args after --teacher-path\n",
        "# -----------------------------\n",
        "if \"--teacher-models\" not in txt:\n",
        "    anchor = \"    parser.add_argument('--teacher-path', type=str, default='')\"\n",
        "    if anchor not in txt:\n",
        "        raise RuntimeError(\"Couldn't find --teacher-path argument to insert after.\")\n",
        "    insert = (\n",
        "        \"    parser.add_argument('--teacher-models', type=str, default='',\\n\"\n",
        "        \"                        help='Comma-separated timm model names for multi-teacher distillation')\\n\"\n",
        "        \"    parser.add_argument('--hdtse-warmup-epochs', default=3, type=int,\\n\"\n",
        "        \"                        help='Use uniform lambdas for first N epochs, then enable HDTSE weighting')\\n\"\n",
        "        \"    parser.add_argument('--lambda-log', action='store_true',\\n\"\n",
        "        \"                        help='Log mean lambda per teacher each epoch (only for multi-teacher)')\\n\"\n",
        "    )\n",
        "    txt = txt.replace(anchor, anchor + \"\\n\" + insert, 1)\n",
        "    print(\"✅ Added --teacher-models, --hdtse-warmup-epochs, --lambda-log\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Allow finetune + multi-teacher distillation (block only single-teacher)\n",
        "# ------------------------------------------------------------\n",
        "txt = re.sub(\n",
        "    r\"if args\\.distillation_type != 'none' and args\\.finetune and not args\\.eval:\\s*\\n\\s*raise NotImplementedError\\(\\\"Finetuning with distillation not yet supported\\\"\\)\",\n",
        "    \"if args.distillation_type != 'none' and args.finetune and not args.eval and not args.teacher_models:\\n\"\n",
        "    \"        raise NotImplementedError(\\\"Finetuning with distillation not yet supported (single-teacher path)\\\")\",\n",
        "    txt\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Ensure SoftTargetCrossEntropy is instantiated (it is already () in your base, keep safe)\n",
        "# ------------------------------------------------------------\n",
        "txt = txt.replace(\"criterion = SoftTargetCrossEntropy\\n\", \"criterion = SoftTargetCrossEntropy()\\n\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Remove early scheduler creation (base does it before distillation + adapter)\n",
        "# ------------------------------------------------------------\n",
        "txt = re.sub(\n",
        "    r\"^\\s*lr_scheduler\\s*,\\s*_\\s*=\\s*create_scheduler\\(\\s*args\\s*,\\s*optimizer\\s*\\)\\s*$\",\n",
        "    \"\",\n",
        "    txt,\n",
        "    flags=re.MULTILINE\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 6) Replace distillation section with unified multi + single teacher logic\n",
        "#    Anchors: \"teacher_model = None\" up to just before \"output_dir = Path(args.output_dir)\"\n",
        "# ============================================================\n",
        "start_key = \"    teacher_model = None\"\n",
        "out_key = \"    output_dir = Path(args.output_dir)\"\n",
        "\n",
        "s = txt.find(start_key)\n",
        "o = txt.find(out_key)\n",
        "\n",
        "if s == -1 or o == -1 or o <= s:\n",
        "    raise RuntimeError(\"❌ Could not locate distillation anchors ('teacher_model = None' / 'output_dir = Path(...)').\")\n",
        "\n",
        "\n",
        "# The base file has the DistillationLoss call ending before output_dir.\n",
        "# We'll replace everything from teacher_model=None up to just before output_dir.\n",
        "replacement_block = \"\"\"    teacher_model = None\n",
        "\n",
        "    if args.distillation_type != 'none':\n",
        "        # Allow either teacher-path (single teacher) OR teacher-models (multi teacher)\n",
        "        assert (args.teacher_path or args.teacher_models), 'need to specify teacher-path OR teacher-models when using distillation'\n",
        "\n",
        "        # -----------------------\n",
        "        # Multi-teacher distillation\n",
        "        # -----------------------\n",
        "        if args.teacher_models:\n",
        "            teacher_names = [t.strip() for t in args.teacher_models.split(',') if t.strip()]\n",
        "            print(\"✅ Multi-teacher distillation enabled. Teachers:\", teacher_names)\n",
        "\n",
        "            criterion = MultiTeacherDistillationLoss(\n",
        "                base_criterion=criterion,\n",
        "                student_num_classes=args.nb_classes,\n",
        "                teacher_names=teacher_names,\n",
        "                distillation_type=args.distillation_type,\n",
        "                alpha=args.distillation_alpha,\n",
        "                tau=args.distillation_tau,\n",
        "                device=device,\n",
        "                use_adapter=True,\n",
        "                hdtse_warmup_epochs=args.hdtse_warmup_epochs,\n",
        "                lambda_log=args.lambda_log,\n",
        "            )\n",
        "\n",
        "            # IMPORTANT: adapter must be trained\n",
        "            if hasattr(criterion, \"adapter\") and criterion.adapter is not None:\n",
        "                optimizer.add_param_group({\n",
        "                    \"params\": criterion.adapter.parameters(),\n",
        "                    \"lr\": args.lr,\n",
        "                    \"weight_decay\": 0.0\n",
        "                })\n",
        "                print(\"✅ Added adapter parameters to optimizer\")\n",
        "\n",
        "        # -----------------------\n",
        "        # Single-teacher distillation (original DeiT)\n",
        "        # -----------------------\n",
        "        else:\n",
        "            print(f\"Creating teacher model: {args.teacher_model}\")\n",
        "            teacher_model = create_model(\n",
        "                args.teacher_model,\n",
        "                pretrained=False,\n",
        "                num_classes=args.nb_classes,\n",
        "                global_pool='avg',\n",
        "            )\n",
        "            if args.teacher_path.startswith('https'):\n",
        "                checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                    args.teacher_path, map_location='cpu', check_hash=True)\n",
        "            else:\n",
        "                checkpoint = torch.load(args.teacher_path, map_location='cpu')\n",
        "            teacher_model.load_state_dict(checkpoint['model'])\n",
        "            teacher_model.to(device)\n",
        "            teacher_model.eval()\n",
        "\n",
        "            criterion = DistillationLoss(\n",
        "                criterion, teacher_model, args.distillation_type, args.distillation_alpha, args.distillation_tau\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        # No distillation\n",
        "        pass\n",
        "\n",
        "    # Create scheduler AFTER optimizer has all param groups (incl adapter)\n",
        "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "txt = txt[:s] + replacement_block + \"\\n\" + txt[o:]  # keep the 'output_dir...' line onward\n",
        "\n",
        "# ============================================================\n",
        "# 6) Add epoch setter + lambda logging in training loop\n",
        "#    We insert two snippets:\n",
        "#    (a) before train_one_epoch call\n",
        "#    (b) after train_one_epoch returns\n",
        "# ============================================================\n",
        "\n",
        "# (a) before train_one_epoch(...) call\n",
        "if \"criterion.set_epoch(epoch)\" not in txt:\n",
        "    # find the call site \"train_stats = train_one_epoch(\" and insert just above it\n",
        "    marker = \"        train_stats = train_one_epoch(\"\n",
        "    idx = txt.find(marker)\n",
        "    if idx == -1:\n",
        "        print(\"⚠️ Could not find train_one_epoch call to insert set_epoch(). Skipping.\")\n",
        "    else:\n",
        "        insert = (\n",
        "            \"        # ---- Multi-teacher: inform loss about current epoch (for HDTSE warmup) ----\\n\"\n",
        "            \"        if hasattr(criterion, \\\"set_epoch\\\"):\\n\"\n",
        "            \"            criterion.set_epoch(epoch)\\n\\n\"\n",
        "        )\n",
        "        txt = txt[:idx] + insert + txt[idx:]\n",
        "        print(\"✅ Inserted criterion.set_epoch(epoch) before train_one_epoch\")\n",
        "\n",
        "# (b) after train_one_epoch(...) returns, log lambdas\n",
        "if \"pop_lambda_stats\" not in txt:\n",
        "    # Insert after the line: train_stats = train_one_epoch(...)\n",
        "    # We'll look for the first occurrence of \"train_stats = train_one_epoch(\" then find the next blank line\n",
        "    m = re.search(r\"train_stats\\s*=\\s*train_one_epoch\\([\\s\\S]*?\\)\\n\", txt)\n",
        "    if not m:\n",
        "        print(\"⚠️ Could not locate end of train_one_epoch(...) call to insert lambda logging. Skipping.\")\n",
        "    else:\n",
        "        insert_after = (\n",
        "            \"\\n        # ---- Multi-teacher: lambda (λ) logging for HDTSE transparency ----\\n\"\n",
        "            \"        if getattr(args, \\\"lambda_log\\\", False) and hasattr(criterion, \\\"pop_lambda_stats\\\"):\\n\"\n",
        "            \"            lmb_stats = criterion.pop_lambda_stats()\\n\"\n",
        "            \"            if lmb_stats:\\n\"\n",
        "            \"                print(\\\"λ(mean over epoch):\\\", lmb_stats)\\n\"\n",
        "            \"                train_stats.update(lmb_stats)\\n\"\n",
        "        )\n",
        "        txt = txt[:m.end()] + insert_after + txt[m.end():]\n",
        "        print(\"✅ Inserted per-epoch λ logging after train_one_epoch\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# (F) In training loop: set_epoch + lambda stats logging\n",
        "# ---------------------------------------------------------\n",
        "loop_anchor = re.search(r\"for epoch in range\\(args\\.start_epoch, args\\.epochs\\):\\n\", txt)\n",
        "if not loop_anchor:\n",
        "    raise RuntimeError(\"Could not find training loop to add set_epoch hook.\")\n",
        "\n",
        "set_epoch_hook = (\n",
        "    \"        # Multi-teacher: drive HDTSE warmup + lambda logging\\n\"\n",
        "    \"        if hasattr(criterion, 'set_epoch'):\\n\"\n",
        "    \"            criterion.set_epoch(epoch)\\n\"\n",
        ")\n",
        "txt = txt[:loop_anchor.end()] + set_epoch_hook + txt[loop_anchor.end():]\n",
        "\n",
        "# After train_one_epoch, print λ stats if available\n",
        "after_train_anchor = re.search(r\"train_stats\\s*=\\s*train_one_epoch\\([\\s\\S]*?\\)\\n\", txt)\n",
        "if not after_train_anchor:\n",
        "    raise RuntimeError(\"Could not find train_one_epoch(...) call block to attach lambda stats reporting.\")\n",
        "\n",
        "lambda_report = (\n",
        "    \"        if hasattr(criterion, 'pop_lambda_stats'):\\n\"\n",
        "    \"            lmb = criterion.pop_lambda_stats()\\n\"\n",
        "    \"            if lmb is not None:\\n\"\n",
        "    \"                print('λ means:', lmb)\\n\"\n",
        ")\n",
        "txt = txt[:after_train_anchor.end()] + lambda_report + txt[after_train_anchor.end():]\n",
        "\n",
        "path.write_text(txt)\n",
        "print(\"✅ Patched main.py written to:\", path)\n",
        "\n",
        "py_compile.compile(str(path), doraise=True)\n",
        "print(\"✅ main.py compiles successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdJZ4F-NoE-",
        "outputId": "b4d1e792-c4f1-4a4b-81de-f5f98a246772"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Added --teacher-models, --hdtse-warmup-epochs, --lambda-log\n",
            "✅ Inserted criterion.set_epoch(epoch) before train_one_epoch\n",
            "✅ Inserted per-epoch λ logging after train_one_epoch\n",
            "✅ Patched main.py written to: multiteacher_loss.py\n",
            "✅ main.py compiles successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "eced0f71-959d-44fe-f34e-2fc2a132f0b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"pretrained_cfg\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxOmdCb90Ymg",
        "outputId": "86a570ee-0ca1-42c3-fc4f-88c80d49ab02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('pretrained_cfg', None)\n",
            "66:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "67:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "84:    kwargs.pop('pretrained_cfg', None)\n",
            "85:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "86:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "103:    kwargs.pop('pretrained_cfg', None)\n",
            "104:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "105:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "122:    kwargs.pop('pretrained_cfg', None)\n",
            "123:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "124:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "141:    kwargs.pop('pretrained_cfg', None)\n",
            "142:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "143:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "160:    kwargs.pop('pretrained_cfg', None)\n",
            "161:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "162:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "179:    kwargs.pop('pretrained_cfg', None)\n",
            "180:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "181:    kwargs.pop('pretrained_cfg_priority', None)\n",
            "198:    kwargs.pop('pretrained_cfg', None)\n",
            "199:    kwargs.pop('pretrained_cfg_overlay', None)\n",
            "200:    kwargs.pop('pretrained_cfg_priority', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "dc4fd4af-3a9e-4da6-fb1a-4ca00f79f81d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "V409XjDO1cdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"cache_dir\" /content/deit/models.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFoOP5c1dbu",
        "outputId": "112159b6-8517-4fe2-c222-ab86bf3fba1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65:    kwargs.pop('cache_dir', None)\n",
            "88:    kwargs.pop('cache_dir', None)\n",
            "111:    kwargs.pop('cache_dir', None)\n",
            "134:    kwargs.pop('cache_dir', None)\n",
            "157:    kwargs.pop('cache_dir', None)\n",
            "180:    kwargs.pop('cache_dir', None)\n",
            "203:    kwargs.pop('cache_dir', None)\n",
            "226:    kwargs.pop('cache_dir', None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 10 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 0 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.2 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --hdtse-warmup-epochs 3 \\\n",
        " --lambda-log \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b2,mobilenetv3_large_100,regnety_040\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "e6ad55cb-47d4-4e1f-95c6-dd48d80749af"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "usage: DeiT training and evaluation script [-h] [--batch-size BATCH_SIZE]\n",
            "                                           [--epochs EPOCHS] [--bce-loss]\n",
            "                                           [--unscale-lr] [--model MODEL]\n",
            "                                           [--input-size INPUT_SIZE]\n",
            "                                           [--drop PCT] [--drop-path PCT]\n",
            "                                           [--model-ema] [--no-model-ema]\n",
            "                                           [--model-ema-decay MODEL_EMA_DECAY]\n",
            "                                           [--model-ema-force-cpu]\n",
            "                                           [--opt OPTIMIZER]\n",
            "                                           [--opt-eps EPSILON]\n",
            "                                           [--opt-betas BETA [BETA ...]]\n",
            "                                           [--clip-grad NORM] [--momentum M]\n",
            "                                           [--weight-decay WEIGHT_DECAY]\n",
            "                                           [--sched SCHEDULER] [--lr LR]\n",
            "                                           [--lr-noise pct, pct [pct, pct ...]]\n",
            "                                           [--lr-noise-pct PERCENT]\n",
            "                                           [--lr-noise-std STDDEV]\n",
            "                                           [--warmup-lr LR] [--min-lr LR]\n",
            "                                           [--decay-epochs N]\n",
            "                                           [--warmup-epochs N]\n",
            "                                           [--cooldown-epochs N]\n",
            "                                           [--patience-epochs N]\n",
            "                                           [--decay-rate RATE]\n",
            "                                           [--color-jitter PCT] [--aa NAME]\n",
            "                                           [--smoothing SMOOTHING]\n",
            "                                           [--train-interpolation TRAIN_INTERPOLATION]\n",
            "                                           [--repeated-aug]\n",
            "                                           [--no-repeated-aug] [--train-mode]\n",
            "                                           [--no-train-mode] [--ThreeAugment]\n",
            "                                           [--src] [--reprob PCT]\n",
            "                                           [--remode REMODE]\n",
            "                                           [--recount RECOUNT] [--resplit]\n",
            "                                           [--mixup MIXUP] [--cutmix CUTMIX]\n",
            "                                           [--cutmix-minmax CUTMIX_MINMAX [CUTMIX_MINMAX ...]]\n",
            "                                           [--mixup-prob MIXUP_PROB]\n",
            "                                           [--mixup-switch-prob MIXUP_SWITCH_PROB]\n",
            "                                           [--mixup-mode MIXUP_MODE]\n",
            "                                           [--teacher-model MODEL]\n",
            "                                           [--teacher-path TEACHER_PATH]\n",
            "                                           [--distillation-type {none,soft,hard}]\n",
            "                                           [--distillation-alpha DISTILLATION_ALPHA]\n",
            "                                           [--distillation-tau DISTILLATION_TAU]\n",
            "                                           [--cosub] [--finetune FINETUNE]\n",
            "                                           [--attn-only]\n",
            "                                           [--data-path DATA_PATH]\n",
            "                                           [--data-set {CIFAR,IMNET,INAT,INAT19}]\n",
            "                                           [--inat-category {kingdom,phylum,class,order,supercategory,family,genus,name}]\n",
            "                                           [--output_dir OUTPUT_DIR]\n",
            "                                           [--device DEVICE] [--seed SEED]\n",
            "                                           [--resume RESUME] [--start_epoch N]\n",
            "                                           [--eval]\n",
            "                                           [--eval-crop-ratio EVAL_CROP_RATIO]\n",
            "                                           [--dist-eval]\n",
            "                                           [--num_workers NUM_WORKERS]\n",
            "                                           [--pin-mem] [--no-pin-mem]\n",
            "                                           [--distributed]\n",
            "                                           [--world_size WORLD_SIZE]\n",
            "                                           [--dist_url DIST_URL]\n",
            "DeiT training and evaluation script: error: unrecognized arguments: --hdtse-warmup-epochs 3 --lambda-log --teacher-models tf_efficientnet_b2,mobilenetv3_large_100,regnety_040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}