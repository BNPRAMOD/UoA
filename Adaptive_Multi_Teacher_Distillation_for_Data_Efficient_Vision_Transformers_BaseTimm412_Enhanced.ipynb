{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7975a96a-4440-4ae6-a98b-35390d25b175"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "5ac3bce1-0bc2-4bac-d639-662e252d6d1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "dc7b09f5-e9c2-4595-c8e4-767d3d55acaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 23.02 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "5cea890a-8671-4960-9406-e50625775311"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "64b1df04-8c25-4684-cbef-d22798bebc89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "786139ea-ece9-4800-c4cb-b7c65d6fb119"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "!pip -q install timm==0.6.13 submitit\n",
        "#!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "30c3c590-1442-4017-9b53-957dabf0fa5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.6.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "0185274a-5bbe-470d-9da7-107552c86cbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "879e2253-6c00-4408-e610-fb769ddd4cac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "c7d850b2-c5e8-4bd7-8ca5-94f8caadce4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.6.13\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "cf8333f4-d228-4690-f2c9-b216ecd872a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "3bef82b8-674e-4967-9da3-79b01ea4c875",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "343147f9-dd9f-4464-c7f0-91c8d19e36d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "37e9cda8-3e94-4291-f58f-314212a88772"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n03854065\n",
            "/content/tiny-imagenet-200/val/n02791270\n",
            "/content/tiny-imagenet-200/val/n01698640\n",
            "/content/tiny-imagenet-200/val/n03670208\n",
            "/content/tiny-imagenet-200/val/n04067472\n",
            "/content/tiny-imagenet-200/val/n02206856\n",
            "/content/tiny-imagenet-200/val/n02129165\n",
            "/content/tiny-imagenet-200/val/n02321529\n",
            "/content/tiny-imagenet-200/val/n02415577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "d1f4307d-b791-49a3-8465-095ea8b9d172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Feb  9  2015 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Feb  8 18:13 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Feb  8 18:13 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "2b976408-fc42-4414-d7f4-f4cc82b7676d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched augment.py for timm compatibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RizknqA6MBXb",
        "outputId": "c4cd017d-af94-4d08-85de-a3cca869a489"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "ls: cannot access 'multiteacher_loss.py': No such file or directory\n",
            "✅ old file removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict, List, Optional\n",
        "import json\n",
        "from pathlib import Path as _Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Normalize teacher weights so they sum to 1 (supports shape (T,) or (B,T)).\n",
        "    \"\"\"\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(\n",
        "    teacher_logits: Dict[str, torch.Tensor],\n",
        "    teacher_order: List[str],\n",
        "    lambdas: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Weighted sum of teacher logits.\n",
        "    teacher_logits[k]: (B,C)\n",
        "    lambdas: (B,T) or (T,)\n",
        "    returns: (B,C)\n",
        "    \"\"\"\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)  # (B,T,C)\n",
        "\n",
        "    lmb = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lmb.dim() == 1:\n",
        "        lmb = lmb.unsqueeze(0).expand(stacked.size(0), -1)  # (B,T)\n",
        "\n",
        "    return (stacked * lmb.unsqueeze(-1)).sum(dim=1)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Standard KL-based soft distillation loss with temperature scaling.\n",
        "    \"\"\"\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Hard distillation: cross-entropy against teacher argmax.\n",
        "    \"\"\"\n",
        "    return F.cross_entropy(student_logits, teacher_logits.argmax(dim=-1))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Teachers\n",
        "# -----------------------------\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Loads a list of timm pretrained teachers and freezes them.\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleDict(\n",
        "            {\n",
        "                name: timm.create_model(name, pretrained=True, num_classes=1000).eval().to(device)\n",
        "                for name in teacher_names\n",
        "            }\n",
        "        )\n",
        "        for m in self.models.values():\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Teacher logits mapping: ImageNet-1k -> Tiny-ImageNet (wnid-aligned gather)\n",
        "# -----------------------------\n",
        "def build_tiny_imagenet_im1k_indices(\n",
        "    tiny_root: str,\n",
        "    class_index_json: str = \"/content/imagenet_class_index.json\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a LongTensor of shape (200,) containing the ImageNet-1k class indices\n",
        "    corresponding to Tiny-ImageNet wnids.txt ordering.\n",
        "\n",
        "    Requires torchvision's imagenet_class_index.json (wnid->index via JSON).\n",
        "    \"\"\"\n",
        "    tiny_root_p = _Path(tiny_root)\n",
        "    wnids_path = tiny_root_p / \"wnids.txt\"\n",
        "    if not wnids_path.exists():\n",
        "        raise FileNotFoundError(f\"Could not find Tiny-ImageNet wnids.txt at: {wnids_path}\")\n",
        "\n",
        "    wnids = wnids_path.read_text().strip().splitlines()\n",
        "\n",
        "    class_index_path = _Path(class_index_json)\n",
        "    if not class_index_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing {class_index_json}. Download it before training.\\n\"\n",
        "            \"Example:\\n\"\n",
        "            \"  !wget -q https://raw.githubusercontent.com/pytorch/vision/main/torchvision/models/imagenet_class_index.json \"\n",
        "            f\"-O {class_index_json}\"\n",
        "        )\n",
        "\n",
        "    class_index = json.loads(class_index_path.read_text())\n",
        "    # class_index: {\"0\": [\"n01440764\", \"tench\"], ...}\n",
        "    wnid_to_idx = {v[0]: int(k) for k, v in class_index.items()}\n",
        "\n",
        "    indices: List[int] = []\n",
        "    missing: List[str] = []\n",
        "    for w in wnids:\n",
        "        if w in wnid_to_idx:\n",
        "            indices.append(wnid_to_idx[w])\n",
        "        else:\n",
        "            missing.append(w)\n",
        "\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            f\"{len(missing)} Tiny-ImageNet wnids were not found in ImageNet-1k mapping. \"\n",
        "            f\"First few missing: {missing[:10]}\"\n",
        "        )\n",
        "\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "\n",
        "class TeacherLogitMapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps ImageNet-1k teacher logits (B,1000) -> Tiny-ImageNet logits (B,200)\n",
        "    by selecting the 200 corresponding ImageNet indices (gather/index_select).\n",
        "    \"\"\"\n",
        "    def __init__(self, teacher_keys: List[str], im1k_indices: torch.Tensor):\n",
        "        super().__init__()\n",
        "        self.teacher_keys = list(teacher_keys)\n",
        "        self.register_buffer(\"im1k_indices\", im1k_indices)  # (200,)\n",
        "\n",
        "    def forward(self, teacher_logits: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        out: Dict[str, torch.Tensor] = {}\n",
        "        idx = self.im1k_indices\n",
        "        for k, v in teacher_logits.items():\n",
        "            # v: (B,1000) -> (B,200)\n",
        "            out[k] = v.index_select(dim=-1, index=idx)\n",
        "        return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# HDTSE confidence weighting\n",
        "# -----------------------------\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes per-sample teacher weights based on each teacher's confidence\n",
        "    on the (possibly soft) targets.\n",
        "    \"\"\"\n",
        "    def __init__(self, temp: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.temp = float(temp)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        teacher_logits: Dict[str, torch.Tensor],\n",
        "        teacher_order: List[str],\n",
        "        targets: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        stacked = torch.stack([teacher_logits[k] for k in teacher_order], dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Hard labels: (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Soft labels (mixup/cutmix): (B,C)\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)  # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Multi-teacher distillation loss\n",
        "# -----------------------------\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion,\n",
        "        student_num_classes: int,\n",
        "        teacher_names: List[str],\n",
        "        distillation_type: str = \"soft\",\n",
        "        alpha: float = 0.5,\n",
        "        tau: float = 2.0,\n",
        "        device=None,\n",
        "        use_adapter: bool = True,\n",
        "        hdtse_warmup_epochs: int = 0,\n",
        "        lambda_log: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        base_criterion: supervised loss (CE or soft-target CE when mixup is enabled)\n",
        "        distillation_type: \"soft\" or \"hard\"\n",
        "        alpha: final KD weight\n",
        "        tau: KD temperature\n",
        "        use_adapter: if True, expects Tiny-ImageNet mapping via set_tiny_root() before training\n",
        "        hdtse_warmup_epochs: use uniform lambdas until this epoch (exclusive)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = str(distillation_type)\n",
        "        self.tau = float(tau)\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # teachers (frozen)\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, self.device)\n",
        "        self.teacher_order = list(self.teachers.teacher_order)\n",
        "\n",
        "        # teacher->student class mapping (ImageNet-1k -> dataset classes)\n",
        "        self.use_adapter = bool(use_adapter)\n",
        "        self.adapter: Optional[nn.Module] = None  # created by set_tiny_root()\n",
        "\n",
        "        # HDTSE teacher weighting\n",
        "        self.hdtse = HDTSEConfidence()\n",
        "\n",
        "        # epoch state\n",
        "        self.epoch: int = 0\n",
        "        self.hdtse_warmup_epochs = int(hdtse_warmup_epochs)\n",
        "\n",
        "        # alpha schedule (KD weight ramp)\n",
        "        self.alpha_final = float(alpha)\n",
        "        self.alpha_start = 0.0\n",
        "        self.alpha_ramp_epochs = 20  # default ramp duration\n",
        "\n",
        "        # lambda logging (epoch-level)\n",
        "        self.lambda_log = bool(lambda_log)\n",
        "        self._lambda_sum = torch.zeros(len(self.teacher_order), dtype=torch.float32)\n",
        "        self._lambda_count = 0\n",
        "\n",
        "    # ---- Public setters ----\n",
        "    def set_epoch(self, epoch: int):\n",
        "        self.epoch = int(epoch)\n",
        "\n",
        "    def set_alpha_schedule(self, alpha_start: float = 0.0, alpha_ramp_epochs: int = 20):\n",
        "        self.alpha_start = float(alpha_start)\n",
        "        self.alpha_ramp_epochs = int(alpha_ramp_epochs)\n",
        "\n",
        "    def set_tiny_root(self, tiny_root: str, class_index_json: str = \"/content/imagenet_class_index.json\"):\n",
        "        \"\"\"\n",
        "        Call once (from main.py) after constructing this loss, before training starts.\n",
        "        Creates the gather-based teacher logits mapper: (B,1000)->(B,C).\n",
        "        \"\"\"\n",
        "        im1k_indices = build_tiny_imagenet_im1k_indices(tiny_root, class_index_json=class_index_json).to(self.device)\n",
        "        self.adapter = TeacherLogitMapper(self.teacher_order, im1k_indices).to(self.device)\n",
        "\n",
        "    # ---- Logging ----\n",
        "    def pop_lambda_stats(self) -> Optional[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Returns mean λ per teacher over the epoch, then resets accumulators.\n",
        "        Call once per epoch from main.py.\n",
        "        \"\"\"\n",
        "        if self._lambda_count <= 0:\n",
        "            return None\n",
        "\n",
        "        mean_lmb = (self._lambda_sum / float(self._lambda_count)).tolist()\n",
        "        out = {f\"lambda_{name}\": float(v) for name, v in zip(self.teacher_order, mean_lmb)}\n",
        "\n",
        "        self._lambda_sum.zero_()\n",
        "        self._lambda_count = 0\n",
        "        return out\n",
        "\n",
        "    # ---- Internals ----\n",
        "    def _uniform_lambdas(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        t = len(self.teacher_order)\n",
        "        return torch.full((batch_size, t), 1.0 / t, device=device, dtype=torch.float32)\n",
        "\n",
        "    def _alpha_effective(self) -> float:\n",
        "        if self.alpha_ramp_epochs <= 0:\n",
        "            return self.alpha_final\n",
        "        t = min(1.0, float(self.epoch) / float(self.alpha_ramp_epochs))\n",
        "        return self.alpha_start + t * (self.alpha_final - self.alpha_start)\n",
        "\n",
        "    # ---- Forward ----\n",
        "    def forward(self, inputs: torch.Tensor, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        inputs: images (B,3,H,W)\n",
        "        outputs: student logits (B,C)\n",
        "        targets: hard labels (B,) or soft labels (B,C) when mixup/cutmix is enabled\n",
        "        \"\"\"\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)  # dict: teacher -> (B,1000)\n",
        "\n",
        "        student_C = outputs.shape[-1]\n",
        "        any_teacher = next(iter(t_logits.values()))\n",
        "        teacher_C = any_teacher.shape[-1]\n",
        "\n",
        "        if teacher_C != student_C:\n",
        "            if self.adapter is None:\n",
        "                raise RuntimeError(\n",
        "                f\"Teacher logits have {teacher_C} classes but student has {student_C}. \"\n",
        "                \"Adapter not initialized. Call criterion.set_tiny_root(args.data_path).\"\n",
        "            )\n",
        "            t_logits = self.adapter(t_logits)  # dict: teacher -> (B,student_C)\n",
        "\n",
        "        # ---- Teacher weights (λ) ----\n",
        "        if self.epoch < self.hdtse_warmup_epochs:\n",
        "            lambdas = self._uniform_lambdas(outputs.size(0), outputs.device)  # (B,T)\n",
        "        else:\n",
        "            lambdas = self.hdtse(t_logits, self.teacher_order, targets)  # (B,T)\n",
        "\n",
        "        # ---- λ logging ----\n",
        "        if self.lambda_log:\n",
        "            batch_mean = lambdas.detach().mean(dim=0).cpu()  # (T,)\n",
        "            self._lambda_sum += batch_mean * outputs.size(0)\n",
        "            self._lambda_count += outputs.size(0)\n",
        "\n",
        "        fused = fuse_logits(t_logits, self.teacher_order, lambdas)  # (B,C)\n",
        "\n",
        "        kd = kd_soft(outputs, fused, self.tau) if self.distillation_type == \"soft\" else kd_hard(outputs, fused)\n",
        "\n",
        "        alpha_eff = self._alpha_effective()\n",
        "        return (1.0 - alpha_eff) * base_loss + alpha_eff * kd\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4jzkzbMHD-",
        "outputId": "5671d20a-5f5c-4d9d-d794-86fcc75fb73e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "File written: multiteacher_loss.py\n",
            "File size (bytes): 11996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, py_compile\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helpers (line-safe insertions to avoid indentation/newline bugs)\n",
        "# ------------------------------------------------------------\n",
        "def fix_broken_import_concatenation():\n",
        "    global txt\n",
        "    # Fix exact failure mode:\n",
        "    txt = txt.replace(\n",
        "        \"from multiteacher_loss import MultiTeacherDistillationLossfrom samplers import RASampler\",\n",
        "        \"from multiteacher_loss import MultiTeacherDistillationLoss\\nfrom samplers import RASampler\"\n",
        "    )\n",
        "\n",
        "def ensure_line_after(match_line_regex: str, new_line: str):\n",
        "    \"\"\"Insert `new_line` as a full line right AFTER the first line matching regex.\"\"\"\n",
        "    global txt\n",
        "    if new_line.strip() in txt:\n",
        "        return\n",
        "    lines = txt.splitlines(True)  # keep line endings\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(match_line_regex, line):\n",
        "            # insert after this line\n",
        "            if not new_line.endswith(\"\\n\"):\n",
        "                new_line2 = new_line + \"\\n\"\n",
        "            else:\n",
        "                new_line2 = new_line\n",
        "            lines.insert(i + 1, new_line2)\n",
        "            txt = \"\".join(lines)\n",
        "            return\n",
        "    raise RuntimeError(f\"Could not find line to insert after: {match_line_regex}\")\n",
        "\n",
        "def ensure_block_after_line(match_line_regex: str, block: str):\n",
        "    \"\"\"Insert a multi-line block after first line matching regex.\"\"\"\n",
        "    global txt\n",
        "    # Heuristic: if first unique token already exists, don't re-add\n",
        "    if \"--teacher-models\" in block and \"--teacher-models\" in txt and \"--hdtse-warmup-epochs\" in txt and \"--lambda-log\" in txt:\n",
        "        return\n",
        "    lines = txt.splitlines(True)\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(match_line_regex, line):\n",
        "            if not block.endswith(\"\\n\"):\n",
        "                block2 = block + \"\\n\"\n",
        "            else:\n",
        "                block2 = block\n",
        "            lines.insert(i + 1, block2)\n",
        "            txt = \"\".join(lines)\n",
        "            return\n",
        "    raise RuntimeError(f\"Could not find line to insert block after: {match_line_regex}\")\n",
        "\n",
        "def replace_first(pattern: str, repl: str, flags=re.DOTALL):\n",
        "    global txt\n",
        "    m = re.search(pattern, txt, flags)\n",
        "    if not m:\n",
        "        return False\n",
        "    txt = txt[:m.start()] + repl + txt[m.end():]\n",
        "    return True\n",
        "\n",
        "def remove_first_line_matching(line_regex: str):\n",
        "    global txt\n",
        "    lines = txt.splitlines(True)\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(line_regex, line):\n",
        "            del lines[i]\n",
        "            txt = \"\".join(lines)\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) Repair if prior patch created the exact SyntaxError\n",
        "# ------------------------------------------------------------\n",
        "fix_broken_import_concatenation()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Ensure MultiTeacherDistillationLoss import (safe line insertion)\n",
        "# Insert after: from losses import DistillationLoss\n",
        "# ------------------------------------------------------------\n",
        "ensure_line_after(\n",
        "    r\"^\\s*from\\s+losses\\s+import\\s+DistillationLoss\\s*$\",\n",
        "    \"from multiteacher_loss import MultiTeacherDistillationLoss\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Ensure CLI args after --teacher-path\n",
        "# ------------------------------------------------------------\n",
        "cli_block = \"\"\"\\\n",
        "    parser.add_argument('--teacher-models', type=str, default='',\n",
        "                        help='Comma-separated timm model names for multi-teacher distillation')\n",
        "    parser.add_argument('--hdtse-warmup-epochs', type=int, default=0,\n",
        "                        help='Use uniform teacher weights for first N epochs, then enable HDTSE weighting')\n",
        "    parser.add_argument('--lambda-log', action='store_true', default=False,\n",
        "                        help='Log mean λ (teacher weights) each epoch for multi-teacher distillation')\n",
        "\"\"\"\n",
        "ensure_block_after_line(r\"^\\s*parser\\.add_argument\\('--teacher-path'\", cli_block)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Allow finetune + distillation ONLY when multi-teacher is used\n",
        "# Base guard is:\n",
        "# if args.distillation_type != 'none' and args.finetune and not args.eval:\n",
        "#     raise NotImplementedError(...)\n",
        "# ------------------------------------------------------------\n",
        "replace_first(\n",
        "    r\"^\\s*if\\s+args\\.distillation_type\\s*!=\\s*'none'\\s+and\\s+args\\.finetune\\s+and\\s+not\\s+args\\.eval\\s*:\\s*\\n\\s*raise\\s+NotImplementedError\\([^\\n]*\\)\\s*$\",\n",
        "    \"    if args.distillation_type != 'none' and args.finetune and not args.eval and not getattr(args, 'teacher_models', ''):\\n\"\n",
        "    \"        raise NotImplementedError(\\\"Finetuning with distillation not yet supported (single-teacher path)\\\")\\n\",\n",
        "    flags=re.MULTILINE\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Move scheduler creation to AFTER adapter param-group add:\n",
        "# Remove early: lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "# ------------------------------------------------------------\n",
        "remove_first_line_matching(r\"^\\s*lr_scheduler,\\s*_\\s*=\\s*create_scheduler\\(\\s*args\\s*,\\s*optimizer\\s*\\)\\s*$\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Unify distillation region (multi-teacher vs single-teacher)\n",
        "# We'll replace from \"teacher_model = None\" up to \"output_dir = Path(args.output_dir)\"\n",
        "# This avoids indentation mistakes and prevents teacher_path='' crash.\n",
        "# ------------------------------------------------------------\n",
        "m_start = re.search(r\"^\\s*teacher_model\\s*=\\s*None\\s*$\", txt, flags=re.MULTILINE)\n",
        "m_end   = re.search(r\"^\\s*output_dir\\s*=\\s*Path\\(args\\.output_dir\\)\\s*$\", txt, flags=re.MULTILINE)\n",
        "if not (m_start and m_end and m_start.start() < m_end.start()):\n",
        "    raise RuntimeError(\"Could not locate distillation region anchors (teacher_model=None ... output_dir=Path(...))\")\n",
        "\n",
        "unified = \"\"\"\\\n",
        "    teacher_model = None\n",
        "\n",
        "    # -------------------------------\n",
        "    # Unified single + multi-teacher distillation\n",
        "    # -------------------------------\n",
        "    teacher_models_str = getattr(args, 'teacher_models', '').strip()\n",
        "\n",
        "    if args.distillation_type != 'none' and teacher_models_str:\n",
        "        teacher_names = [t.strip() for t in teacher_models_str.split(',') if t.strip()]\n",
        "        print(f\"✅ Multi-teacher distillation enabled. Teachers: {teacher_names}\")\n",
        "\n",
        "        criterion = MultiTeacherDistillationLoss(\n",
        "            base_criterion=criterion,\n",
        "            student_num_classes=args.nb_classes,\n",
        "            teacher_names=teacher_names,\n",
        "            distillation_type=args.distillation_type,\n",
        "            alpha=args.distillation_alpha,\n",
        "            tau=args.distillation_tau,\n",
        "            device=device,\n",
        "            use_adapter=True,\n",
        "            hdtse_warmup_epochs=getattr(args, 'hdtse_warmup_epochs', 0),\n",
        "            lambda_log=getattr(args, 'lambda_log', False),\n",
        "        )\n",
        "\n",
        "        # Initialize Tiny-ImageNet wnid -> ImageNet-1k index mapping for teacher logits\n",
        "        if hasattr(criterion, \"set_tiny_root\"):\n",
        "            criterion.set_tiny_root(args.data_path)\n",
        "\n",
        "        # Optional: alpha ramp if you add args later\n",
        "        if hasattr(criterion, \"set_alpha_schedule\") and hasattr(args, \"alpha_ramp_epochs\"):\n",
        "            criterion.set_alpha_schedule(\n",
        "                alpha_start=getattr(args, \"alpha_start\", 0.0),\n",
        "                alpha_ramp_epochs=getattr(args, \"alpha_ramp_epochs\", 20),\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        if args.distillation_type != 'none':\n",
        "            assert args.teacher_path, 'need to specify teacher-path when using single-teacher distillation'\n",
        "            print(f\"Creating teacher model: {args.teacher_model}\")\n",
        "            teacher_model = create_model(\n",
        "                args.teacher_model,\n",
        "                pretrained=False,\n",
        "                num_classes=args.nb_classes,\n",
        "                global_pool='avg',\n",
        "            )\n",
        "            if args.teacher_path.startswith('https'):\n",
        "                checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                    args.teacher_path, map_location='cpu', check_hash=True)\n",
        "            else:\n",
        "                checkpoint = torch.load(args.teacher_path, map_location='cpu')\n",
        "            teacher_model.load_state_dict(checkpoint['model'])\n",
        "            teacher_model.to(device)\n",
        "            teacher_model.eval()\n",
        "\n",
        "        criterion = DistillationLoss(\n",
        "            criterion, teacher_model, args.distillation_type, args.distillation_alpha, args.distillation_tau\n",
        "        )\n",
        "\n",
        "    # Scheduler must be created AFTER all optimizer param groups are finalized\n",
        "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "\"\"\"\n",
        "\n",
        "txt = txt[:m_start.start()] + unified + \"\\n    output_dir = Path(args.output_dir)\\n\" + txt[m_end.end():]\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Ensure loss call uses (samples, outputs, targets)\n",
        "# ----------------------------\n",
        "# Patch ONLY the simple 2-arg form if present.\n",
        "if \"criterion(samples, outputs, targets)\" not in txt:\n",
        "    txt = re.sub(\n",
        "        r\"loss\\s*=\\s*criterion\\(\\s*outputs\\s*,\\s*targets\\s*\\)\",\n",
        "        r\"loss = criterion(samples, outputs, targets)\",\n",
        "        txt\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Insert criterion.set_epoch(epoch) before train_one_epoch\n",
        "# We add it inside the epoch loop, after sampler.set_epoch if present.\n",
        "# ------------------------------------------------------------\n",
        "if \"criterion.set_epoch(epoch)\" not in txt:\n",
        "    # If distributed block exists, insert after it\n",
        "    if re.search(r\"^\\s*if\\s+args\\.distributed\\s*:\\s*\\n\\s*data_loader_train\\.sampler\\.set_epoch\\(epoch\\)\\s*$\", txt, flags=re.MULTILINE):\n",
        "        txt = re.sub(\n",
        "            r\"(^\\s*if\\s+args\\.distributed\\s*:\\s*\\n\\s*data_loader_train\\.sampler\\.set_epoch\\(epoch\\)\\s*$)\",\n",
        "            r\"\\1\\n        if hasattr(criterion, 'set_epoch'):\\n            criterion.set_epoch(epoch)\",\n",
        "            txt,\n",
        "            flags=re.MULTILINE,\n",
        "            count=1\n",
        "        )\n",
        "    else:\n",
        "        # Otherwise put at top of loop\n",
        "        txt = re.sub(\n",
        "            r\"(^\\s*for\\s+epoch\\s+in\\s+range\\(args\\.start_epoch,\\s*args\\.epochs\\)\\s*:\\s*$)\",\n",
        "            r\"\\1\\n        if hasattr(criterion, 'set_epoch'):\\n            criterion.set_epoch(epoch)\",\n",
        "            txt,\n",
        "            flags=re.MULTILINE,\n",
        "            count=1\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Per-epoch λ logging after train_one_epoch call\n",
        "# ------------------------------------------------------------\n",
        "if \"print('λ means:'\" not in txt:\n",
        "    txt = re.sub(\n",
        "        r\"(train_stats\\s*=\\s*train_one_epoch\\([\\s\\S]*?\\)\\s*)\\n\",\n",
        "        r\"\\1\\n\\n\"\n",
        "        r\"        # Optional: log mean λ per teacher (multi-teacher only)\\n\"\n",
        "        r\"        if getattr(args, 'lambda_log', False) and hasattr(criterion, 'pop_lambda_stats'):\\n\"\n",
        "        r\"            lambda_means = criterion.pop_lambda_stats()\\n\"\n",
        "        r\"            if lambda_means:\\n\"\n",
        "        r\"                print('λ means:', lambda_means)\\n\",\n",
        "        txt,\n",
        "        count=1\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Write + compile check\n",
        "# ------------------------------------------------------------\n",
        "MAIN.write_text(txt)\n",
        "py_compile.compile(str(MAIN), doraise=True)\n",
        "print(\"✅ Patched main.py written and compiles:\", MAIN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdJZ4F-NoE-",
        "outputId": "938180f3-44bd-4efd-a82c-91528ea20565"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched main.py written and compiles: /content/deit/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "b819777f-e5ff-4c90-b85f-ae619aa1e5d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "ff135192-7cb9-433d-ad3d-c6774be93f08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/imagenet_class_index.json\n",
        "!wget https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json \\\n",
        "  -O /content/imagenet_class_index.json\n",
        "\n",
        "!python - <<'PY'\n",
        "import json\n",
        "p=\"/content/imagenet_class_index.json\"\n",
        "with open(p,\"r\",encoding=\"utf-8\") as f:\n",
        "    obj=json.load(f)\n",
        "print(\"Loaded OK. Entries:\", len(obj))\n",
        "print(\"Example 0:\", obj[\"0\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAeUxFOQMFrE",
        "outputId": "cdf076ff-6798-4131-8ec5-00c1aeef405d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-08 18:14:47--  https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.12.82, 52.216.61.96, 16.15.177.142, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.12.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35363 (35K) [application/octet-stream]\n",
            "Saving to: ‘/content/imagenet_class_index.json’\n",
            "\n",
            "/content/imagenet_c 100%[===================>]  34.53K   159KB/s    in 0.2s    \n",
            "\n",
            "2026-02-08 18:14:48 (159 KB/s) - ‘/content/imagenet_class_index.json’ saved [35363/35363]\n",
            "\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "Loaded OK. Entries: 1000\n",
            "Example 0: ['n01440764', 'tench']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 100 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 3 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.6 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --hdtse-warmup-epochs 5 \\\n",
        " --lambda-log \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b2,mobilenetv3_large_100,regnety_040\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "a1a94ca6-1968-48f9-a2c1-edd44e7539f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=100, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.00025, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=3, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m6-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.1, remode='pixel', recount=1, resplit=False, mixup=0.2, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', teacher_models='tf_efficientnet_b2,mobilenetv3_large_100,regnety_040', hdtse_warmup_epochs=5, lambda_log=True, distillation_type='soft', distillation_alpha=0.6, distillation_tau=2.0, cosub=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100% 21.9M/21.9M [00:00<00:00, 199MB/s]\n",
            "number of params: 5717416\n",
            "/usr/local/lib/python3.12/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "✅ Multi-teacher distillation enabled. Teachers: ['tf_efficientnet_b2', 'mobilenetv3_large_100', 'regnety_040']\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b2_aa-60c94f97.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_large_100_ra-f55367f5.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tpu-weights/regnety_040_ra3-670e1166.pth\" to /root/.cache/torch/hub/checkpoints/regnety_040_ra3-670e1166.pth\n",
            "Start training for 100 epochs\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch: [0]  [  0/781]  eta: 3:40:54  lr: 0.000001  loss: 8.6686 (8.6686)  time: 16.9713  data: 0.9823  max mem: 4927\n",
            "Epoch: [0]  [ 10/781]  eta: 0:21:39  lr: 0.000001  loss: 8.5756 (8.5405)  time: 1.6856  data: 0.0896  max mem: 4927\n",
            "Epoch: [0]  [ 20/781]  eta: 0:12:01  lr: 0.000001  loss: 8.5035 (8.5056)  time: 0.1468  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 30/781]  eta: 0:08:35  lr: 0.000001  loss: 8.3766 (8.4383)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 40/781]  eta: 0:06:49  lr: 0.000001  loss: 8.3076 (8.3971)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 50/781]  eta: 0:05:44  lr: 0.000001  loss: 8.2300 (8.3563)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 60/781]  eta: 0:04:59  lr: 0.000001  loss: 8.1200 (8.3081)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 70/781]  eta: 0:04:27  lr: 0.000001  loss: 8.0406 (8.2603)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 80/781]  eta: 0:04:03  lr: 0.000001  loss: 7.8685 (8.1973)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [ 90/781]  eta: 0:03:43  lr: 0.000001  loss: 7.6678 (8.1377)  time: 0.1367  data: 0.0009  max mem: 4927\n",
            "Epoch: [0]  [100/781]  eta: 0:03:28  lr: 0.000001  loss: 7.5912 (8.0772)  time: 0.1444  data: 0.0077  max mem: 4927\n",
            "Epoch: [0]  [110/781]  eta: 0:03:16  lr: 0.000001  loss: 7.4599 (8.0152)  time: 0.1507  data: 0.0127  max mem: 4927\n",
            "Epoch: [0]  [120/781]  eta: 0:03:05  lr: 0.000001  loss: 7.3576 (7.9563)  time: 0.1461  data: 0.0063  max mem: 4927\n",
            "Epoch: [0]  [130/781]  eta: 0:02:55  lr: 0.000001  loss: 7.1716 (7.8983)  time: 0.1388  data: 0.0007  max mem: 4927\n",
            "Epoch: [0]  [140/781]  eta: 0:02:46  lr: 0.000001  loss: 7.1566 (7.8449)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [150/781]  eta: 0:02:38  lr: 0.000001  loss: 7.0932 (7.7917)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [160/781]  eta: 0:02:31  lr: 0.000001  loss: 7.0428 (7.7429)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [170/781]  eta: 0:02:25  lr: 0.000001  loss: 6.9470 (7.6940)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [180/781]  eta: 0:02:19  lr: 0.000001  loss: 6.8731 (7.6487)  time: 0.1375  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [190/781]  eta: 0:02:14  lr: 0.000001  loss: 6.8351 (7.6052)  time: 0.1383  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [200/781]  eta: 0:02:09  lr: 0.000001  loss: 6.7870 (7.5631)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [210/781]  eta: 0:02:05  lr: 0.000001  loss: 6.7549 (7.5257)  time: 0.1382  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [220/781]  eta: 0:02:00  lr: 0.000001  loss: 6.7549 (7.4891)  time: 0.1385  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [230/781]  eta: 0:01:56  lr: 0.000001  loss: 6.6794 (7.4526)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [240/781]  eta: 0:01:52  lr: 0.000001  loss: 6.6443 (7.4192)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [250/781]  eta: 0:01:49  lr: 0.000001  loss: 6.6240 (7.3864)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [260/781]  eta: 0:01:45  lr: 0.000001  loss: 6.5944 (7.3564)  time: 0.1378  data: 0.0004  max mem: 4927\n",
            "Epoch: [0]  [270/781]  eta: 0:01:42  lr: 0.000001  loss: 6.5677 (7.3276)  time: 0.1386  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [280/781]  eta: 0:01:39  lr: 0.000001  loss: 6.5606 (7.3005)  time: 0.1393  data: 0.0004  max mem: 4927\n",
            "Epoch: [0]  [290/781]  eta: 0:01:36  lr: 0.000001  loss: 6.5101 (7.2729)  time: 0.1382  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [300/781]  eta: 0:01:33  lr: 0.000001  loss: 6.4814 (7.2472)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [310/781]  eta: 0:01:30  lr: 0.000001  loss: 6.4947 (7.2233)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [320/781]  eta: 0:01:28  lr: 0.000001  loss: 6.4781 (7.1998)  time: 0.1343  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [330/781]  eta: 0:01:25  lr: 0.000001  loss: 6.4435 (7.1767)  time: 0.1344  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [340/781]  eta: 0:01:22  lr: 0.000001  loss: 6.4251 (7.1551)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [350/781]  eta: 0:01:20  lr: 0.000001  loss: 6.4148 (7.1334)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [360/781]  eta: 0:01:17  lr: 0.000001  loss: 6.3871 (7.1129)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [370/781]  eta: 0:01:15  lr: 0.000001  loss: 6.3978 (7.0937)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [380/781]  eta: 0:01:13  lr: 0.000001  loss: 6.3945 (7.0749)  time: 0.1361  data: 0.0017  max mem: 4927\n",
            "Epoch: [0]  [390/781]  eta: 0:01:10  lr: 0.000001  loss: 6.3945 (7.0576)  time: 0.1375  data: 0.0017  max mem: 4927\n",
            "Epoch: [0]  [400/781]  eta: 0:01:08  lr: 0.000001  loss: 6.3765 (7.0408)  time: 0.1391  data: 0.0023  max mem: 4927\n",
            "Epoch: [0]  [410/781]  eta: 0:01:06  lr: 0.000001  loss: 6.3765 (7.0248)  time: 0.1406  data: 0.0035  max mem: 4927\n",
            "Epoch: [0]  [420/781]  eta: 0:01:04  lr: 0.000001  loss: 6.3527 (7.0086)  time: 0.1384  data: 0.0015  max mem: 4927\n",
            "Epoch: [0]  [430/781]  eta: 0:01:02  lr: 0.000001  loss: 6.3381 (6.9926)  time: 0.1415  data: 0.0058  max mem: 4927\n",
            "Epoch: [0]  [440/781]  eta: 0:01:00  lr: 0.000001  loss: 6.3197 (6.9773)  time: 0.1422  data: 0.0067  max mem: 4927\n",
            "Epoch: [0]  [450/781]  eta: 0:00:58  lr: 0.000001  loss: 6.3196 (6.9629)  time: 0.1401  data: 0.0035  max mem: 4927\n",
            "Epoch: [0]  [460/781]  eta: 0:00:56  lr: 0.000001  loss: 6.3024 (6.9482)  time: 0.1404  data: 0.0026  max mem: 4927\n",
            "Epoch: [0]  [470/781]  eta: 0:00:54  lr: 0.000001  loss: 6.2889 (6.9340)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [480/781]  eta: 0:00:52  lr: 0.000001  loss: 6.2889 (6.9204)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [490/781]  eta: 0:00:50  lr: 0.000001  loss: 6.2888 (6.9078)  time: 0.1345  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [500/781]  eta: 0:00:48  lr: 0.000001  loss: 6.2888 (6.8948)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [510/781]  eta: 0:00:46  lr: 0.000001  loss: 6.2512 (6.8823)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [520/781]  eta: 0:00:44  lr: 0.000001  loss: 6.2412 (6.8699)  time: 0.1348  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [530/781]  eta: 0:00:42  lr: 0.000001  loss: 6.2446 (6.8585)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [540/781]  eta: 0:00:40  lr: 0.000001  loss: 6.2313 (6.8465)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [550/781]  eta: 0:00:38  lr: 0.000001  loss: 6.2142 (6.8350)  time: 0.1459  data: 0.0075  max mem: 4927\n",
            "Epoch: [0]  [560/781]  eta: 0:00:37  lr: 0.000001  loss: 6.2304 (6.8244)  time: 0.1462  data: 0.0090  max mem: 4927\n",
            "Epoch: [0]  [570/781]  eta: 0:00:35  lr: 0.000001  loss: 6.2206 (6.8137)  time: 0.1381  data: 0.0028  max mem: 4927\n",
            "Epoch: [0]  [580/781]  eta: 0:00:33  lr: 0.000001  loss: 6.1907 (6.8028)  time: 0.1384  data: 0.0035  max mem: 4927\n",
            "Epoch: [0]  [590/781]  eta: 0:00:31  lr: 0.000001  loss: 6.2062 (6.7930)  time: 0.1384  data: 0.0038  max mem: 4927\n",
            "Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000001  loss: 6.2155 (6.7829)  time: 0.1390  data: 0.0041  max mem: 4927\n",
            "Epoch: [0]  [610/781]  eta: 0:00:28  lr: 0.000001  loss: 6.1700 (6.7728)  time: 0.1425  data: 0.0075  max mem: 4927\n",
            "Epoch: [0]  [620/781]  eta: 0:00:26  lr: 0.000001  loss: 6.1627 (6.7632)  time: 0.1404  data: 0.0049  max mem: 4927\n",
            "Epoch: [0]  [630/781]  eta: 0:00:24  lr: 0.000001  loss: 6.1627 (6.7538)  time: 0.1403  data: 0.0022  max mem: 4927\n",
            "Epoch: [0]  [640/781]  eta: 0:00:23  lr: 0.000001  loss: 6.1707 (6.7446)  time: 0.1410  data: 0.0022  max mem: 4927\n",
            "Epoch: [0]  [650/781]  eta: 0:00:21  lr: 0.000001  loss: 6.1750 (6.7359)  time: 0.1396  data: 0.0028  max mem: 4927\n",
            "Epoch: [0]  [660/781]  eta: 0:00:19  lr: 0.000001  loss: 6.1583 (6.7270)  time: 0.1384  data: 0.0028  max mem: 4927\n",
            "Epoch: [0]  [670/781]  eta: 0:00:18  lr: 0.000001  loss: 6.1442 (6.7184)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [680/781]  eta: 0:00:16  lr: 0.000001  loss: 6.1529 (6.7101)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [690/781]  eta: 0:00:14  lr: 0.000001  loss: 6.1529 (6.7017)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000001  loss: 6.1130 (6.6934)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [710/781]  eta: 0:00:11  lr: 0.000001  loss: 6.1069 (6.6853)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [720/781]  eta: 0:00:09  lr: 0.000001  loss: 6.1427 (6.6780)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [730/781]  eta: 0:00:08  lr: 0.000001  loss: 6.1224 (6.6700)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000001  loss: 6.1141 (6.6627)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 6.1099 (6.6552)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000001  loss: 6.1006 (6.6481)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 6.1091 (6.6409)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 6.1103 (6.6339)  time: 0.1346  data: 0.0006  max mem: 4927\n",
            "Epoch: [0] Total time: 0:02:04 (0.1597 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 6.1103 (6.6339)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.33333146572113037, 'lambda_mobilenetv3_large_100': 0.33333146572113037, 'lambda_regnety_040': 0.33333146572113037}\n",
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Test:  [ 0/53]  eta: 0:00:52  loss: 6.2483 (6.2483)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 0.9985  data: 0.7818  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 5.6388 (5.6776)  acc1: 0.0000 (0.1894)  acc5: 2.0833 (2.7936)  time: 0.1613  data: 0.1137  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 5.6622 (5.7224)  acc1: 0.0000 (0.5704)  acc5: 2.6042 (3.4226)  time: 0.1016  data: 0.0708  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.7682 (5.7868)  acc1: 0.0000 (0.4704)  acc5: 2.6042 (3.1586)  time: 0.1205  data: 0.0895  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.7547 (5.7948)  acc1: 0.0000 (0.8511)  acc5: 3.1250 (3.8872)  time: 0.1147  data: 0.0837  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 5.9970 (5.8665)  acc1: 0.0000 (0.6944)  acc5: 0.0000 (3.1556)  time: 0.1229  data: 0.0921  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 6.0940 (5.8837)  acc1: 0.0000 (0.6800)  acc5: 0.0000 (3.0900)  time: 0.1142  data: 0.0838  max mem: 4927\n",
            "Test: Total time: 0:00:06 (0.1272 s / it)\n",
            "* Acc@1 0.680 Acc@5 3.090 loss 5.884\n",
            "Accuracy of the network on the 10000 test images: 0.7%\n",
            "Max accuracy: 0.68%\n",
            "Epoch: [1]  [  0/781]  eta: 0:12:09  lr: 0.000001  loss: 6.0629 (6.0629)  time: 0.9339  data: 0.7775  max mem: 4927\n",
            "Epoch: [1]  [ 10/781]  eta: 0:02:44  lr: 0.000001  loss: 5.9611 (5.9683)  time: 0.2135  data: 0.0766  max mem: 4927\n",
            "Epoch: [1]  [ 20/781]  eta: 0:02:17  lr: 0.000001  loss: 5.9561 (5.9606)  time: 0.1428  data: 0.0078  max mem: 4927\n",
            "Epoch: [1]  [ 30/781]  eta: 0:02:07  lr: 0.000001  loss: 5.9528 (5.9613)  time: 0.1459  data: 0.0097  max mem: 4927\n",
            "Epoch: [1]  [ 40/781]  eta: 0:02:01  lr: 0.000001  loss: 5.9528 (5.9593)  time: 0.1459  data: 0.0095  max mem: 4927\n",
            "Epoch: [1]  [ 50/781]  eta: 0:01:55  lr: 0.000001  loss: 5.9451 (5.9573)  time: 0.1396  data: 0.0046  max mem: 4927\n",
            "Epoch: [1]  [ 60/781]  eta: 0:01:52  lr: 0.000001  loss: 5.9411 (5.9508)  time: 0.1405  data: 0.0056  max mem: 4927\n",
            "Epoch: [1]  [ 70/781]  eta: 0:01:49  lr: 0.000001  loss: 5.9172 (5.9492)  time: 0.1455  data: 0.0099  max mem: 4927\n",
            "Epoch: [1]  [ 80/781]  eta: 0:01:46  lr: 0.000001  loss: 5.9232 (5.9478)  time: 0.1407  data: 0.0048  max mem: 4927\n",
            "Epoch: [1]  [ 90/781]  eta: 0:01:44  lr: 0.000001  loss: 5.9125 (5.9448)  time: 0.1403  data: 0.0045  max mem: 4927\n",
            "Epoch: [1]  [100/781]  eta: 0:01:42  lr: 0.000001  loss: 5.9314 (5.9452)  time: 0.1435  data: 0.0082  max mem: 4927\n",
            "Epoch: [1]  [110/781]  eta: 0:01:40  lr: 0.000001  loss: 5.9285 (5.9429)  time: 0.1398  data: 0.0042  max mem: 4927\n",
            "Epoch: [1]  [120/781]  eta: 0:01:37  lr: 0.000001  loss: 5.9226 (5.9414)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [130/781]  eta: 0:01:35  lr: 0.000001  loss: 5.9108 (5.9392)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [140/781]  eta: 0:01:34  lr: 0.000001  loss: 5.9060 (5.9372)  time: 0.1379  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [150/781]  eta: 0:01:32  lr: 0.000001  loss: 5.9145 (5.9369)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [1]  [160/781]  eta: 0:01:30  lr: 0.000001  loss: 5.9130 (5.9353)  time: 0.1368  data: 0.0004  max mem: 4927\n",
            "Epoch: [1]  [170/781]  eta: 0:01:28  lr: 0.000001  loss: 5.9024 (5.9336)  time: 0.1369  data: 0.0004  max mem: 4927\n",
            "Epoch: [1]  [180/781]  eta: 0:01:26  lr: 0.000001  loss: 5.8994 (5.9319)  time: 0.1372  data: 0.0004  max mem: 4927\n",
            "Epoch: [1]  [190/781]  eta: 0:01:25  lr: 0.000001  loss: 5.8994 (5.9301)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [200/781]  eta: 0:01:23  lr: 0.000001  loss: 5.8923 (5.9299)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [210/781]  eta: 0:01:21  lr: 0.000001  loss: 5.9176 (5.9292)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [220/781]  eta: 0:01:20  lr: 0.000001  loss: 5.8945 (5.9266)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [230/781]  eta: 0:01:18  lr: 0.000001  loss: 5.8836 (5.9246)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [240/781]  eta: 0:01:17  lr: 0.000001  loss: 5.8769 (5.9226)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [250/781]  eta: 0:01:15  lr: 0.000001  loss: 5.8671 (5.9197)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [260/781]  eta: 0:01:13  lr: 0.000001  loss: 5.8569 (5.9185)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [270/781]  eta: 0:01:12  lr: 0.000001  loss: 5.8927 (5.9178)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [280/781]  eta: 0:01:10  lr: 0.000001  loss: 5.8763 (5.9159)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [290/781]  eta: 0:01:09  lr: 0.000001  loss: 5.8581 (5.9139)  time: 0.1379  data: 0.0031  max mem: 4927\n",
            "Epoch: [1]  [300/781]  eta: 0:01:08  lr: 0.000001  loss: 5.8591 (5.9126)  time: 0.1412  data: 0.0056  max mem: 4927\n",
            "Epoch: [1]  [310/781]  eta: 0:01:06  lr: 0.000001  loss: 5.8591 (5.9106)  time: 0.1415  data: 0.0055  max mem: 4927\n",
            "Epoch: [1]  [320/781]  eta: 0:01:05  lr: 0.000001  loss: 5.8507 (5.9093)  time: 0.1389  data: 0.0034  max mem: 4927\n",
            "Epoch: [1]  [330/781]  eta: 0:01:03  lr: 0.000001  loss: 5.8471 (5.9075)  time: 0.1465  data: 0.0103  max mem: 4927\n",
            "Epoch: [1]  [340/781]  eta: 0:01:02  lr: 0.000001  loss: 5.8389 (5.9057)  time: 0.1502  data: 0.0135  max mem: 4927\n",
            "Epoch: [1]  [350/781]  eta: 0:01:01  lr: 0.000001  loss: 5.8439 (5.9042)  time: 0.1464  data: 0.0105  max mem: 4927\n",
            "Epoch: [1]  [360/781]  eta: 0:00:59  lr: 0.000001  loss: 5.8642 (5.9030)  time: 0.1426  data: 0.0075  max mem: 4927\n",
            "Epoch: [1]  [370/781]  eta: 0:00:58  lr: 0.000001  loss: 5.8574 (5.9014)  time: 0.1378  data: 0.0025  max mem: 4927\n",
            "Epoch: [1]  [380/781]  eta: 0:00:56  lr: 0.000001  loss: 5.8557 (5.9002)  time: 0.1380  data: 0.0018  max mem: 4927\n",
            "Epoch: [1]  [390/781]  eta: 0:00:55  lr: 0.000001  loss: 5.8664 (5.8994)  time: 0.1370  data: 0.0011  max mem: 4927\n",
            "Epoch: [1]  [400/781]  eta: 0:00:53  lr: 0.000001  loss: 5.8595 (5.8980)  time: 0.1360  data: 0.0010  max mem: 4927\n",
            "Epoch: [1]  [410/781]  eta: 0:00:52  lr: 0.000001  loss: 5.8467 (5.8966)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [420/781]  eta: 0:00:50  lr: 0.000001  loss: 5.8299 (5.8950)  time: 0.1384  data: 0.0016  max mem: 4927\n",
            "Epoch: [1]  [430/781]  eta: 0:00:49  lr: 0.000001  loss: 5.8261 (5.8939)  time: 0.1383  data: 0.0016  max mem: 4927\n",
            "Epoch: [1]  [440/781]  eta: 0:00:48  lr: 0.000001  loss: 5.8580 (5.8931)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [450/781]  eta: 0:00:46  lr: 0.000001  loss: 5.8622 (5.8924)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [460/781]  eta: 0:00:45  lr: 0.000001  loss: 5.8565 (5.8913)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [470/781]  eta: 0:00:43  lr: 0.000001  loss: 5.8489 (5.8903)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [480/781]  eta: 0:00:42  lr: 0.000001  loss: 5.8328 (5.8889)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [490/781]  eta: 0:00:40  lr: 0.000001  loss: 5.8391 (5.8879)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [500/781]  eta: 0:00:39  lr: 0.000001  loss: 5.8456 (5.8869)  time: 0.1376  data: 0.0005  max mem: 4927\n",
            "Epoch: [1]  [510/781]  eta: 0:00:38  lr: 0.000001  loss: 5.8076 (5.8854)  time: 0.1472  data: 0.0102  max mem: 4927\n",
            "Epoch: [1]  [520/781]  eta: 0:00:36  lr: 0.000001  loss: 5.8068 (5.8839)  time: 0.1490  data: 0.0133  max mem: 4927\n",
            "Epoch: [1]  [530/781]  eta: 0:00:35  lr: 0.000001  loss: 5.7981 (5.8822)  time: 0.1415  data: 0.0059  max mem: 4927\n",
            "Epoch: [1]  [540/781]  eta: 0:00:33  lr: 0.000001  loss: 5.7981 (5.8808)  time: 0.1422  data: 0.0065  max mem: 4927\n",
            "Epoch: [1]  [550/781]  eta: 0:00:32  lr: 0.000001  loss: 5.8033 (5.8799)  time: 0.1416  data: 0.0042  max mem: 4927\n",
            "Epoch: [1]  [560/781]  eta: 0:00:31  lr: 0.000001  loss: 5.7985 (5.8784)  time: 0.1387  data: 0.0018  max mem: 4927\n",
            "Epoch: [1]  [570/781]  eta: 0:00:29  lr: 0.000001  loss: 5.7860 (5.8766)  time: 0.1385  data: 0.0030  max mem: 4927\n",
            "Epoch: [1]  [580/781]  eta: 0:00:28  lr: 0.000001  loss: 5.7883 (5.8752)  time: 0.1452  data: 0.0103  max mem: 4927\n",
            "Epoch: [1]  [590/781]  eta: 0:00:26  lr: 0.000001  loss: 5.8066 (5.8743)  time: 0.1443  data: 0.0090  max mem: 4927\n",
            "Epoch: [1]  [600/781]  eta: 0:00:25  lr: 0.000001  loss: 5.8124 (5.8733)  time: 0.1457  data: 0.0103  max mem: 4927\n",
            "Epoch: [1]  [610/781]  eta: 0:00:24  lr: 0.000001  loss: 5.8090 (5.8723)  time: 0.1466  data: 0.0120  max mem: 4927\n",
            "Epoch: [1]  [620/781]  eta: 0:00:22  lr: 0.000001  loss: 5.7971 (5.8708)  time: 0.1386  data: 0.0040  max mem: 4927\n",
            "Epoch: [1]  [630/781]  eta: 0:00:21  lr: 0.000001  loss: 5.7870 (5.8698)  time: 0.1369  data: 0.0023  max mem: 4927\n",
            "Epoch: [1]  [640/781]  eta: 0:00:19  lr: 0.000001  loss: 5.7931 (5.8684)  time: 0.1348  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [650/781]  eta: 0:00:18  lr: 0.000001  loss: 5.7678 (5.8671)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [660/781]  eta: 0:00:17  lr: 0.000001  loss: 5.7678 (5.8663)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [670/781]  eta: 0:00:15  lr: 0.000001  loss: 5.8009 (5.8651)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [680/781]  eta: 0:00:14  lr: 0.000001  loss: 5.7824 (5.8639)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [690/781]  eta: 0:00:12  lr: 0.000001  loss: 5.7800 (5.8626)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [700/781]  eta: 0:00:11  lr: 0.000001  loss: 5.7785 (5.8616)  time: 0.1388  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [710/781]  eta: 0:00:09  lr: 0.000001  loss: 5.7905 (5.8608)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [720/781]  eta: 0:00:08  lr: 0.000001  loss: 5.7927 (5.8599)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [730/781]  eta: 0:00:07  lr: 0.000001  loss: 5.7875 (5.8588)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [740/781]  eta: 0:00:05  lr: 0.000001  loss: 5.7878 (5.8580)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [750/781]  eta: 0:00:04  lr: 0.000001  loss: 5.7644 (5.8566)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [760/781]  eta: 0:00:02  lr: 0.000001  loss: 5.7644 (5.8557)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [770/781]  eta: 0:00:01  lr: 0.000001  loss: 5.7735 (5.8546)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 5.7596 (5.8536)  time: 0.1379  data: 0.0006  max mem: 4927\n",
            "Epoch: [1] Total time: 0:01:49 (0.1401 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 5.7596 (5.8536)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.33333146572113037, 'lambda_mobilenetv3_large_100': 0.33333146572113037, 'lambda_regnety_040': 0.33333146572113037}\n",
            "Test:  [ 0/53]  eta: 0:00:36  loss: 5.8072 (5.8072)  acc1: 0.0000 (0.0000)  acc5: 1.0417 (1.0417)  time: 0.6875  data: 0.6564  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 5.4835 (5.5327)  acc1: 0.0000 (0.3788)  acc5: 2.0833 (3.0303)  time: 0.1677  data: 0.1370  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 5.4515 (5.5034)  acc1: 0.0000 (0.7688)  acc5: 2.6042 (3.9931)  time: 0.1318  data: 0.1011  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 5.5341 (5.5783)  acc1: 0.0000 (0.7224)  acc5: 2.0833 (3.7466)  time: 0.1291  data: 0.0984  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 5.5238 (5.5750)  acc1: 0.0000 (1.0417)  acc5: 2.0833 (4.5605)  time: 0.1297  data: 0.0990  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 5.5354 (5.5888)  acc1: 0.5208 (0.9293)  acc5: 3.1250 (4.1054)  time: 0.1297  data: 0.0990  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 5.6247 (5.6015)  acc1: 0.0000 (0.9300)  acc5: 2.0833 (4.1500)  time: 0.1091  data: 0.0793  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1341 s / it)\n",
            "* Acc@1 0.930 Acc@5 4.150 loss 5.601\n",
            "Accuracy of the network on the 10000 test images: 0.9%\n",
            "Max accuracy: 0.93%\n",
            "Epoch: [2]  [  0/781]  eta: 0:11:17  lr: 0.000022  loss: 5.6083 (5.6083)  time: 0.8674  data: 0.7176  max mem: 4927\n",
            "Epoch: [2]  [ 10/781]  eta: 0:02:39  lr: 0.000022  loss: 5.6403 (5.6420)  time: 0.2075  data: 0.0699  max mem: 4927\n",
            "Epoch: [2]  [ 20/781]  eta: 0:02:18  lr: 0.000022  loss: 5.6232 (5.6277)  time: 0.1471  data: 0.0104  max mem: 4927\n",
            "Epoch: [2]  [ 30/781]  eta: 0:02:08  lr: 0.000022  loss: 5.5942 (5.6154)  time: 0.1511  data: 0.0139  max mem: 4927\n",
            "Epoch: [2]  [ 40/781]  eta: 0:02:03  lr: 0.000022  loss: 5.5766 (5.6016)  time: 0.1511  data: 0.0153  max mem: 4927\n",
            "Epoch: [2]  [ 50/781]  eta: 0:01:58  lr: 0.000022  loss: 5.5467 (5.5882)  time: 0.1479  data: 0.0131  max mem: 4927\n",
            "Epoch: [2]  [ 60/781]  eta: 0:01:55  lr: 0.000022  loss: 5.5215 (5.5779)  time: 0.1471  data: 0.0122  max mem: 4927\n",
            "Epoch: [2]  [ 70/781]  eta: 0:01:52  lr: 0.000022  loss: 5.5049 (5.5668)  time: 0.1477  data: 0.0129  max mem: 4927\n",
            "Epoch: [2]  [ 80/781]  eta: 0:01:49  lr: 0.000022  loss: 5.4914 (5.5533)  time: 0.1425  data: 0.0076  max mem: 4927\n",
            "Epoch: [2]  [ 90/781]  eta: 0:01:46  lr: 0.000022  loss: 5.4303 (5.5398)  time: 0.1380  data: 0.0030  max mem: 4927\n",
            "Epoch: [2]  [100/781]  eta: 0:01:43  lr: 0.000022  loss: 5.4201 (5.5276)  time: 0.1391  data: 0.0036  max mem: 4927\n",
            "Epoch: [2]  [110/781]  eta: 0:01:42  lr: 0.000022  loss: 5.4049 (5.5133)  time: 0.1458  data: 0.0096  max mem: 4927\n",
            "Epoch: [2]  [120/781]  eta: 0:01:40  lr: 0.000022  loss: 5.3491 (5.4995)  time: 0.1504  data: 0.0144  max mem: 4927\n",
            "Epoch: [2]  [130/781]  eta: 0:01:38  lr: 0.000022  loss: 5.3163 (5.4878)  time: 0.1450  data: 0.0095  max mem: 4927\n",
            "Epoch: [2]  [140/781]  eta: 0:01:36  lr: 0.000022  loss: 5.3061 (5.4735)  time: 0.1421  data: 0.0069  max mem: 4927\n",
            "Epoch: [2]  [150/781]  eta: 0:01:34  lr: 0.000022  loss: 5.2869 (5.4610)  time: 0.1427  data: 0.0080  max mem: 4927\n",
            "Epoch: [2]  [160/781]  eta: 0:01:33  lr: 0.000022  loss: 5.2427 (5.4499)  time: 0.1433  data: 0.0083  max mem: 4927\n",
            "Epoch: [2]  [170/781]  eta: 0:01:31  lr: 0.000022  loss: 5.1913 (5.4352)  time: 0.1447  data: 0.0081  max mem: 4927\n",
            "Epoch: [2]  [180/781]  eta: 0:01:29  lr: 0.000022  loss: 5.1770 (5.4209)  time: 0.1449  data: 0.0087  max mem: 4927\n",
            "Epoch: [2]  [190/781]  eta: 0:01:28  lr: 0.000022  loss: 5.1324 (5.4024)  time: 0.1476  data: 0.0119  max mem: 4927\n",
            "Epoch: [2]  [200/781]  eta: 0:01:26  lr: 0.000022  loss: 5.0525 (5.3864)  time: 0.1472  data: 0.0103  max mem: 4927\n",
            "Epoch: [2]  [210/781]  eta: 0:01:24  lr: 0.000022  loss: 5.0479 (5.3717)  time: 0.1437  data: 0.0063  max mem: 4927\n",
            "Epoch: [2]  [220/781]  eta: 0:01:23  lr: 0.000022  loss: 5.0184 (5.3545)  time: 0.1421  data: 0.0056  max mem: 4927\n",
            "Epoch: [2]  [230/781]  eta: 0:01:21  lr: 0.000022  loss: 4.9756 (5.3373)  time: 0.1417  data: 0.0063  max mem: 4927\n",
            "Epoch: [2]  [240/781]  eta: 0:01:19  lr: 0.000022  loss: 4.9197 (5.3227)  time: 0.1398  data: 0.0042  max mem: 4927\n",
            "Epoch: [2]  [250/781]  eta: 0:01:18  lr: 0.000022  loss: 4.9197 (5.3086)  time: 0.1418  data: 0.0061  max mem: 4927\n",
            "Epoch: [2]  [260/781]  eta: 0:01:16  lr: 0.000022  loss: 4.9227 (5.2931)  time: 0.1424  data: 0.0064  max mem: 4927\n",
            "Epoch: [2]  [270/781]  eta: 0:01:15  lr: 0.000022  loss: 4.8484 (5.2745)  time: 0.1408  data: 0.0048  max mem: 4927\n",
            "Epoch: [2]  [280/781]  eta: 0:01:13  lr: 0.000022  loss: 4.7919 (5.2614)  time: 0.1497  data: 0.0137  max mem: 4927\n",
            "Epoch: [2]  [290/781]  eta: 0:01:12  lr: 0.000022  loss: 4.8249 (5.2473)  time: 0.1494  data: 0.0136  max mem: 4927\n",
            "Epoch: [2]  [300/781]  eta: 0:01:10  lr: 0.000022  loss: 4.7658 (5.2306)  time: 0.1408  data: 0.0056  max mem: 4927\n",
            "Epoch: [2]  [310/781]  eta: 0:01:09  lr: 0.000022  loss: 4.7096 (5.2149)  time: 0.1405  data: 0.0056  max mem: 4927\n",
            "Epoch: [2]  [320/781]  eta: 0:01:07  lr: 0.000022  loss: 4.6877 (5.1996)  time: 0.1417  data: 0.0070  max mem: 4927\n",
            "Epoch: [2]  [330/781]  eta: 0:01:06  lr: 0.000022  loss: 4.7170 (5.1879)  time: 0.1415  data: 0.0058  max mem: 4927\n",
            "Epoch: [2]  [340/781]  eta: 0:01:04  lr: 0.000022  loss: 4.6679 (5.1738)  time: 0.1393  data: 0.0033  max mem: 4927\n",
            "Epoch: [2]  [350/781]  eta: 0:01:03  lr: 0.000022  loss: 4.6565 (5.1607)  time: 0.1440  data: 0.0070  max mem: 4927\n",
            "Epoch: [2]  [360/781]  eta: 0:01:01  lr: 0.000022  loss: 4.6292 (5.1480)  time: 0.1469  data: 0.0094  max mem: 4927\n",
            "Epoch: [2]  [370/781]  eta: 0:01:00  lr: 0.000022  loss: 4.5606 (5.1322)  time: 0.1467  data: 0.0104  max mem: 4927\n",
            "Epoch: [2]  [380/781]  eta: 0:00:58  lr: 0.000022  loss: 4.6153 (5.1212)  time: 0.1462  data: 0.0103  max mem: 4927\n",
            "Epoch: [2]  [390/781]  eta: 0:00:57  lr: 0.000022  loss: 4.6577 (5.1127)  time: 0.1427  data: 0.0072  max mem: 4927\n",
            "Epoch: [2]  [400/781]  eta: 0:00:55  lr: 0.000022  loss: 4.6101 (5.1028)  time: 0.1426  data: 0.0078  max mem: 4927\n",
            "Epoch: [2]  [410/781]  eta: 0:00:54  lr: 0.000022  loss: 4.5402 (5.0884)  time: 0.1426  data: 0.0077  max mem: 4927\n",
            "Epoch: [2]  [420/781]  eta: 0:00:52  lr: 0.000022  loss: 4.5275 (5.0779)  time: 0.1439  data: 0.0087  max mem: 4927\n",
            "Epoch: [2]  [430/781]  eta: 0:00:51  lr: 0.000022  loss: 4.5072 (5.0666)  time: 0.1465  data: 0.0114  max mem: 4927\n",
            "Epoch: [2]  [440/781]  eta: 0:00:49  lr: 0.000022  loss: 4.4597 (5.0531)  time: 0.1425  data: 0.0072  max mem: 4927\n",
            "Epoch: [2]  [450/781]  eta: 0:00:48  lr: 0.000022  loss: 4.4382 (5.0420)  time: 0.1393  data: 0.0018  max mem: 4927\n",
            "Epoch: [2]  [460/781]  eta: 0:00:46  lr: 0.000022  loss: 4.4661 (5.0317)  time: 0.1466  data: 0.0087  max mem: 4927\n",
            "Epoch: [2]  [470/781]  eta: 0:00:45  lr: 0.000022  loss: 4.4060 (5.0197)  time: 0.1453  data: 0.0090  max mem: 4927\n",
            "Epoch: [2]  [480/781]  eta: 0:00:43  lr: 0.000022  loss: 4.4002 (5.0084)  time: 0.1392  data: 0.0026  max mem: 4927\n",
            "Epoch: [2]  [490/781]  eta: 0:00:42  lr: 0.000022  loss: 4.3715 (4.9961)  time: 0.1408  data: 0.0033  max mem: 4927\n",
            "Epoch: [2]  [500/781]  eta: 0:00:40  lr: 0.000022  loss: 4.3436 (4.9845)  time: 0.1427  data: 0.0056  max mem: 4927\n",
            "Epoch: [2]  [510/781]  eta: 0:00:39  lr: 0.000022  loss: 4.3556 (4.9712)  time: 0.1430  data: 0.0061  max mem: 4927\n",
            "Epoch: [2]  [520/781]  eta: 0:00:37  lr: 0.000022  loss: 4.2622 (4.9575)  time: 0.1398  data: 0.0035  max mem: 4927\n",
            "Epoch: [2]  [530/781]  eta: 0:00:36  lr: 0.000022  loss: 4.2477 (4.9457)  time: 0.1384  data: 0.0020  max mem: 4927\n",
            "Epoch: [2]  [540/781]  eta: 0:00:35  lr: 0.000022  loss: 4.2438 (4.9343)  time: 0.1500  data: 0.0031  max mem: 4927\n",
            "Epoch: [2]  [550/781]  eta: 0:00:33  lr: 0.000022  loss: 4.2089 (4.9237)  time: 0.1490  data: 0.0030  max mem: 4927\n",
            "Epoch: [2]  [560/781]  eta: 0:00:32  lr: 0.000022  loss: 4.2099 (4.9138)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [570/781]  eta: 0:00:30  lr: 0.000022  loss: 4.2099 (4.9015)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [580/781]  eta: 0:00:29  lr: 0.000022  loss: 4.1550 (4.8892)  time: 0.1344  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [590/781]  eta: 0:00:27  lr: 0.000022  loss: 4.2427 (4.8803)  time: 0.1345  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [600/781]  eta: 0:00:26  lr: 0.000022  loss: 4.2137 (4.8686)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [610/781]  eta: 0:00:24  lr: 0.000022  loss: 4.2143 (4.8619)  time: 0.1345  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [620/781]  eta: 0:00:23  lr: 0.000022  loss: 4.1994 (4.8515)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [630/781]  eta: 0:00:21  lr: 0.000022  loss: 4.0915 (4.8417)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [640/781]  eta: 0:00:20  lr: 0.000022  loss: 4.0915 (4.8334)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [650/781]  eta: 0:00:18  lr: 0.000022  loss: 4.0847 (4.8244)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [660/781]  eta: 0:00:17  lr: 0.000022  loss: 4.0190 (4.8117)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [670/781]  eta: 0:00:15  lr: 0.000022  loss: 4.0760 (4.8009)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [680/781]  eta: 0:00:14  lr: 0.000022  loss: 4.0760 (4.7921)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [690/781]  eta: 0:00:13  lr: 0.000022  loss: 4.0655 (4.7833)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [700/781]  eta: 0:00:11  lr: 0.000022  loss: 4.0698 (4.7744)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [710/781]  eta: 0:00:10  lr: 0.000022  loss: 4.0520 (4.7643)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [720/781]  eta: 0:00:08  lr: 0.000022  loss: 4.0443 (4.7542)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [730/781]  eta: 0:00:07  lr: 0.000022  loss: 4.0017 (4.7468)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [740/781]  eta: 0:00:05  lr: 0.000022  loss: 3.9642 (4.7367)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [750/781]  eta: 0:00:04  lr: 0.000022  loss: 3.9642 (4.7265)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [760/781]  eta: 0:00:02  lr: 0.000022  loss: 3.9789 (4.7177)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [770/781]  eta: 0:00:01  lr: 0.000022  loss: 3.9382 (4.7100)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000022  loss: 3.9295 (4.7045)  time: 0.1350  data: 0.0006  max mem: 4927\n",
            "Epoch: [2] Total time: 0:01:51 (0.1426 s / it)\n",
            "Averaged stats: lr: 0.000022  loss: 3.9295 (4.7045)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.33333146572113037, 'lambda_mobilenetv3_large_100': 0.33333146572113037, 'lambda_regnety_040': 0.33333146572113037}\n",
            "Test:  [ 0/53]  eta: 0:00:42  loss: 1.6710 (1.6710)  acc1: 59.3750 (59.3750)  acc5: 86.9792 (86.9792)  time: 0.8113  data: 0.7804  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 2.3783 (2.2972)  acc1: 51.0417 (49.1004)  acc5: 80.7292 (79.8295)  time: 0.1561  data: 0.1254  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 2.4220 (2.5601)  acc1: 39.5833 (44.5933)  acc5: 77.6042 (75.2976)  time: 0.1103  data: 0.0796  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 2.9403 (2.6692)  acc1: 40.6250 (44.6741)  acc5: 68.7500 (73.4879)  time: 0.1186  data: 0.0878  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 3.0069 (2.7793)  acc1: 39.0625 (42.8227)  acc5: 66.1458 (71.0747)  time: 0.1189  data: 0.0881  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 2.8627 (2.7558)  acc1: 42.7083 (43.7092)  acc5: 66.6667 (71.2112)  time: 0.1131  data: 0.0823  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 2.8627 (2.7548)  acc1: 42.7083 (43.7700)  acc5: 68.2292 (71.4500)  time: 0.1057  data: 0.0759  max mem: 4927\n",
            "Test: Total time: 0:00:06 (0.1256 s / it)\n",
            "* Acc@1 43.770 Acc@5 71.450 loss 2.755\n",
            "Accuracy of the network on the 10000 test images: 43.8%\n",
            "Max accuracy: 43.77%\n",
            "Epoch: [3]  [  0/781]  eta: 0:11:44  lr: 0.000042  loss: 3.6779 (3.6779)  time: 0.9018  data: 0.7598  max mem: 4927\n",
            "Epoch: [3]  [ 10/781]  eta: 0:02:41  lr: 0.000042  loss: 4.0033 (4.1519)  time: 0.2092  data: 0.0729  max mem: 4927\n",
            "Epoch: [3]  [ 20/781]  eta: 0:02:13  lr: 0.000042  loss: 3.8408 (4.0485)  time: 0.1394  data: 0.0040  max mem: 4927\n",
            "Epoch: [3]  [ 30/781]  eta: 0:02:02  lr: 0.000042  loss: 3.8172 (4.0328)  time: 0.1376  data: 0.0027  max mem: 4927\n",
            "Epoch: [3]  [ 40/781]  eta: 0:01:58  lr: 0.000042  loss: 3.8338 (4.0103)  time: 0.1424  data: 0.0074  max mem: 4927\n",
            "Epoch: [3]  [ 50/781]  eta: 0:01:53  lr: 0.000042  loss: 3.8213 (4.0119)  time: 0.1422  data: 0.0072  max mem: 4927\n",
            "Epoch: [3]  [ 60/781]  eta: 0:01:50  lr: 0.000042  loss: 3.8365 (4.0029)  time: 0.1415  data: 0.0054  max mem: 4927\n",
            "Epoch: [3]  [ 70/781]  eta: 0:01:48  lr: 0.000042  loss: 3.8263 (3.9713)  time: 0.1479  data: 0.0112  max mem: 4927\n",
            "Epoch: [3]  [ 80/781]  eta: 0:01:46  lr: 0.000042  loss: 3.6877 (3.9619)  time: 0.1442  data: 0.0077  max mem: 4927\n",
            "Epoch: [3]  [ 90/781]  eta: 0:01:43  lr: 0.000042  loss: 3.8894 (3.9622)  time: 0.1391  data: 0.0028  max mem: 4927\n",
            "Epoch: [3]  [100/781]  eta: 0:01:41  lr: 0.000042  loss: 3.8080 (3.9358)  time: 0.1416  data: 0.0054  max mem: 4927\n",
            "Epoch: [3]  [110/781]  eta: 0:01:40  lr: 0.000042  loss: 3.7182 (3.9185)  time: 0.1458  data: 0.0098  max mem: 4927\n",
            "Epoch: [3]  [120/781]  eta: 0:01:37  lr: 0.000042  loss: 3.7445 (3.9132)  time: 0.1425  data: 0.0078  max mem: 4927\n",
            "Epoch: [3]  [130/781]  eta: 0:01:35  lr: 0.000042  loss: 3.7231 (3.8999)  time: 0.1364  data: 0.0019  max mem: 4927\n",
            "Epoch: [3]  [140/781]  eta: 0:01:33  lr: 0.000042  loss: 3.6938 (3.8995)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [150/781]  eta: 0:01:32  lr: 0.000042  loss: 3.6884 (3.8906)  time: 0.1419  data: 0.0049  max mem: 4927\n",
            "Epoch: [3]  [160/781]  eta: 0:01:30  lr: 0.000042  loss: 3.6519 (3.8879)  time: 0.1414  data: 0.0049  max mem: 4927\n",
            "Epoch: [3]  [170/781]  eta: 0:01:28  lr: 0.000042  loss: 3.7067 (3.8829)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [180/781]  eta: 0:01:26  lr: 0.000042  loss: 3.6928 (3.8784)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [190/781]  eta: 0:01:25  lr: 0.000042  loss: 3.6873 (3.8772)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [200/781]  eta: 0:01:23  lr: 0.000042  loss: 3.7158 (3.8818)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [210/781]  eta: 0:01:21  lr: 0.000042  loss: 3.7158 (3.8779)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [220/781]  eta: 0:01:20  lr: 0.000042  loss: 3.6932 (3.8700)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [230/781]  eta: 0:01:18  lr: 0.000042  loss: 3.6748 (3.8721)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [240/781]  eta: 0:01:16  lr: 0.000042  loss: 3.6748 (3.8735)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [250/781]  eta: 0:01:15  lr: 0.000042  loss: 3.6029 (3.8674)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [260/781]  eta: 0:01:13  lr: 0.000042  loss: 3.6029 (3.8554)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [270/781]  eta: 0:01:12  lr: 0.000042  loss: 3.5922 (3.8491)  time: 0.1353  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [280/781]  eta: 0:01:10  lr: 0.000042  loss: 3.6170 (3.8480)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [290/781]  eta: 0:01:09  lr: 0.000042  loss: 3.5639 (3.8403)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [300/781]  eta: 0:01:07  lr: 0.000042  loss: 3.5353 (3.8353)  time: 0.1371  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [310/781]  eta: 0:01:06  lr: 0.000042  loss: 3.4719 (3.8241)  time: 0.1368  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [320/781]  eta: 0:01:04  lr: 0.000042  loss: 3.4417 (3.8160)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [330/781]  eta: 0:01:03  lr: 0.000042  loss: 3.4470 (3.8069)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [340/781]  eta: 0:01:01  lr: 0.000042  loss: 3.4557 (3.8004)  time: 0.1368  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [350/781]  eta: 0:01:00  lr: 0.000042  loss: 3.4594 (3.7952)  time: 0.1370  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [360/781]  eta: 0:00:59  lr: 0.000042  loss: 3.5023 (3.7896)  time: 0.1373  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [370/781]  eta: 0:00:57  lr: 0.000042  loss: 3.5016 (3.7884)  time: 0.1371  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [380/781]  eta: 0:00:56  lr: 0.000042  loss: 3.4507 (3.7795)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [390/781]  eta: 0:00:54  lr: 0.000042  loss: 3.4507 (3.7826)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [400/781]  eta: 0:00:53  lr: 0.000042  loss: 3.4413 (3.7807)  time: 0.1383  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [410/781]  eta: 0:00:51  lr: 0.000042  loss: 3.3995 (3.7717)  time: 0.1380  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [420/781]  eta: 0:00:50  lr: 0.000042  loss: 3.4272 (3.7637)  time: 0.1394  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [430/781]  eta: 0:00:49  lr: 0.000042  loss: 3.4728 (3.7585)  time: 0.1391  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [440/781]  eta: 0:00:47  lr: 0.000042  loss: 3.4066 (3.7512)  time: 0.1379  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [450/781]  eta: 0:00:46  lr: 0.000042  loss: 3.4066 (3.7450)  time: 0.1377  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [460/781]  eta: 0:00:44  lr: 0.000042  loss: 3.4200 (3.7389)  time: 0.1377  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [470/781]  eta: 0:00:43  lr: 0.000042  loss: 3.3875 (3.7363)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [480/781]  eta: 0:00:42  lr: 0.000042  loss: 3.3740 (3.7284)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [490/781]  eta: 0:00:40  lr: 0.000042  loss: 3.3648 (3.7216)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [500/781]  eta: 0:00:39  lr: 0.000042  loss: 3.3554 (3.7178)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [510/781]  eta: 0:00:37  lr: 0.000042  loss: 3.3104 (3.7118)  time: 0.1385  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [520/781]  eta: 0:00:36  lr: 0.000042  loss: 3.3104 (3.7096)  time: 0.1382  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [530/781]  eta: 0:00:34  lr: 0.000042  loss: 3.3253 (3.7036)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [540/781]  eta: 0:00:33  lr: 0.000042  loss: 3.2937 (3.6952)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [550/781]  eta: 0:00:32  lr: 0.000042  loss: 3.2757 (3.6971)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [560/781]  eta: 0:00:30  lr: 0.000042  loss: 3.4065 (3.6914)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [570/781]  eta: 0:00:29  lr: 0.000042  loss: 3.3155 (3.6861)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [580/781]  eta: 0:00:27  lr: 0.000042  loss: 3.2731 (3.6841)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [590/781]  eta: 0:00:26  lr: 0.000042  loss: 3.3835 (3.6812)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [600/781]  eta: 0:00:25  lr: 0.000042  loss: 3.3835 (3.6778)  time: 0.1377  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [610/781]  eta: 0:00:23  lr: 0.000042  loss: 3.3906 (3.6773)  time: 0.1376  data: 0.0004  max mem: 4927\n",
            "Epoch: [3]  [620/781]  eta: 0:00:22  lr: 0.000042  loss: 3.2924 (3.6690)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [630/781]  eta: 0:00:20  lr: 0.000042  loss: 3.2547 (3.6662)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [640/781]  eta: 0:00:19  lr: 0.000042  loss: 3.2807 (3.6595)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [650/781]  eta: 0:00:18  lr: 0.000042  loss: 3.2810 (3.6590)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [660/781]  eta: 0:00:16  lr: 0.000042  loss: 3.3427 (3.6554)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [670/781]  eta: 0:00:15  lr: 0.000042  loss: 3.3737 (3.6552)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [680/781]  eta: 0:00:14  lr: 0.000042  loss: 3.3436 (3.6492)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [690/781]  eta: 0:00:12  lr: 0.000042  loss: 3.2984 (3.6448)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [700/781]  eta: 0:00:11  lr: 0.000042  loss: 3.2248 (3.6385)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [710/781]  eta: 0:00:09  lr: 0.000042  loss: 3.2681 (3.6380)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [720/781]  eta: 0:00:08  lr: 0.000042  loss: 3.3614 (3.6366)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [730/781]  eta: 0:00:07  lr: 0.000042  loss: 3.2892 (3.6311)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [740/781]  eta: 0:00:05  lr: 0.000042  loss: 3.2297 (3.6285)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [750/781]  eta: 0:00:04  lr: 0.000042  loss: 3.2391 (3.6254)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [760/781]  eta: 0:00:02  lr: 0.000042  loss: 3.2320 (3.6222)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [770/781]  eta: 0:00:01  lr: 0.000042  loss: 3.1896 (3.6183)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000042  loss: 3.2189 (3.6151)  time: 0.1358  data: 0.0007  max mem: 4927\n",
            "Epoch: [3] Total time: 0:01:48 (0.1385 s / it)\n",
            "Averaged stats: lr: 0.000042  loss: 3.2189 (3.6151)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.33333146572113037, 'lambda_mobilenetv3_large_100': 0.33333146572113037, 'lambda_regnety_040': 0.33333146572113037}\n",
            "Test:  [ 0/53]  eta: 0:00:45  loss: 1.0366 (1.0366)  acc1: 76.0417 (76.0417)  acc5: 92.1875 (92.1875)  time: 0.8529  data: 0.8219  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.5917 (1.4728)  acc1: 62.5000 (66.0511)  acc5: 89.0625 (88.4470)  time: 0.1777  data: 0.1470  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.6873 (1.6402)  acc1: 60.9375 (63.2937)  acc5: 85.9375 (85.7887)  time: 0.1259  data: 0.0952  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.9131 (1.7052)  acc1: 61.9792 (62.9872)  acc5: 82.2917 (85.0638)  time: 0.1261  data: 0.0954  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.9324 (1.7983)  acc1: 58.8542 (61.0518)  acc5: 81.2500 (83.6763)  time: 0.1264  data: 0.0956  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.8164 (1.7913)  acc1: 57.8125 (61.0805)  acc5: 82.2917 (83.7316)  time: 0.1255  data: 0.0947  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.8164 (1.8013)  acc1: 56.7708 (60.9300)  acc5: 82.2917 (83.8000)  time: 0.1081  data: 0.0783  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1336 s / it)\n",
            "* Acc@1 60.930 Acc@5 83.800 loss 1.801\n",
            "Accuracy of the network on the 10000 test images: 60.9%\n",
            "Max accuracy: 60.93%\n",
            "Epoch: [4]  [  0/781]  eta: 0:11:35  lr: 0.000062  loss: 3.1930 (3.1930)  time: 0.8910  data: 0.7345  max mem: 4927\n",
            "Epoch: [4]  [ 10/781]  eta: 0:02:42  lr: 0.000062  loss: 3.2594 (3.2810)  time: 0.2113  data: 0.0723  max mem: 4927\n",
            "Epoch: [4]  [ 20/781]  eta: 0:02:19  lr: 0.000062  loss: 3.2594 (3.3166)  time: 0.1475  data: 0.0107  max mem: 4927\n",
            "Epoch: [4]  [ 30/781]  eta: 0:02:07  lr: 0.000062  loss: 3.1717 (3.2791)  time: 0.1462  data: 0.0100  max mem: 4927\n",
            "Epoch: [4]  [ 40/781]  eta: 0:02:02  lr: 0.000062  loss: 3.0820 (3.2298)  time: 0.1463  data: 0.0094  max mem: 4927\n",
            "Epoch: [4]  [ 50/781]  eta: 0:01:56  lr: 0.000062  loss: 3.1036 (3.2342)  time: 0.1454  data: 0.0072  max mem: 4927\n",
            "Epoch: [4]  [ 60/781]  eta: 0:01:52  lr: 0.000062  loss: 3.2091 (3.2605)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [ 70/781]  eta: 0:01:49  lr: 0.000062  loss: 3.2128 (3.2752)  time: 0.1373  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [ 80/781]  eta: 0:01:46  lr: 0.000062  loss: 3.2127 (3.2874)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [ 90/781]  eta: 0:01:43  lr: 0.000062  loss: 3.1968 (3.2886)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [100/781]  eta: 0:01:41  lr: 0.000062  loss: 3.1626 (3.2743)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [110/781]  eta: 0:01:38  lr: 0.000062  loss: 3.1530 (3.2672)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [120/781]  eta: 0:01:36  lr: 0.000062  loss: 3.0872 (3.2521)  time: 0.1377  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [130/781]  eta: 0:01:35  lr: 0.000062  loss: 3.1550 (3.2475)  time: 0.1397  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [140/781]  eta: 0:01:33  lr: 0.000062  loss: 3.2102 (3.2504)  time: 0.1394  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [150/781]  eta: 0:01:31  lr: 0.000062  loss: 3.1229 (3.2613)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [160/781]  eta: 0:01:29  lr: 0.000062  loss: 3.1092 (3.2533)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [170/781]  eta: 0:01:27  lr: 0.000062  loss: 3.0967 (3.2536)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [180/781]  eta: 0:01:26  lr: 0.000062  loss: 3.1399 (3.2532)  time: 0.1376  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [190/781]  eta: 0:01:24  lr: 0.000062  loss: 3.1148 (3.2453)  time: 0.1381  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [200/781]  eta: 0:01:23  lr: 0.000062  loss: 3.1085 (3.2459)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [210/781]  eta: 0:01:21  lr: 0.000062  loss: 3.1286 (3.2485)  time: 0.1379  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [220/781]  eta: 0:01:19  lr: 0.000062  loss: 3.1569 (3.2584)  time: 0.1377  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [230/781]  eta: 0:01:18  lr: 0.000062  loss: 3.1439 (3.2660)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [240/781]  eta: 0:01:16  lr: 0.000062  loss: 3.1338 (3.2615)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [250/781]  eta: 0:01:15  lr: 0.000062  loss: 3.0514 (3.2546)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [260/781]  eta: 0:01:13  lr: 0.000062  loss: 3.0713 (3.2549)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [270/781]  eta: 0:01:12  lr: 0.000062  loss: 3.0637 (3.2486)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [280/781]  eta: 0:01:10  lr: 0.000062  loss: 3.0452 (3.2438)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [290/781]  eta: 0:01:09  lr: 0.000062  loss: 3.1220 (3.2423)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [300/781]  eta: 0:01:07  lr: 0.000062  loss: 3.1220 (3.2437)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [310/781]  eta: 0:01:06  lr: 0.000062  loss: 3.1509 (3.2448)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [320/781]  eta: 0:01:04  lr: 0.000062  loss: 3.0644 (3.2429)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [330/781]  eta: 0:01:03  lr: 0.000062  loss: 2.9903 (3.2390)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [340/781]  eta: 0:01:01  lr: 0.000062  loss: 2.9908 (3.2385)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [350/781]  eta: 0:01:00  lr: 0.000062  loss: 3.0461 (3.2328)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [360/781]  eta: 0:00:58  lr: 0.000062  loss: 3.0461 (3.2308)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [370/781]  eta: 0:00:57  lr: 0.000062  loss: 3.0571 (3.2299)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [380/781]  eta: 0:00:56  lr: 0.000062  loss: 3.0617 (3.2383)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [390/781]  eta: 0:00:54  lr: 0.000062  loss: 3.0646 (3.2333)  time: 0.1366  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [400/781]  eta: 0:00:53  lr: 0.000062  loss: 3.0031 (3.2314)  time: 0.1362  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [410/781]  eta: 0:00:51  lr: 0.000062  loss: 3.0066 (3.2316)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [420/781]  eta: 0:00:50  lr: 0.000062  loss: 3.0869 (3.2351)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [430/781]  eta: 0:00:48  lr: 0.000062  loss: 3.0703 (3.2299)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [440/781]  eta: 0:00:47  lr: 0.000062  loss: 3.0231 (3.2276)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [450/781]  eta: 0:00:46  lr: 0.000062  loss: 2.9977 (3.2320)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [460/781]  eta: 0:00:44  lr: 0.000062  loss: 3.0677 (3.2313)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [470/781]  eta: 0:00:43  lr: 0.000062  loss: 3.0677 (3.2295)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [480/781]  eta: 0:00:41  lr: 0.000062  loss: 3.0540 (3.2265)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [490/781]  eta: 0:00:40  lr: 0.000062  loss: 2.9804 (3.2230)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [500/781]  eta: 0:00:39  lr: 0.000062  loss: 2.9428 (3.2177)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [510/781]  eta: 0:00:37  lr: 0.000062  loss: 3.0887 (3.2210)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [520/781]  eta: 0:00:36  lr: 0.000062  loss: 3.0523 (3.2169)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [530/781]  eta: 0:00:34  lr: 0.000062  loss: 2.9891 (3.2169)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [540/781]  eta: 0:00:33  lr: 0.000062  loss: 2.9963 (3.2146)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [550/781]  eta: 0:00:31  lr: 0.000062  loss: 3.0517 (3.2116)  time: 0.1360  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [560/781]  eta: 0:00:30  lr: 0.000062  loss: 3.0517 (3.2150)  time: 0.1375  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [570/781]  eta: 0:00:29  lr: 0.000062  loss: 2.9925 (3.2137)  time: 0.1383  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [580/781]  eta: 0:00:27  lr: 0.000062  loss: 2.9925 (3.2136)  time: 0.1371  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [590/781]  eta: 0:00:26  lr: 0.000062  loss: 3.1036 (3.2176)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [600/781]  eta: 0:00:25  lr: 0.000062  loss: 3.0591 (3.2163)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [610/781]  eta: 0:00:23  lr: 0.000062  loss: 2.9880 (3.2130)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [620/781]  eta: 0:00:22  lr: 0.000062  loss: 2.9694 (3.2107)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [630/781]  eta: 0:00:20  lr: 0.000062  loss: 3.0027 (3.2138)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [640/781]  eta: 0:00:19  lr: 0.000062  loss: 3.0024 (3.2143)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [650/781]  eta: 0:00:18  lr: 0.000062  loss: 2.9842 (3.2140)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [660/781]  eta: 0:00:16  lr: 0.000062  loss: 2.9338 (3.2115)  time: 0.1388  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [670/781]  eta: 0:00:15  lr: 0.000062  loss: 2.9666 (3.2112)  time: 0.1394  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [680/781]  eta: 0:00:13  lr: 0.000062  loss: 3.0990 (3.2113)  time: 0.1378  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [690/781]  eta: 0:00:12  lr: 0.000062  loss: 3.0990 (3.2100)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [700/781]  eta: 0:00:11  lr: 0.000062  loss: 3.0711 (3.2105)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [710/781]  eta: 0:00:09  lr: 0.000062  loss: 2.9925 (3.2081)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [720/781]  eta: 0:00:08  lr: 0.000062  loss: 2.9560 (3.2055)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [730/781]  eta: 0:00:07  lr: 0.000062  loss: 2.9445 (3.2031)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [4]  [740/781]  eta: 0:00:05  lr: 0.000062  loss: 2.9500 (3.2041)  time: 0.1362  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [750/781]  eta: 0:00:04  lr: 0.000062  loss: 2.9718 (3.2027)  time: 0.1368  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [760/781]  eta: 0:00:02  lr: 0.000062  loss: 2.9029 (3.2033)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [770/781]  eta: 0:00:01  lr: 0.000062  loss: 2.8701 (3.1992)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000062  loss: 3.0064 (3.2044)  time: 0.1367  data: 0.0006  max mem: 4927\n",
            "Epoch: [4] Total time: 0:01:47 (0.1382 s / it)\n",
            "Averaged stats: lr: 0.000062  loss: 3.0064 (3.2044)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.33333146572113037, 'lambda_mobilenetv3_large_100': 0.33333146572113037, 'lambda_regnety_040': 0.33333146572113037}\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.1345 (1.1345)  acc1: 74.4792 (74.4792)  acc5: 93.2292 (93.2292)  time: 0.8455  data: 0.8146  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.4646 (1.4290)  acc1: 68.7500 (67.7083)  acc5: 89.0625 (88.9678)  time: 0.1730  data: 0.1423  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.5270 (1.5072)  acc1: 66.6667 (67.0387)  acc5: 87.5000 (87.2520)  time: 0.1292  data: 0.0985  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.6064 (1.5282)  acc1: 66.1458 (66.9187)  acc5: 86.4583 (87.2144)  time: 0.1323  data: 0.1016  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.7147 (1.5936)  acc1: 62.5000 (65.3836)  acc5: 84.8958 (86.3440)  time: 0.1320  data: 0.1013  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.6326 (1.5794)  acc1: 62.5000 (65.6965)  acc5: 85.9375 (86.4481)  time: 0.1294  data: 0.0987  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.6047 (1.5821)  acc1: 62.5000 (65.6300)  acc5: 85.9375 (86.5100)  time: 0.1081  data: 0.0783  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1364 s / it)\n",
            "* Acc@1 65.630 Acc@5 86.510 loss 1.582\n",
            "Accuracy of the network on the 10000 test images: 65.6%\n",
            "Max accuracy: 65.63%\n",
            "Epoch: [5]  [  0/781]  eta: 0:11:50  lr: 0.000062  loss: 2.8727 (2.8727)  time: 0.9093  data: 0.7339  max mem: 4927\n",
            "Epoch: [5]  [ 10/781]  eta: 0:02:42  lr: 0.000062  loss: 3.0041 (3.2406)  time: 0.2110  data: 0.0703  max mem: 4927\n",
            "Epoch: [5]  [ 20/781]  eta: 0:02:17  lr: 0.000062  loss: 2.9240 (3.1235)  time: 0.1448  data: 0.0077  max mem: 4927\n",
            "Epoch: [5]  [ 30/781]  eta: 0:02:05  lr: 0.000062  loss: 2.8777 (3.0692)  time: 0.1419  data: 0.0059  max mem: 4927\n",
            "Epoch: [5]  [ 40/781]  eta: 0:01:57  lr: 0.000062  loss: 2.7958 (3.0017)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [ 50/781]  eta: 0:01:53  lr: 0.000062  loss: 2.7958 (3.0022)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [ 60/781]  eta: 0:01:49  lr: 0.000062  loss: 2.8829 (3.0294)  time: 0.1378  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [ 70/781]  eta: 0:01:46  lr: 0.000062  loss: 2.9465 (3.0855)  time: 0.1376  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [ 80/781]  eta: 0:01:43  lr: 0.000062  loss: 2.8724 (3.0657)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [ 90/781]  eta: 0:01:41  lr: 0.000062  loss: 2.8595 (3.0750)  time: 0.1379  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [100/781]  eta: 0:01:39  lr: 0.000062  loss: 2.8437 (3.0583)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [110/781]  eta: 0:01:37  lr: 0.000062  loss: 2.8599 (3.0729)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [120/781]  eta: 0:01:35  lr: 0.000062  loss: 2.8599 (3.0671)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [130/781]  eta: 0:01:33  lr: 0.000062  loss: 2.8389 (3.0871)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [140/781]  eta: 0:01:31  lr: 0.000062  loss: 2.9073 (3.0928)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [150/781]  eta: 0:01:29  lr: 0.000062  loss: 2.8890 (3.0965)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [160/781]  eta: 0:01:28  lr: 0.000062  loss: 2.8398 (3.0813)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [170/781]  eta: 0:01:26  lr: 0.000062  loss: 2.8477 (3.0694)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [180/781]  eta: 0:01:25  lr: 0.000062  loss: 2.8650 (3.0697)  time: 0.1373  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [190/781]  eta: 0:01:23  lr: 0.000062  loss: 2.9654 (3.0739)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [200/781]  eta: 0:01:21  lr: 0.000062  loss: 2.8354 (3.0700)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [210/781]  eta: 0:01:20  lr: 0.000062  loss: 2.8630 (3.0765)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [220/781]  eta: 0:01:18  lr: 0.000062  loss: 2.8885 (3.0705)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [230/781]  eta: 0:01:17  lr: 0.000062  loss: 2.8694 (3.0694)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [240/781]  eta: 0:01:15  lr: 0.000062  loss: 2.8236 (3.0662)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [250/781]  eta: 0:01:14  lr: 0.000062  loss: 2.8134 (3.0565)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [260/781]  eta: 0:01:12  lr: 0.000062  loss: 2.8325 (3.0603)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [270/781]  eta: 0:01:11  lr: 0.000062  loss: 2.8580 (3.0568)  time: 0.1374  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [280/781]  eta: 0:01:09  lr: 0.000062  loss: 2.7808 (3.0467)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [290/781]  eta: 0:01:08  lr: 0.000062  loss: 2.8035 (3.0487)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [300/781]  eta: 0:01:07  lr: 0.000062  loss: 2.8177 (3.0490)  time: 0.1346  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [310/781]  eta: 0:01:05  lr: 0.000062  loss: 2.8149 (3.0472)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [320/781]  eta: 0:01:04  lr: 0.000062  loss: 2.9035 (3.0491)  time: 0.1458  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [330/781]  eta: 0:01:02  lr: 0.000062  loss: 2.8748 (3.0458)  time: 0.1461  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [340/781]  eta: 0:01:01  lr: 0.000062  loss: 2.8711 (3.0512)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [350/781]  eta: 0:01:00  lr: 0.000062  loss: 2.8010 (3.0430)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [360/781]  eta: 0:00:58  lr: 0.000062  loss: 2.7921 (3.0428)  time: 0.1369  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [370/781]  eta: 0:00:57  lr: 0.000062  loss: 2.7728 (3.0394)  time: 0.1375  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [380/781]  eta: 0:00:55  lr: 0.000062  loss: 2.7528 (3.0343)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [390/781]  eta: 0:00:54  lr: 0.000062  loss: 2.8035 (3.0391)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [400/781]  eta: 0:00:52  lr: 0.000062  loss: 2.8614 (3.0348)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [410/781]  eta: 0:00:51  lr: 0.000062  loss: 2.8581 (3.0351)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [420/781]  eta: 0:00:50  lr: 0.000062  loss: 2.9106 (3.0348)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [430/781]  eta: 0:00:48  lr: 0.000062  loss: 2.8017 (3.0299)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [440/781]  eta: 0:00:47  lr: 0.000062  loss: 2.8036 (3.0332)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [450/781]  eta: 0:00:45  lr: 0.000062  loss: 2.8644 (3.0284)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [460/781]  eta: 0:00:44  lr: 0.000062  loss: 2.7902 (3.0238)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [470/781]  eta: 0:00:43  lr: 0.000062  loss: 2.7674 (3.0217)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [480/781]  eta: 0:00:41  lr: 0.000062  loss: 2.7509 (3.0219)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [490/781]  eta: 0:00:40  lr: 0.000062  loss: 2.8233 (3.0220)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [500/781]  eta: 0:00:38  lr: 0.000062  loss: 2.8887 (3.0187)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [510/781]  eta: 0:00:37  lr: 0.000062  loss: 2.8345 (3.0145)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [520/781]  eta: 0:00:36  lr: 0.000062  loss: 2.8107 (3.0121)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [530/781]  eta: 0:00:34  lr: 0.000062  loss: 2.8157 (3.0129)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [540/781]  eta: 0:00:33  lr: 0.000062  loss: 2.7883 (3.0109)  time: 0.1376  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [550/781]  eta: 0:00:31  lr: 0.000062  loss: 2.7823 (3.0095)  time: 0.1391  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [560/781]  eta: 0:00:30  lr: 0.000062  loss: 2.8121 (3.0075)  time: 0.1381  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [570/781]  eta: 0:00:29  lr: 0.000062  loss: 2.7855 (3.0041)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [580/781]  eta: 0:00:27  lr: 0.000062  loss: 2.7974 (3.0050)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [590/781]  eta: 0:00:26  lr: 0.000062  loss: 2.8051 (3.0035)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [600/781]  eta: 0:00:25  lr: 0.000062  loss: 2.8039 (3.0047)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [610/781]  eta: 0:00:23  lr: 0.000062  loss: 2.7828 (3.0042)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [620/781]  eta: 0:00:22  lr: 0.000062  loss: 2.7157 (3.0048)  time: 0.1379  data: 0.0004  max mem: 4927\n",
            "Epoch: [5]  [630/781]  eta: 0:00:20  lr: 0.000062  loss: 2.7157 (3.0024)  time: 0.1373  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [640/781]  eta: 0:00:19  lr: 0.000062  loss: 2.7250 (2.9991)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [650/781]  eta: 0:00:18  lr: 0.000062  loss: 2.8315 (3.0039)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [660/781]  eta: 0:00:16  lr: 0.000062  loss: 2.8315 (3.0009)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [670/781]  eta: 0:00:15  lr: 0.000062  loss: 2.7483 (2.9974)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [680/781]  eta: 0:00:13  lr: 0.000062  loss: 2.6882 (2.9938)  time: 0.1348  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [690/781]  eta: 0:00:12  lr: 0.000062  loss: 2.6882 (2.9938)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [700/781]  eta: 0:00:11  lr: 0.000062  loss: 2.7293 (2.9915)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [710/781]  eta: 0:00:09  lr: 0.000062  loss: 2.8387 (2.9911)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [720/781]  eta: 0:00:08  lr: 0.000062  loss: 2.9252 (2.9923)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [730/781]  eta: 0:00:07  lr: 0.000062  loss: 2.8664 (2.9934)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [740/781]  eta: 0:00:05  lr: 0.000062  loss: 2.7338 (2.9935)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [750/781]  eta: 0:00:04  lr: 0.000062  loss: 2.8135 (2.9937)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [760/781]  eta: 0:00:02  lr: 0.000062  loss: 2.6504 (2.9934)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [770/781]  eta: 0:00:01  lr: 0.000062  loss: 2.6927 (2.9956)  time: 0.1384  data: 0.0003  max mem: 4927\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000062  loss: 2.7658 (2.9946)  time: 0.1376  data: 0.0007  max mem: 4927\n",
            "Epoch: [5] Total time: 0:01:47 (0.1379 s / it)\n",
            "Averaged stats: lr: 0.000062  loss: 2.7658 (2.9946)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.32180917263031006, 'lambda_mobilenetv3_large_100': 0.30253738164901733, 'lambda_regnety_040': 0.3756534159183502}\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.0383 (1.0383)  acc1: 76.0417 (76.0417)  acc5: 93.7500 (93.7500)  time: 0.8402  data: 0.8092  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2055 (1.1463)  acc1: 73.9583 (73.5795)  acc5: 93.7500 (92.3769)  time: 0.1756  data: 0.1449  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2714 (1.2859)  acc1: 67.1875 (71.0565)  acc5: 90.6250 (90.0546)  time: 0.1304  data: 0.0996  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.4110 (1.3217)  acc1: 66.6667 (70.7157)  acc5: 88.0208 (89.7177)  time: 0.1340  data: 0.1032  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4110 (1.3845)  acc1: 69.2708 (69.2200)  acc5: 88.0208 (88.9609)  time: 0.1334  data: 0.1027  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4553 (1.3985)  acc1: 66.1458 (68.7806)  acc5: 86.9792 (88.6949)  time: 0.1326  data: 0.1018  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.5631 (1.4150)  acc1: 61.9792 (68.6200)  acc5: 87.5000 (88.7400)  time: 0.1102  data: 0.0804  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1385 s / it)\n",
            "* Acc@1 68.620 Acc@5 88.740 loss 1.415\n",
            "Accuracy of the network on the 10000 test images: 68.6%\n",
            "Max accuracy: 68.62%\n",
            "Epoch: [6]  [  0/781]  eta: 0:11:54  lr: 0.000062  loss: 2.6886 (2.6886)  time: 0.9149  data: 0.7701  max mem: 4927\n",
            "Epoch: [6]  [ 10/781]  eta: 0:02:38  lr: 0.000062  loss: 2.5552 (2.5721)  time: 0.2062  data: 0.0703  max mem: 4927\n",
            "Epoch: [6]  [ 20/781]  eta: 0:02:10  lr: 0.000062  loss: 2.6657 (2.7114)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 30/781]  eta: 0:02:00  lr: 0.000062  loss: 2.6657 (2.6961)  time: 0.1352  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 40/781]  eta: 0:01:54  lr: 0.000062  loss: 2.6760 (2.8394)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 50/781]  eta: 0:01:50  lr: 0.000062  loss: 2.7482 (2.8693)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 60/781]  eta: 0:01:47  lr: 0.000062  loss: 2.6892 (2.8427)  time: 0.1372  data: 0.0004  max mem: 4927\n",
            "Epoch: [6]  [ 70/781]  eta: 0:01:44  lr: 0.000062  loss: 2.6843 (2.8460)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 80/781]  eta: 0:01:41  lr: 0.000062  loss: 2.7419 (2.8381)  time: 0.1350  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [ 90/781]  eta: 0:01:39  lr: 0.000062  loss: 2.7636 (2.8554)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [100/781]  eta: 0:01:37  lr: 0.000062  loss: 2.7360 (2.8544)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [110/781]  eta: 0:01:35  lr: 0.000062  loss: 2.6947 (2.8500)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [120/781]  eta: 0:01:34  lr: 0.000062  loss: 2.7124 (2.8391)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [130/781]  eta: 0:01:32  lr: 0.000062  loss: 2.6685 (2.8239)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [140/781]  eta: 0:01:30  lr: 0.000062  loss: 2.6685 (2.8325)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [150/781]  eta: 0:01:28  lr: 0.000062  loss: 2.7100 (2.8410)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [160/781]  eta: 0:01:27  lr: 0.000062  loss: 2.6857 (2.8406)  time: 0.1363  data: 0.0004  max mem: 4927\n",
            "Epoch: [6]  [170/781]  eta: 0:01:25  lr: 0.000062  loss: 2.6529 (2.8269)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [180/781]  eta: 0:01:24  lr: 0.000062  loss: 2.6765 (2.8361)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [190/781]  eta: 0:01:22  lr: 0.000062  loss: 2.7007 (2.8304)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [200/781]  eta: 0:01:21  lr: 0.000062  loss: 2.7007 (2.8403)  time: 0.1351  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [210/781]  eta: 0:01:19  lr: 0.000062  loss: 2.6236 (2.8394)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [220/781]  eta: 0:01:18  lr: 0.000062  loss: 2.6698 (2.8470)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [230/781]  eta: 0:01:16  lr: 0.000062  loss: 2.7216 (2.8453)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [240/781]  eta: 0:01:15  lr: 0.000062  loss: 2.7020 (2.8459)  time: 0.1377  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [250/781]  eta: 0:01:13  lr: 0.000062  loss: 2.6292 (2.8399)  time: 0.1378  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [260/781]  eta: 0:01:12  lr: 0.000062  loss: 2.6746 (2.8501)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [270/781]  eta: 0:01:11  lr: 0.000062  loss: 2.7118 (2.8466)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [280/781]  eta: 0:01:09  lr: 0.000062  loss: 2.5504 (2.8345)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [290/781]  eta: 0:01:08  lr: 0.000062  loss: 2.5566 (2.8396)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [300/781]  eta: 0:01:06  lr: 0.000062  loss: 2.7445 (2.8475)  time: 0.1388  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [310/781]  eta: 0:01:05  lr: 0.000062  loss: 2.7600 (2.8457)  time: 0.1373  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [320/781]  eta: 0:01:03  lr: 0.000062  loss: 2.6180 (2.8413)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [330/781]  eta: 0:01:02  lr: 0.000062  loss: 2.6270 (2.8400)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [340/781]  eta: 0:01:01  lr: 0.000062  loss: 2.7137 (2.8382)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [350/781]  eta: 0:00:59  lr: 0.000062  loss: 2.7538 (2.8394)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [360/781]  eta: 0:00:58  lr: 0.000062  loss: 2.7889 (2.8445)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [370/781]  eta: 0:00:56  lr: 0.000062  loss: 2.6164 (2.8406)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [380/781]  eta: 0:00:55  lr: 0.000062  loss: 2.5593 (2.8342)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [390/781]  eta: 0:00:54  lr: 0.000062  loss: 2.6577 (2.8375)  time: 0.1377  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [400/781]  eta: 0:00:52  lr: 0.000062  loss: 2.7557 (2.8386)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [410/781]  eta: 0:00:51  lr: 0.000062  loss: 2.6602 (2.8350)  time: 0.1364  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [420/781]  eta: 0:00:49  lr: 0.000062  loss: 2.6556 (2.8394)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [430/781]  eta: 0:00:48  lr: 0.000062  loss: 2.7266 (2.8436)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [440/781]  eta: 0:00:47  lr: 0.000062  loss: 2.7470 (2.8473)  time: 0.1366  data: 0.0004  max mem: 4927\n",
            "Epoch: [6]  [450/781]  eta: 0:00:45  lr: 0.000062  loss: 2.6996 (2.8432)  time: 0.1359  data: 0.0004  max mem: 4927\n",
            "Epoch: [6]  [460/781]  eta: 0:00:44  lr: 0.000062  loss: 2.6178 (2.8406)  time: 0.1365  data: 0.0004  max mem: 4927\n",
            "Epoch: [6]  [470/781]  eta: 0:00:42  lr: 0.000062  loss: 2.6175 (2.8433)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [480/781]  eta: 0:00:41  lr: 0.000062  loss: 2.6324 (2.8463)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [490/781]  eta: 0:00:40  lr: 0.000062  loss: 2.6449 (2.8477)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [500/781]  eta: 0:00:38  lr: 0.000062  loss: 2.6267 (2.8433)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [510/781]  eta: 0:00:37  lr: 0.000062  loss: 2.6228 (2.8407)  time: 0.1363  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [520/781]  eta: 0:00:35  lr: 0.000062  loss: 2.7010 (2.8457)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [530/781]  eta: 0:00:34  lr: 0.000062  loss: 2.6922 (2.8448)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [540/781]  eta: 0:00:33  lr: 0.000062  loss: 2.6438 (2.8431)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [550/781]  eta: 0:00:31  lr: 0.000062  loss: 2.6143 (2.8378)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [560/781]  eta: 0:00:30  lr: 0.000062  loss: 2.5894 (2.8336)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [570/781]  eta: 0:00:29  lr: 0.000062  loss: 2.6353 (2.8342)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [580/781]  eta: 0:00:27  lr: 0.000062  loss: 2.7679 (2.8363)  time: 0.1368  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [590/781]  eta: 0:00:26  lr: 0.000062  loss: 2.8027 (2.8431)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [600/781]  eta: 0:00:24  lr: 0.000062  loss: 2.7145 (2.8413)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [610/781]  eta: 0:00:23  lr: 0.000062  loss: 2.7738 (2.8480)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [620/781]  eta: 0:00:22  lr: 0.000062  loss: 2.7005 (2.8443)  time: 0.1362  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [630/781]  eta: 0:00:20  lr: 0.000062  loss: 2.6234 (2.8451)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [640/781]  eta: 0:00:19  lr: 0.000062  loss: 2.6415 (2.8418)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [650/781]  eta: 0:00:18  lr: 0.000062  loss: 2.6415 (2.8448)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [660/781]  eta: 0:00:16  lr: 0.000062  loss: 2.6603 (2.8414)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [670/781]  eta: 0:00:15  lr: 0.000062  loss: 2.5772 (2.8403)  time: 0.1358  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [680/781]  eta: 0:00:13  lr: 0.000062  loss: 2.5996 (2.8390)  time: 0.1367  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [690/781]  eta: 0:00:12  lr: 0.000062  loss: 2.6596 (2.8408)  time: 0.1372  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [700/781]  eta: 0:00:11  lr: 0.000062  loss: 2.7210 (2.8395)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [710/781]  eta: 0:00:09  lr: 0.000062  loss: 2.5895 (2.8383)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [720/781]  eta: 0:00:08  lr: 0.000062  loss: 2.6558 (2.8365)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [730/781]  eta: 0:00:07  lr: 0.000062  loss: 2.6860 (2.8365)  time: 0.1349  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [740/781]  eta: 0:00:05  lr: 0.000062  loss: 2.7257 (2.8368)  time: 0.1347  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [750/781]  eta: 0:00:04  lr: 0.000062  loss: 2.6722 (2.8366)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [760/781]  eta: 0:00:02  lr: 0.000062  loss: 2.6090 (2.8350)  time: 0.1380  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [770/781]  eta: 0:00:01  lr: 0.000062  loss: 2.6238 (2.8324)  time: 0.1375  data: 0.0003  max mem: 4927\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000062  loss: 2.6144 (2.8297)  time: 0.1354  data: 0.0007  max mem: 4927\n",
            "Epoch: [6] Total time: 0:01:47 (0.1373 s / it)\n",
            "Averaged stats: lr: 0.000062  loss: 2.6144 (2.8297)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.32188260555267334, 'lambda_mobilenetv3_large_100': 0.3026082217693329, 'lambda_regnety_040': 0.3755091726779938}\n",
            "Test:  [ 0/53]  eta: 0:00:45  loss: 0.8859 (0.8859)  acc1: 78.6458 (78.6458)  acc5: 95.3125 (95.3125)  time: 0.8563  data: 0.8254  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2920 (1.1327)  acc1: 74.4792 (74.1004)  acc5: 91.6667 (91.9981)  time: 0.1733  data: 0.1425  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2946 (1.2312)  acc1: 71.3542 (73.0655)  acc5: 90.6250 (90.1538)  time: 0.1284  data: 0.0977  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3712 (1.2874)  acc1: 67.7083 (71.9254)  acc5: 88.5417 (89.6169)  time: 0.1327  data: 0.1020  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4378 (1.3440)  acc1: 67.1875 (70.7317)  acc5: 88.0208 (89.0879)  time: 0.1336  data: 0.1029  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.4071 (1.3407)  acc1: 68.2292 (70.7108)  acc5: 88.5417 (89.2361)  time: 0.1337  data: 0.1029  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4071 (1.3423)  acc1: 68.2292 (70.6500)  acc5: 89.5833 (89.3700)  time: 0.1123  data: 0.0824  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1382 s / it)\n",
            "* Acc@1 70.650 Acc@5 89.370 loss 1.342\n",
            "Accuracy of the network on the 10000 test images: 70.7%\n",
            "Max accuracy: 70.65%\n",
            "Epoch: [7]  [  0/781]  eta: 0:12:11  lr: 0.000062  loss: 2.6096 (2.6096)  time: 0.9365  data: 0.7975  max mem: 4927\n",
            "Epoch: [7]  [ 10/781]  eta: 0:02:42  lr: 0.000062  loss: 2.4971 (2.6070)  time: 0.2109  data: 0.0762  max mem: 4927\n",
            "Epoch: [7]  [ 20/781]  eta: 0:02:18  lr: 0.000062  loss: 2.4977 (2.7021)  time: 0.1441  data: 0.0091  max mem: 4927\n",
            "Epoch: [7]  [ 30/781]  eta: 0:02:07  lr: 0.000062  loss: 2.5656 (2.7280)  time: 0.1473  data: 0.0099  max mem: 4927\n",
            "Epoch: [7]  [ 40/781]  eta: 0:02:03  lr: 0.000062  loss: 2.5906 (2.7321)  time: 0.1516  data: 0.0139  max mem: 4927\n",
            "Epoch: [7]  [ 50/781]  eta: 0:01:58  lr: 0.000062  loss: 2.5785 (2.7590)  time: 0.1503  data: 0.0147  max mem: 4927\n",
            "Epoch: [7]  [ 60/781]  eta: 0:01:55  lr: 0.000062  loss: 2.6378 (2.7741)  time: 0.1446  data: 0.0101  max mem: 4927\n",
            "Epoch: [7]  [ 70/781]  eta: 0:01:51  lr: 0.000062  loss: 2.5747 (2.7504)  time: 0.1440  data: 0.0096  max mem: 4927\n",
            "Epoch: [7]  [ 80/781]  eta: 0:01:49  lr: 0.000062  loss: 2.5161 (2.7805)  time: 0.1430  data: 0.0078  max mem: 4927\n",
            "Epoch: [7]  [ 90/781]  eta: 0:01:46  lr: 0.000062  loss: 2.5607 (2.7941)  time: 0.1436  data: 0.0081  max mem: 4927\n",
            "Epoch: [7]  [100/781]  eta: 0:01:44  lr: 0.000062  loss: 2.5310 (2.7870)  time: 0.1411  data: 0.0058  max mem: 4927\n",
            "Epoch: [7]  [110/781]  eta: 0:01:41  lr: 0.000062  loss: 2.5382 (2.7864)  time: 0.1382  data: 0.0034  max mem: 4927\n",
            "Epoch: [7]  [120/781]  eta: 0:01:39  lr: 0.000062  loss: 2.5581 (2.7788)  time: 0.1430  data: 0.0072  max mem: 4927\n",
            "Epoch: [7]  [130/781]  eta: 0:01:37  lr: 0.000062  loss: 2.5713 (2.7662)  time: 0.1446  data: 0.0087  max mem: 4927\n",
            "Epoch: [7]  [140/781]  eta: 0:01:35  lr: 0.000062  loss: 2.5860 (2.7552)  time: 0.1404  data: 0.0052  max mem: 4927\n",
            "Epoch: [7]  [150/781]  eta: 0:01:34  lr: 0.000062  loss: 2.5510 (2.7516)  time: 0.1421  data: 0.0061  max mem: 4927\n",
            "Epoch: [7]  [160/781]  eta: 0:01:32  lr: 0.000062  loss: 2.5210 (2.7576)  time: 0.1418  data: 0.0060  max mem: 4927\n",
            "Epoch: [7]  [170/781]  eta: 0:01:30  lr: 0.000062  loss: 2.5675 (2.7672)  time: 0.1403  data: 0.0051  max mem: 4927\n",
            "Epoch: [7]  [180/781]  eta: 0:01:28  lr: 0.000062  loss: 2.5872 (2.7704)  time: 0.1381  data: 0.0032  max mem: 4927\n",
            "Epoch: [7]  [190/781]  eta: 0:01:26  lr: 0.000062  loss: 2.5617 (2.7615)  time: 0.1385  data: 0.0038  max mem: 4927\n",
            "Epoch: [7]  [200/781]  eta: 0:01:25  lr: 0.000062  loss: 2.5617 (2.7581)  time: 0.1449  data: 0.0100  max mem: 4927\n",
            "Epoch: [7]  [210/781]  eta: 0:01:23  lr: 0.000062  loss: 2.5623 (2.7493)  time: 0.1465  data: 0.0088  max mem: 4927\n",
            "Epoch: [7]  [220/781]  eta: 0:01:22  lr: 0.000062  loss: 2.5270 (2.7381)  time: 0.1426  data: 0.0041  max mem: 4927\n",
            "Epoch: [7]  [230/781]  eta: 0:01:20  lr: 0.000062  loss: 2.5443 (2.7375)  time: 0.1392  data: 0.0027  max mem: 4927\n",
            "Epoch: [7]  [240/781]  eta: 0:01:18  lr: 0.000062  loss: 2.5443 (2.7287)  time: 0.1367  data: 0.0007  max mem: 4927\n",
            "Epoch: [7]  [250/781]  eta: 0:01:17  lr: 0.000062  loss: 2.5132 (2.7237)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [7]  [260/781]  eta: 0:01:15  lr: 0.000062  loss: 2.4743 (2.7139)  time: 0.1391  data: 0.0033  max mem: 4927\n",
            "Epoch: [7]  [270/781]  eta: 0:01:14  lr: 0.000062  loss: 2.4743 (2.7137)  time: 0.1394  data: 0.0037  max mem: 4927\n",
            "Epoch: [7]  [280/781]  eta: 0:01:12  lr: 0.000062  loss: 2.5805 (2.7190)  time: 0.1363  data: 0.0008  max mem: 4927\n",
            "Epoch: [7]  [290/781]  eta: 0:01:11  lr: 0.000062  loss: 2.5608 (2.7138)  time: 0.1411  data: 0.0036  max mem: 4927\n",
            "Epoch: [7]  [300/781]  eta: 0:01:09  lr: 0.000062  loss: 2.4714 (2.7104)  time: 0.1496  data: 0.0109  max mem: 4927\n",
            "Epoch: [7]  [310/781]  eta: 0:01:08  lr: 0.000062  loss: 2.5186 (2.7079)  time: 0.1439  data: 0.0076  max mem: 4927\n",
            "Epoch: [7]  [320/781]  eta: 0:01:06  lr: 0.000062  loss: 2.5186 (2.7027)  time: 0.1366  data: 0.0005  max mem: 4927\n",
            "Epoch: [7]  [330/781]  eta: 0:01:05  lr: 0.000062  loss: 2.5148 (2.7027)  time: 0.1433  data: 0.0061  max mem: 4927\n",
            "Epoch: [7]  [340/781]  eta: 0:01:03  lr: 0.000062  loss: 2.5068 (2.6976)  time: 0.1459  data: 0.0104  max mem: 4927\n",
            "Epoch: [7]  [350/781]  eta: 0:01:02  lr: 0.000062  loss: 2.4930 (2.6931)  time: 0.1442  data: 0.0077  max mem: 4927\n",
            "Epoch: [7]  [360/781]  eta: 0:01:00  lr: 0.000062  loss: 2.4708 (2.6858)  time: 0.1434  data: 0.0065  max mem: 4927\n",
            "Epoch: [7]  [370/781]  eta: 0:00:59  lr: 0.000062  loss: 2.4754 (2.6812)  time: 0.1407  data: 0.0035  max mem: 4927\n",
            "Epoch: [7]  [380/781]  eta: 0:00:57  lr: 0.000062  loss: 2.5371 (2.6835)  time: 0.1401  data: 0.0020  max mem: 4927\n",
            "Epoch: [7]  [390/781]  eta: 0:00:56  lr: 0.000062  loss: 2.5438 (2.6804)  time: 0.1411  data: 0.0048  max mem: 4927\n",
            "Epoch: [7]  [400/781]  eta: 0:00:54  lr: 0.000062  loss: 2.5852 (2.6826)  time: 0.1401  data: 0.0041  max mem: 4927\n",
            "Epoch: [7]  [410/781]  eta: 0:00:53  lr: 0.000062  loss: 2.5011 (2.6813)  time: 0.1417  data: 0.0057  max mem: 4927\n",
            "Epoch: [7]  [420/781]  eta: 0:00:52  lr: 0.000062  loss: 2.5011 (2.6810)  time: 0.1427  data: 0.0072  max mem: 4927\n",
            "Epoch: [7]  [430/781]  eta: 0:00:50  lr: 0.000062  loss: 2.5278 (2.6808)  time: 0.1424  data: 0.0065  max mem: 4927\n",
            "Epoch: [7]  [440/781]  eta: 0:00:49  lr: 0.000062  loss: 2.6515 (2.6864)  time: 0.1418  data: 0.0053  max mem: 4927\n",
            "Epoch: [7]  [450/781]  eta: 0:00:47  lr: 0.000062  loss: 2.6515 (2.6889)  time: 0.1453  data: 0.0089  max mem: 4927\n",
            "Epoch: [7]  [460/781]  eta: 0:00:46  lr: 0.000062  loss: 2.5592 (2.6900)  time: 0.1490  data: 0.0122  max mem: 4927\n",
            "Epoch: [7]  [470/781]  eta: 0:00:44  lr: 0.000062  loss: 2.6086 (2.6937)  time: 0.1531  data: 0.0159  max mem: 4927\n",
            "Epoch: [7]  [480/781]  eta: 0:00:43  lr: 0.000062  loss: 2.5853 (2.6909)  time: 0.1507  data: 0.0149  max mem: 4927\n",
            "Epoch: [7]  [490/781]  eta: 0:00:42  lr: 0.000062  loss: 2.5219 (2.6918)  time: 0.1472  data: 0.0129  max mem: 4927\n",
            "Epoch: [7]  [500/781]  eta: 0:00:40  lr: 0.000062  loss: 2.5488 (2.6936)  time: 0.1490  data: 0.0148  max mem: 4927\n",
            "Epoch: [7]  [510/781]  eta: 0:00:39  lr: 0.000062  loss: 2.5737 (2.6942)  time: 0.1487  data: 0.0136  max mem: 4927\n",
            "Epoch: [7]  [520/781]  eta: 0:00:37  lr: 0.000062  loss: 2.5119 (2.6908)  time: 0.1458  data: 0.0103  max mem: 4927\n",
            "Epoch: [7]  [530/781]  eta: 0:00:36  lr: 0.000062  loss: 2.5108 (2.6942)  time: 0.1459  data: 0.0107  max mem: 4927\n",
            "Epoch: [7]  [540/781]  eta: 0:00:34  lr: 0.000062  loss: 2.6124 (2.6955)  time: 0.1508  data: 0.0153  max mem: 4927\n",
            "Epoch: [7]  [550/781]  eta: 0:00:33  lr: 0.000062  loss: 2.5266 (2.6917)  time: 0.1531  data: 0.0166  max mem: 4927\n",
            "Epoch: [7]  [560/781]  eta: 0:00:32  lr: 0.000062  loss: 2.5176 (2.6944)  time: 0.1488  data: 0.0112  max mem: 4927\n",
            "Epoch: [7]  [570/781]  eta: 0:00:30  lr: 0.000062  loss: 2.5251 (2.6946)  time: 0.1400  data: 0.0017  max mem: 4927\n",
            "Epoch: [7]  [580/781]  eta: 0:00:29  lr: 0.000062  loss: 2.5254 (2.6976)  time: 0.1399  data: 0.0024  max mem: 4927\n",
            "Epoch: [7]  [590/781]  eta: 0:00:27  lr: 0.000062  loss: 2.5523 (2.6994)  time: 0.1424  data: 0.0074  max mem: 4927\n",
            "Epoch: [7]  [600/781]  eta: 0:00:26  lr: 0.000062  loss: 2.5388 (2.6971)  time: 0.1402  data: 0.0057  max mem: 4927\n",
            "Epoch: [7]  [610/781]  eta: 0:00:24  lr: 0.000062  loss: 2.4787 (2.6992)  time: 0.1408  data: 0.0061  max mem: 4927\n",
            "Epoch: [7]  [620/781]  eta: 0:00:23  lr: 0.000062  loss: 2.5235 (2.6972)  time: 0.1448  data: 0.0105  max mem: 4927\n",
            "Epoch: [7]  [630/781]  eta: 0:00:21  lr: 0.000062  loss: 2.4689 (2.6956)  time: 0.1479  data: 0.0130  max mem: 4927\n",
            "Epoch: [7]  [640/781]  eta: 0:00:20  lr: 0.000062  loss: 2.4526 (2.6940)  time: 0.1516  data: 0.0156  max mem: 4927\n",
            "Epoch: [7]  [650/781]  eta: 0:00:18  lr: 0.000062  loss: 2.4526 (2.6908)  time: 0.1509  data: 0.0155  max mem: 4927\n",
            "Epoch: [7]  [660/781]  eta: 0:00:17  lr: 0.000062  loss: 2.4316 (2.6892)  time: 0.1456  data: 0.0111  max mem: 4927\n",
            "Epoch: [7]  [670/781]  eta: 0:00:16  lr: 0.000062  loss: 2.5409 (2.6886)  time: 0.1446  data: 0.0103  max mem: 4927\n",
            "Epoch: [7]  [680/781]  eta: 0:00:14  lr: 0.000062  loss: 2.4915 (2.6879)  time: 0.1435  data: 0.0088  max mem: 4927\n",
            "Epoch: [7]  [690/781]  eta: 0:00:13  lr: 0.000062  loss: 2.4745 (2.6842)  time: 0.1436  data: 0.0087  max mem: 4927\n",
            "Epoch: [7]  [700/781]  eta: 0:00:11  lr: 0.000062  loss: 2.4880 (2.6822)  time: 0.1468  data: 0.0114  max mem: 4927\n",
            "Epoch: [7]  [710/781]  eta: 0:00:10  lr: 0.000062  loss: 2.5039 (2.6840)  time: 0.1448  data: 0.0092  max mem: 4927\n",
            "Epoch: [7]  [720/781]  eta: 0:00:08  lr: 0.000062  loss: 2.4509 (2.6830)  time: 0.1423  data: 0.0071  max mem: 4927\n",
            "Epoch: [7]  [730/781]  eta: 0:00:07  lr: 0.000062  loss: 2.4457 (2.6810)  time: 0.1456  data: 0.0103  max mem: 4927\n",
            "Epoch: [7]  [740/781]  eta: 0:00:05  lr: 0.000062  loss: 2.5554 (2.6794)  time: 0.1470  data: 0.0122  max mem: 4927\n",
            "Epoch: [7]  [750/781]  eta: 0:00:04  lr: 0.000062  loss: 2.5179 (2.6786)  time: 0.1490  data: 0.0142  max mem: 4927\n",
            "Epoch: [7]  [760/781]  eta: 0:00:03  lr: 0.000062  loss: 2.4740 (2.6776)  time: 0.1497  data: 0.0148  max mem: 4927\n",
            "Epoch: [7]  [770/781]  eta: 0:00:01  lr: 0.000062  loss: 2.4778 (2.6767)  time: 0.1495  data: 0.0147  max mem: 4927\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000062  loss: 2.5289 (2.6782)  time: 0.1567  data: 0.0110  max mem: 4927\n",
            "Epoch: [7] Total time: 0:01:53 (0.1455 s / it)\n",
            "Averaged stats: lr: 0.000062  loss: 2.5289 (2.6782)\n",
            "λ means: {'lambda_tf_efficientnet_b2': 0.32198622822761536, 'lambda_mobilenetv3_large_100': 0.3017784655094147, 'lambda_regnety_040': 0.37623530626296997}\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 1.0420 (1.0420)  acc1: 76.5625 (76.5625)  acc5: 93.2292 (93.2292)  time: 0.8253  data: 0.7943  max mem: 4927\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2014 (1.1653)  acc1: 75.0000 (74.7159)  acc5: 92.7083 (91.8561)  time: 0.1695  data: 0.1388  max mem: 4927\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2762 (1.2237)  acc1: 71.3542 (73.9831)  acc5: 90.6250 (90.6746)  time: 0.1321  data: 0.1013  max mem: 4927\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2879 (1.2687)  acc1: 70.3125 (73.2527)  acc5: 89.5833 (90.2890)  time: 0.1378  data: 0.1070  max mem: 4927\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4202 (1.3107)  acc1: 70.3125 (72.0783)  acc5: 89.5833 (89.9771)  time: 0.1352  data: 0.1044  max mem: 4927\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3809 (1.3171)  acc1: 70.3125 (71.9363)  acc5: 89.5833 (89.9714)  time: 0.1347  data: 0.1040  max mem: 4927\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3930 (1.3218)  acc1: 69.2708 (71.8600)  acc5: 89.5833 (90.0100)  time: 0.1150  data: 0.0852  max mem: 4927\n",
            "Test: Total time: 0:00:07 (0.1396 s / it)\n",
            "* Acc@1 71.860 Acc@5 90.010 loss 1.322\n",
            "Accuracy of the network on the 10000 test images: 71.9%\n",
            "Max accuracy: 71.86%\n",
            "Epoch: [8]  [  0/781]  eta: 0:11:47  lr: 0.000062  loss: 2.3324 (2.3324)  time: 0.9059  data: 0.7509  max mem: 4927\n",
            "Epoch: [8]  [ 10/781]  eta: 0:02:40  lr: 0.000062  loss: 2.5253 (2.7049)  time: 0.2077  data: 0.0695  max mem: 4927\n",
            "Epoch: [8]  [ 20/781]  eta: 0:02:11  lr: 0.000062  loss: 2.4774 (2.6414)  time: 0.1368  data: 0.0008  max mem: 4927\n",
            "Epoch: [8]  [ 30/781]  eta: 0:02:01  lr: 0.000062  loss: 2.3963 (2.5963)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [ 40/781]  eta: 0:01:55  lr: 0.000062  loss: 2.3701 (2.5690)  time: 0.1376  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [ 50/781]  eta: 0:01:51  lr: 0.000062  loss: 2.3481 (2.5625)  time: 0.1386  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [ 60/781]  eta: 0:01:47  lr: 0.000062  loss: 2.4820 (2.6072)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [ 70/781]  eta: 0:01:45  lr: 0.000062  loss: 2.5738 (2.6350)  time: 0.1373  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [ 80/781]  eta: 0:01:42  lr: 0.000062  loss: 2.5635 (2.6558)  time: 0.1382  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [ 90/781]  eta: 0:01:40  lr: 0.000062  loss: 2.4817 (2.6565)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [100/781]  eta: 0:01:38  lr: 0.000062  loss: 2.4798 (2.6412)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [110/781]  eta: 0:01:36  lr: 0.000062  loss: 2.4422 (2.6336)  time: 0.1355  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [120/781]  eta: 0:01:34  lr: 0.000062  loss: 2.3953 (2.6140)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [130/781]  eta: 0:01:33  lr: 0.000062  loss: 2.4072 (2.6010)  time: 0.1374  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [140/781]  eta: 0:01:31  lr: 0.000062  loss: 2.4239 (2.6021)  time: 0.1383  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [150/781]  eta: 0:01:29  lr: 0.000062  loss: 2.3856 (2.6058)  time: 0.1377  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [160/781]  eta: 0:01:28  lr: 0.000062  loss: 2.3856 (2.5985)  time: 0.1369  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [170/781]  eta: 0:01:26  lr: 0.000062  loss: 2.4253 (2.5981)  time: 0.1366  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [180/781]  eta: 0:01:24  lr: 0.000062  loss: 2.4337 (2.5924)  time: 0.1365  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [190/781]  eta: 0:01:23  lr: 0.000062  loss: 2.4426 (2.5926)  time: 0.1361  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [200/781]  eta: 0:01:21  lr: 0.000062  loss: 2.4537 (2.5934)  time: 0.1359  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [210/781]  eta: 0:01:20  lr: 0.000062  loss: 2.4080 (2.5889)  time: 0.1360  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [220/781]  eta: 0:01:18  lr: 0.000062  loss: 2.3668 (2.5783)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [230/781]  eta: 0:01:17  lr: 0.000062  loss: 2.3760 (2.5778)  time: 0.1371  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [240/781]  eta: 0:01:15  lr: 0.000062  loss: 2.4812 (2.5786)  time: 0.1379  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [250/781]  eta: 0:01:14  lr: 0.000062  loss: 2.4355 (2.5787)  time: 0.1381  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [260/781]  eta: 0:01:12  lr: 0.000062  loss: 2.4127 (2.5823)  time: 0.1382  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [270/781]  eta: 0:01:11  lr: 0.000062  loss: 2.4406 (2.5834)  time: 0.1378  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [280/781]  eta: 0:01:09  lr: 0.000062  loss: 2.5147 (2.5925)  time: 0.1370  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [290/781]  eta: 0:01:08  lr: 0.000062  loss: 2.4021 (2.5846)  time: 0.1354  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [300/781]  eta: 0:01:07  lr: 0.000062  loss: 2.3807 (2.5833)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [310/781]  eta: 0:01:05  lr: 0.000062  loss: 2.4285 (2.5862)  time: 0.1353  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [320/781]  eta: 0:01:04  lr: 0.000062  loss: 2.4039 (2.5822)  time: 0.1356  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [330/781]  eta: 0:01:02  lr: 0.000062  loss: 2.3767 (2.5789)  time: 0.1373  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [340/781]  eta: 0:01:01  lr: 0.000062  loss: 2.3868 (2.5799)  time: 0.1369  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [350/781]  eta: 0:00:59  lr: 0.000062  loss: 2.5047 (2.5779)  time: 0.1357  data: 0.0003  max mem: 4927\n",
            "Epoch: [8]  [360/781]  eta: 0:00:58  lr: 0.000062  loss: 2.5047 (2.5767)  time: 0.1368  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [370/781]  eta: 0:00:57  lr: 0.000062  loss: 2.4194 (2.5718)  time: 0.1370  data: 0.0004  max mem: 4927\n",
            "Epoch: [8]  [380/781]  eta: 0:00:55  lr: 0.000062  loss: 2.4173 (2.5722)  time: 0.1358  data: 0.0003  max mem: 4927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}