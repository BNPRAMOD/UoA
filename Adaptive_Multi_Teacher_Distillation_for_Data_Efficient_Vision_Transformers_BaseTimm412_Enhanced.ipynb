{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 1: Baseline DeiT environment**"
      ],
      "metadata": {
        "id": "A814LG7i7w0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DeiT’s baseline training script expects a teacher model name and distillation settings via CLI flags in main.py (e.g., --teacher-model, --teacher-path, --distillation-type).\n",
        "GitHub\n",
        "+1\n",
        "\n",
        "So the “base environment” Layer 1 must include:\n",
        "\n",
        "DeiT repo (cloned)\n",
        "\n",
        "PyTorch (Colab default) + GPU\n",
        "\n",
        "timm installed (for both student and teacher models)\n",
        "\n",
        "compatibility patches if any (because Colab uses new torch/timm)"
      ],
      "metadata": {
        "id": "yZ7gvhPl8OL3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install PyTorch without pinning"
      ],
      "metadata": {
        "id": "25JXNJNx7v2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "OZgeujT4qBSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f47bebc-4f86-40d4-d5c4-14b7ea4f1d01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "WWb1brNPqbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"CUDA:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2uvYnPeqaBB",
        "outputId": "8b6423c0-5df2-497e-8304-667435cfda31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the baseline repo (official DeiT)"
      ],
      "metadata": {
        "id": "3awWPnZtp7E6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aYSAUqVmQid",
        "outputId": "4ac0eda5-f11d-427b-fb6b-8667b494bf7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'deit'...\n",
            "remote: Enumerating objects: 456, done.\u001b[K\n",
            "remote: Total 456 (delta 0), reused 0 (delta 0), pack-reused 456 (from 1)\u001b[K\n",
            "Receiving objects: 100% (456/456), 5.73 MiB | 27.17 MiB/s, done.\n",
            "Resolving deltas: 100% (255/255), done.\n",
            "/content/deit\n",
            "1:torch==1.13.1\n",
            "2:torchvision==0.8.1\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/deit.git\n",
        "%cd /content/deit\n",
        "!grep -n \"torch\" -n requirements.txt || true"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Compatibility Fixes\n",
        "\n",
        "1. torch pin removal\n",
        "\n",
        "2. timm API changes\n",
        "\n",
        "3. kwargs popping (pretrained_cfg, cache_dir, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "fVJsxhJv4Dwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch requirements.txt to remove torch pins"
      ],
      "metadata": {
        "id": "kHpCHaaDr1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"requirements.txt\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "filtered = []\n",
        "removed = []\n",
        "for line in lines:\n",
        "    s = line.strip()\n",
        "    if s.startswith(\"torch==\") or s.startswith(\"torchvision==\") or s.startswith(\"torchaudio==\"):\n",
        "        removed.append(line)\n",
        "        continue\n",
        "    filtered.append(line)\n",
        "\n",
        "p.write_text(\"\\n\".join(filtered) + \"\\n\")\n",
        "print(\"✅ Removed these pinned lines:\")\n",
        "for r in removed:\n",
        "    print(\"  -\", r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3mRQRCcrLmU",
        "outputId": "db43741d-8c69-4b52-b831-75a03b48ddbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Removed these pinned lines:\n",
            "  - torch==1.13.1\n",
            "  - torchvision==0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Pins are gone!i.e torch==1.13.1 pin was removed"
      ],
      "metadata": {
        "id": "lyODjd5lsAqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -nE \"torch|torchvision|torchaudio\" requirements.txt || echo \"✅ No torch pins remain\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7QRJmf7rg6a",
        "outputId": "8b3e4415-9e63-44cc-a99e-4bcf158e3d56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No torch pins remain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the baseline dependencies"
      ],
      "metadata": {
        "id": "csYbu0BampB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"jedi>=0.16,<0.19\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNoLOzs5xUxa",
        "outputId": "f251a4dd-a5d5-48a3-d91e-520f5e94b08e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi<0.19,>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from jedi<0.19,>=0.16) (0.8.5)\n",
            "Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q uninstall -y timm\n",
        "!pip -q install \"jedi>=0.16,<0.19\"\n",
        "# !pip -q install timm==0.6.13 submitit\n",
        "!pip -q install timm==0.4.12 submitit\n"
      ],
      "metadata": {
        "id": "Xsc3-5Ab2Azw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "llX7-GOnsQQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import timm; print('timm:', timm.__version__)\"\n",
        "#0.4.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG39iey7tfMQ",
        "outputId": "3006e02e-d6e1-46d5-f398-1e8a46d9dae0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timm: 0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Session**"
      ],
      "metadata": {
        "id": "r3tle6N46b7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "needle = \"OPENAI_CLIP_MEAN\"\n",
        "if needle in txt:\n",
        "    print(\"✅ timm.data already mentions OPENAI_CLIP_MEAN; no patch needed.\")\n",
        "else:\n",
        "    patch = \"\"\"\n",
        "\n",
        "# --- Colab patch: expose CLIP normalization constants for older exports ---\n",
        "try:\n",
        "    from .constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD  # timm versions where defined in constants\n",
        "except Exception:\n",
        "    # Standard OpenAI CLIP normalization\n",
        "    OPENAI_CLIP_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
        "    OPENAI_CLIP_STD  = (0.26862954, 0.26130258, 0.27577711)\n",
        "# --- end patch ---\n",
        "\"\"\"\n",
        "    p.write_text(txt + patch)\n",
        "    print(\"✅ Patched:\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEsR06SsuQa1",
        "outputId": "8678be23-b5c8-413c-d0c6-6f1aec7873c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched: /usr/local/lib/python3.12/dist-packages/timm/data/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "from models import deit_tiny_patch16_224\n",
        "m = deit_tiny_patch16_224()\n",
        "print(\"✅ DeiT model instantiated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h97jFzzrupzp",
        "outputId": "21b845b8-442e-4a40-a4f2-42c6940771ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "✅ DeiT model instantiated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, timm\n",
        "print(torch.__version__)\n",
        "print(timm.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37b1qcS72uJs",
        "outputId": "5ebb2883-2bb7-4b1c-d908-3fbdd02da2ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.4.12\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Tiny-ImageNet"
      ],
      "metadata": {
        "id": "uu-A5-G7vzTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrbd2wbQyqMV",
        "outputId": "94522ba7-8c8c-45a0-c7ce-54da8c73ac5a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IraDkD4vavm",
        "outputId": "a2b98462-bcf1-4e37-c447-2563392af539",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix Tiny-ImageNet validation folder"
      ],
      "metadata": {
        "id": "qlrZWkYCvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python - << 'EOF'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/tiny-imagenet-200\")\n",
        "val_dir = root/\"val\"\n",
        "img_dir = val_dir/\"images\"\n",
        "ann = val_dir/\"val_annotations.txt\"\n",
        "\n",
        "with ann.open(\"r\") as f:\n",
        "    for line in f:\n",
        "        img, cls = line.strip().split(\"\\t\")[:2]\n",
        "        (val_dir/cls).mkdir(parents=True, exist_ok=True)\n",
        "        src = img_dir/img\n",
        "        dst = val_dir/cls/img\n",
        "        if src.exists():\n",
        "            shutil.move(str(src), str(dst))\n",
        "\n",
        "if img_dir.exists():\n",
        "    shutil.rmtree(img_dir)\n",
        "\n",
        "print(\"✅ Tiny-ImageNet val reorganized into class subfolders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYzGeXJwSsy",
        "outputId": "ca5a556b-c71a-4f65-c502-9f65c6a9e85d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n",
            "✅ Tiny-ImageNet val reorganized into class subfolders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/tiny-imagenet-200/val -maxdepth 1 -type d | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bwwo30Qwi0V",
        "outputId": "19516d79-f9cd-4858-96ff-7674c27e43e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tiny-imagenet-200/val\n",
            "/content/tiny-imagenet-200/val/n02950826\n",
            "/content/tiny-imagenet-200/val/n02085620\n",
            "/content/tiny-imagenet-200/val/n01641577\n",
            "/content/tiny-imagenet-200/val/n04254777\n",
            "/content/tiny-imagenet-200/val/n02917067\n",
            "/content/tiny-imagenet-200/val/n03404251\n",
            "/content/tiny-imagenet-200/val/n03085013\n",
            "/content/tiny-imagenet-200/val/n02504458\n",
            "/content/tiny-imagenet-200/val/n03424325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lah /content/tiny-imagenet-200 | head"
      ],
      "metadata": {
        "id": "0e-EkPZf6GgG",
        "outputId": "1830e3bb-776a-480f-9762-8179076b2df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.6M\n",
            "drwxrwxr-x   5 root root 4.0K Feb  9  2015 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x   1 root root 4.0K Jan 31 13:09 \u001b[01;34m..\u001b[0m/\n",
            "drwxrwxr-x   3 root root 4.0K Dec 12  2014 \u001b[01;34mtest\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Dec 12  2014 \u001b[01;34mtrain\u001b[0m/\n",
            "drwxrwxr-x 202 root root 4.0K Jan 31 13:09 \u001b[01;34mval\u001b[0m/\n",
            "-rw-rw-r--   1 root root 2.0K Feb  9  2015 wnids.txt\n",
            "-rw-------   1 root root 2.6M Feb  9  2015 words.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle timm incompatibilities. Although we can instantiate the model directly, the training script uses timm.create_model(), which injects metadata arguments such as pretrained_cfg and cache_dir.\n",
        "The original DeiT constructors do not support these arguments, so we remove them\n",
        "YOUR NOTEBOOK CALL\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224()          ✅ works (no kwargs)\n",
        "\n",
        "TRAINING PIPELINE\n",
        "    |\n",
        "    v\n",
        "timm.create_model()\n",
        "    |\n",
        "    v\n",
        "deit_tiny_patch16_224(**kwargs)  ❌ injects extra keys\n"
      ],
      "metadata": {
        "id": "Rtyo7rkj3vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch /content/deit/augment.py (safe compatibility fix)"
      ],
      "metadata": {
        "id": "mWebMtbWxHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!python - << 'PY'\n",
        "from pathlib import Path\n",
        "p = Path(\"augment.py\")\n",
        "txt = p.read_text()\n",
        "\n",
        "old = \"from timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\"\n",
        "if old in txt:\n",
        "    txt = txt.replace(\n",
        "        old,\n",
        "        \"from timm.data.transforms import RandomResizedCropAndInterpolation, ToNumpy, ToTensor\\n\"\n",
        "        \"try:\\n\"\n",
        "        \"    from timm.data.transforms import _pil_interp  # older timm\\n\"\n",
        "        \"except Exception:\\n\"\n",
        "        \"    _pil_interp = None  # newer timm doesn't expose this\\n\"\n",
        "    )\n",
        "    p.write_text(txt)\n",
        "    print(\"✅ Patched augment.py for timm compatibility.\")\n",
        "else:\n",
        "    print(\"ℹ️ Expected import line not found; augment.py may already be patched or different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZwKyJqIxG2d",
        "outputId": "7cd189b8-8f1c-4310-ce51-86c5c1c7df85"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✅ Patched augment.py for timm compatibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "!rm -f multiteacher_loss.py\n",
        "!ls -l multiteacher_loss.py || echo \"✅ old file removed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RizknqA6MBXb",
        "outputId": "9b6de4bb-be94-48ed-eafe-872f42e6ed57"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "ls: cannot access 'multiteacher_loss.py': No such file or directory\n",
            "✅ old file removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deit\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "code = r'''\n",
        "from __future__ import annotations\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "\n",
        "def normalize_lambdas(lmb: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    if lmb.dim() == 1:\n",
        "        return lmb / lmb.sum().clamp_min(eps)\n",
        "    return lmb / lmb.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
        "\n",
        "\n",
        "def fuse_logits(\n",
        "    teacher_logits: Dict[str, torch.Tensor],\n",
        "    teacher_order: List[str],\n",
        "    lambdas: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    logits_list = [teacher_logits[k] for k in teacher_order]\n",
        "    stacked = torch.stack(logits_list, dim=1)  # (B, T, C)\n",
        "\n",
        "    lambdas = normalize_lambdas(lambdas).to(stacked.device)\n",
        "    if lambdas.dim() == 1:\n",
        "        lambdas = lambdas.unsqueeze(0).expand(stacked.size(0), -1)  # (B, T)\n",
        "\n",
        "    return (stacked * lambdas.unsqueeze(-1)).sum(dim=1)  # (B, C)\n",
        "\n",
        "\n",
        "def kd_soft(student_logits: torch.Tensor, teacher_logits: torch.Tensor, T: float) -> torch.Tensor:\n",
        "    p_t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    log_p_s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    return F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
        "\n",
        "\n",
        "def kd_hard(student_logits: torch.Tensor, teacher_logits: torch.Tensor) -> torch.Tensor:\n",
        "    return F.cross_entropy(student_logits, teacher_logits.argmax(dim=-1))\n",
        "\n",
        "\n",
        "class FrozenTeacherEnsemble(nn.Module):\n",
        "    def __init__(self, teacher_names: List[str], device: torch.device):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleDict({\n",
        "            name: timm.create_model(name, pretrained=True, num_classes=1000).eval().to(device)\n",
        "            for name in teacher_names\n",
        "        })\n",
        "        for m in self.models.values():\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad_(False)\n",
        "        self.teacher_order = list(self.models.keys())\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        return {k: m(x) for k, m in self.models.items()}\n",
        "\n",
        "\n",
        "class TeacherLogitAdapter(nn.Module):\n",
        "    def __init__(self, teacher_keys: List[str], student_num_classes: int):\n",
        "        super().__init__()\n",
        "        self.adapters = nn.ModuleDict({\n",
        "            k: nn.Linear(1000, student_num_classes, bias=False) for k in teacher_keys\n",
        "        })\n",
        "\n",
        "    def forward(self, teacher_logits: Dict[str, torch.Tensor]):\n",
        "        return {k: self.adapters[k](v) for k, v in teacher_logits.items()}\n",
        "\n",
        "\n",
        "class HDTSEConfidence(nn.Module):\n",
        "    def __init__(self, temp: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.temp = temp\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, student_logits, teacher_logits, teacher_order, targets):\n",
        "        stacked = torch.stack([teacher_logits[k] for k in teacher_order], dim=1)  # (B,T,C)\n",
        "        probs = F.softmax(stacked / self.temp, dim=-1)  # (B,T,C)\n",
        "\n",
        "        # Hard labels: (B,)\n",
        "        if targets.dim() == 1:\n",
        "            idx = targets.to(dtype=torch.long, device=probs.device)\n",
        "            conf = probs.gather(-1, idx[:, None, None]).squeeze(-1)  # (B,T)\n",
        "            return normalize_lambdas(conf)\n",
        "\n",
        "        # Soft labels (mixup/cutmix): (B,C)\n",
        "        tgt = targets.to(dtype=probs.dtype, device=probs.device)\n",
        "        conf = (probs * tgt[:, None, :]).sum(dim=-1)  # (B,T)\n",
        "        return normalize_lambdas(conf)\n",
        "\n",
        "\n",
        "class MultiTeacherDistillationLoss(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_criterion,\n",
        "        student_num_classes: int,\n",
        "        teacher_names: List[str],\n",
        "        distillation_type: str = \"soft\",\n",
        "        alpha: float = 0.5,\n",
        "        tau: float = 2.0,\n",
        "        device=None,\n",
        "        use_adapter: bool = True,\n",
        "        hdtse_warmup_epochs: int = 0,\n",
        "        lambda_log: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.base_criterion = base_criterion\n",
        "        self.distillation_type = distillation_type\n",
        "        self.alpha = alpha\n",
        "        self.tau = tau\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.teachers = FrozenTeacherEnsemble(teacher_names, self.device)\n",
        "        self.teacher_order = list(self.teachers.teacher_order)\n",
        "\n",
        "        self.adapter = TeacherLogitAdapter(self.teachers.teacher_order, student_num_classes).to(self.device) if use_adapter else None\n",
        "        self.hdtse = HDTSEConfidence()\n",
        "\n",
        "        # ---- New controls ----\n",
        "        self.epoch: int = 0\n",
        "        self.hdtse_warmup_epochs = int(hdtse_warmup_epochs)\n",
        "        self.lambda_log = bool(lambda_log)\n",
        "\n",
        "        # ---- Logging state (epoch-level) ----\n",
        "        self._lambda_sum = torch.zeros(len(self.teacher_order), dtype=torch.float32)\n",
        "        self._lambda_count = 0\n",
        "        self.last_lambdas: Optional[torch.Tensor] = None  # (B,T) from last forward\n",
        "\n",
        "    def set_epoch(self, epoch: int):\n",
        "        self.epoch = int(epoch)\n",
        "\n",
        "    def _uniform_lambdas(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        t = len(self.teacher_order)\n",
        "        return torch.full((batch_size, t), 1.0 / t, device=device, dtype=torch.float32)\n",
        "\n",
        "    def pop_lambda_stats(self) -> Optional[Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Returns mean λ per teacher over the epoch, then resets accumulators.\n",
        "        Call this once per epoch from main.py.\n",
        "        \"\"\"\n",
        "        if self._lambda_count <= 0:\n",
        "            return None\n",
        "\n",
        "        mean_lmb = (self._lambda_sum / float(self._lambda_count)).tolist()\n",
        "        out = {f\"lambda_{name}\": float(v) for name, v in zip(self.teacher_order, mean_lmb)}\n",
        "\n",
        "        # reset\n",
        "        self._lambda_sum.zero_()\n",
        "        self._lambda_count = 0\n",
        "        return out\n",
        "\n",
        "    def forward(self, inputs, outputs, targets):\n",
        "        base_loss = self.base_criterion(outputs, targets)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_logits = self.teachers(inputs)\n",
        "        if self.adapter is not None:\n",
        "            t_logits = self.adapter(t_logits)\n",
        "\n",
        "        # ---- HDTSE delay ----\n",
        "        if self.epoch < self.hdtse_warmup_epochs:\n",
        "            lambdas = self._uniform_lambdas(outputs.size(0), outputs.device)  # (B,T)\n",
        "        else:\n",
        "            lambdas = self.hdtse(outputs, t_logits, list(t_logits.keys()), targets)  # (B,T)\n",
        "\n",
        "        self.last_lambdas = lambdas.detach()\n",
        "\n",
        "        # ---- λ logging ----\n",
        "        if self.lambda_log:\n",
        "            # accumulate batch mean λ, weighted by batch size\n",
        "            batch_mean = lambdas.detach().mean(dim=0).cpu()  # (T,)\n",
        "            self._lambda_sum += batch_mean * outputs.size(0)\n",
        "            self._lambda_count += outputs.size(0)\n",
        "\n",
        "        fused = fuse_logits(t_logits, self.teacher_order, lambdas)\n",
        "\n",
        "        kd = kd_soft(outputs, fused, self.tau) if self.distillation_type == \"soft\" else kd_hard(outputs, fused)\n",
        "        return (1 - self.alpha) * base_loss + self.alpha * kd\n",
        "'''\n",
        "\n",
        "path = Path(\"multiteacher_loss.py\")\n",
        "path.write_text(code)\n",
        "\n",
        "print(\"File written:\", path)\n",
        "print(\"File size (bytes):\", path.stat().st_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4jzkzbMHD-",
        "outputId": "54030e4b-43b4-4287-b7d6-83818d598736"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "File written: multiteacher_loss.py\n",
            "File size (bytes): 6509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, py_compile\n",
        "\n",
        "MAIN = Path(\"/content/deit/main.py\")\n",
        "txt = MAIN.read_text()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Helpers (line-safe insertions to avoid indentation/newline bugs)\n",
        "# ------------------------------------------------------------\n",
        "def fix_broken_import_concatenation():\n",
        "    global txt\n",
        "    # Fix exact failure mode:\n",
        "    txt = txt.replace(\n",
        "        \"from multiteacher_loss import MultiTeacherDistillationLossfrom samplers import RASampler\",\n",
        "        \"from multiteacher_loss import MultiTeacherDistillationLoss\\nfrom samplers import RASampler\"\n",
        "    )\n",
        "\n",
        "def ensure_line_after(match_line_regex: str, new_line: str):\n",
        "    \"\"\"Insert `new_line` as a full line right AFTER the first line matching regex.\"\"\"\n",
        "    global txt\n",
        "    if new_line.strip() in txt:\n",
        "        return\n",
        "    lines = txt.splitlines(True)  # keep line endings\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(match_line_regex, line):\n",
        "            # insert after this line\n",
        "            if not new_line.endswith(\"\\n\"):\n",
        "                new_line2 = new_line + \"\\n\"\n",
        "            else:\n",
        "                new_line2 = new_line\n",
        "            lines.insert(i + 1, new_line2)\n",
        "            txt = \"\".join(lines)\n",
        "            return\n",
        "    raise RuntimeError(f\"Could not find line to insert after: {match_line_regex}\")\n",
        "\n",
        "def ensure_block_after_line(match_line_regex: str, block: str):\n",
        "    \"\"\"Insert a multi-line block after first line matching regex.\"\"\"\n",
        "    global txt\n",
        "    # Heuristic: if first unique token already exists, don't re-add\n",
        "    if \"--teacher-models\" in block and \"--teacher-models\" in txt and \"--hdtse-warmup-epochs\" in txt and \"--lambda-log\" in txt:\n",
        "        return\n",
        "    lines = txt.splitlines(True)\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(match_line_regex, line):\n",
        "            if not block.endswith(\"\\n\"):\n",
        "                block2 = block + \"\\n\"\n",
        "            else:\n",
        "                block2 = block\n",
        "            lines.insert(i + 1, block2)\n",
        "            txt = \"\".join(lines)\n",
        "            return\n",
        "    raise RuntimeError(f\"Could not find line to insert block after: {match_line_regex}\")\n",
        "\n",
        "def replace_first(pattern: str, repl: str, flags=re.DOTALL):\n",
        "    global txt\n",
        "    m = re.search(pattern, txt, flags)\n",
        "    if not m:\n",
        "        return False\n",
        "    txt = txt[:m.start()] + repl + txt[m.end():]\n",
        "    return True\n",
        "\n",
        "def remove_first_line_matching(line_regex: str):\n",
        "    global txt\n",
        "    lines = txt.splitlines(True)\n",
        "    for i, line in enumerate(lines):\n",
        "        if re.search(line_regex, line):\n",
        "            del lines[i]\n",
        "            txt = \"\".join(lines)\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0) Repair if prior patch created the exact SyntaxError\n",
        "# ------------------------------------------------------------\n",
        "fix_broken_import_concatenation()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1) Ensure MultiTeacherDistillationLoss import (safe line insertion)\n",
        "# Insert after: from losses import DistillationLoss\n",
        "# ------------------------------------------------------------\n",
        "ensure_line_after(\n",
        "    r\"^\\s*from\\s+losses\\s+import\\s+DistillationLoss\\s*$\",\n",
        "    \"from multiteacher_loss import MultiTeacherDistillationLoss\"\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2) Ensure CLI args after --teacher-path\n",
        "# ------------------------------------------------------------\n",
        "cli_block = \"\"\"\\\n",
        "    parser.add_argument('--teacher-models', type=str, default='',\n",
        "                        help='Comma-separated timm model names for multi-teacher distillation')\n",
        "    parser.add_argument('--hdtse-warmup-epochs', type=int, default=0,\n",
        "                        help='Use uniform teacher weights for first N epochs, then enable HDTSE weighting')\n",
        "    parser.add_argument('--lambda-log', action='store_true', default=False,\n",
        "                        help='Log mean λ (teacher weights) each epoch for multi-teacher distillation')\n",
        "\"\"\"\n",
        "ensure_block_after_line(r\"^\\s*parser\\.add_argument\\('--teacher-path'\", cli_block)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) Allow finetune + distillation ONLY when multi-teacher is used\n",
        "# Base guard is:\n",
        "# if args.distillation_type != 'none' and args.finetune and not args.eval:\n",
        "#     raise NotImplementedError(...)\n",
        "# ------------------------------------------------------------\n",
        "replace_first(\n",
        "    r\"^\\s*if\\s+args\\.distillation_type\\s*!=\\s*'none'\\s+and\\s+args\\.finetune\\s+and\\s+not\\s+args\\.eval\\s*:\\s*\\n\\s*raise\\s+NotImplementedError\\([^\\n]*\\)\\s*$\",\n",
        "    \"    if args.distillation_type != 'none' and args.finetune and not args.eval and not getattr(args, 'teacher_models', ''):\\n\"\n",
        "    \"        raise NotImplementedError(\\\"Finetuning with distillation not yet supported (single-teacher path)\\\")\\n\",\n",
        "    flags=re.MULTILINE\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4) Move scheduler creation to AFTER adapter param-group add:\n",
        "# Remove early: lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "# ------------------------------------------------------------\n",
        "remove_first_line_matching(r\"^\\s*lr_scheduler,\\s*_\\s*=\\s*create_scheduler\\(\\s*args\\s*,\\s*optimizer\\s*\\)\\s*$\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5) Unify distillation region (multi-teacher vs single-teacher)\n",
        "# We'll replace from \"teacher_model = None\" up to \"output_dir = Path(args.output_dir)\"\n",
        "# This avoids indentation mistakes and prevents teacher_path='' crash.\n",
        "# ------------------------------------------------------------\n",
        "m_start = re.search(r\"^\\s*teacher_model\\s*=\\s*None\\s*$\", txt, flags=re.MULTILINE)\n",
        "m_end   = re.search(r\"^\\s*output_dir\\s*=\\s*Path\\(args\\.output_dir\\)\\s*$\", txt, flags=re.MULTILINE)\n",
        "if not (m_start and m_end and m_start.start() < m_end.start()):\n",
        "    raise RuntimeError(\"Could not locate distillation region anchors (teacher_model=None ... output_dir=Path(...))\")\n",
        "\n",
        "unified = \"\"\"\\\n",
        "    teacher_model = None\n",
        "\n",
        "    # -------------------------------\n",
        "    # Unified single + multi-teacher distillation\n",
        "    # -------------------------------\n",
        "    teacher_models_str = getattr(args, 'teacher_models', '').strip()\n",
        "\n",
        "    if args.distillation_type != 'none' and teacher_models_str:\n",
        "        teacher_names = [t.strip() for t in teacher_models_str.split(',') if t.strip()]\n",
        "        print(f\"✅ Multi-teacher distillation enabled. Teachers: {teacher_names}\")\n",
        "\n",
        "        criterion = MultiTeacherDistillationLoss(\n",
        "            base_criterion=criterion,\n",
        "            student_num_classes=args.nb_classes,\n",
        "            teacher_names=teacher_names,\n",
        "            distillation_type=args.distillation_type,\n",
        "            alpha=args.distillation_alpha,\n",
        "            tau=args.distillation_tau,\n",
        "            device=device,\n",
        "            use_adapter=True,\n",
        "            hdtse_warmup_epochs=getattr(args, 'hdtse_warmup_epochs', 0),\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        if args.distillation_type != 'none':\n",
        "            assert args.teacher_path, 'need to specify teacher-path when using single-teacher distillation'\n",
        "            print(f\"Creating teacher model: {args.teacher_model}\")\n",
        "            teacher_model = create_model(\n",
        "                args.teacher_model,\n",
        "                pretrained=False,\n",
        "                num_classes=args.nb_classes,\n",
        "                global_pool='avg',\n",
        "            )\n",
        "            if args.teacher_path.startswith('https'):\n",
        "                checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                    args.teacher_path, map_location='cpu', check_hash=True)\n",
        "            else:\n",
        "                checkpoint = torch.load(args.teacher_path, map_location='cpu')\n",
        "            teacher_model.load_state_dict(checkpoint['model'])\n",
        "            teacher_model.to(device)\n",
        "            teacher_model.eval()\n",
        "\n",
        "        criterion = DistillationLoss(\n",
        "            criterion, teacher_model, args.distillation_type, args.distillation_alpha, args.distillation_tau\n",
        "        )\n",
        "\n",
        "    # ---- Multi-teacher: add adapter params (if present) BEFORE scheduler ----\n",
        "    if hasattr(criterion, \"adapter\") and getattr(criterion, \"adapter\", None) is not None:\n",
        "        optimizer.add_param_group({\n",
        "            \"params\": criterion.adapter.parameters(),\n",
        "            \"lr\": args.lr,\n",
        "            \"weight_decay\": 0.0,\n",
        "        })\n",
        "        print(\"✅ Added adapter parameters to optimizer\")\n",
        "\n",
        "    # Scheduler must be created AFTER all optimizer param groups are finalized\n",
        "    lr_scheduler, _ = create_scheduler(args, optimizer)\n",
        "\"\"\"\n",
        "\n",
        "txt = txt[:m_start.start()] + unified + \"\\n    output_dir = Path(args.output_dir)\\n\" + txt[m_end.end():]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6) Insert criterion.set_epoch(epoch) before train_one_epoch\n",
        "# We add it inside the epoch loop, after sampler.set_epoch if present.\n",
        "# ------------------------------------------------------------\n",
        "if \"criterion.set_epoch(epoch)\" not in txt:\n",
        "    # If distributed block exists, insert after it\n",
        "    if re.search(r\"^\\s*if\\s+args\\.distributed\\s*:\\s*\\n\\s*data_loader_train\\.sampler\\.set_epoch\\(epoch\\)\\s*$\", txt, flags=re.MULTILINE):\n",
        "        txt = re.sub(\n",
        "            r\"(^\\s*if\\s+args\\.distributed\\s*:\\s*\\n\\s*data_loader_train\\.sampler\\.set_epoch\\(epoch\\)\\s*$)\",\n",
        "            r\"\\1\\n        if hasattr(criterion, 'set_epoch'):\\n            criterion.set_epoch(epoch)\",\n",
        "            txt,\n",
        "            flags=re.MULTILINE,\n",
        "            count=1\n",
        "        )\n",
        "    else:\n",
        "        # Otherwise put at top of loop\n",
        "        txt = re.sub(\n",
        "            r\"(^\\s*for\\s+epoch\\s+in\\s+range\\(args\\.start_epoch,\\s*args\\.epochs\\)\\s*:\\s*$)\",\n",
        "            r\"\\1\\n        if hasattr(criterion, 'set_epoch'):\\n            criterion.set_epoch(epoch)\",\n",
        "            txt,\n",
        "            flags=re.MULTILINE,\n",
        "            count=1\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7) Per-epoch λ logging after train_one_epoch call\n",
        "# ------------------------------------------------------------\n",
        "if \"print('λ means:'\" not in txt:\n",
        "    txt = re.sub(\n",
        "        r\"(train_stats\\s*=\\s*train_one_epoch\\([\\s\\S]*?\\)\\s*)\\n\",\n",
        "        r\"\\1\\n\\n        # Optional: log mean λ per teacher (multi-teacher only)\\n\"\n",
        "        r\"        if getattr(args, 'lambda_log', False) and hasattr(criterion, 'get_last_lambda_stats'):\\n\"\n",
        "        r\"            lambda_means = criterion.get_last_lambda_stats()\\n\"\n",
        "        r\"            if lambda_means:\\n\"\n",
        "        r\"                print('λ means:', lambda_means)\\n\\n\",\n",
        "        txt,\n",
        "        count=1\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Write + compile check\n",
        "# ------------------------------------------------------------\n",
        "MAIN.write_text(txt)\n",
        "py_compile.compile(str(MAIN), doraise=True)\n",
        "print(\"✅ Patched main.py written and compiles:\", MAIN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUdJZ4F-NoE-",
        "outputId": "42bc2b7f-fac1-4bff-9877-70ef98e69198"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched main.py written and compiles: /content/deit/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the model, remove those keys from kwargs"
      ],
      "metadata": {
        "id": "4sFpztpw00XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    if line.strip().startswith(\"def deit_\") and \"**kwargs\" in line:\n",
        "        out.append(\"    # Drop timm-injected kwargs not supported by DeiT\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_overlay', None)\")\n",
        "        out.append(\"    kwargs.pop('pretrained_cfg_priority', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ models.py patched to drop pretrained_cfg kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1qywwxV0RS-",
        "outputId": "967f6922-1726-4b69-d03b-b9d9e346ee1a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ models.py patched to drop pretrained_cfg kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify"
      ],
      "metadata": {
        "id": "Yh47-0Pv0-R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix: Patch /content/deit/models.py to drop pretrained_cfg=..."
      ],
      "metadata": {
        "id": "hfueTM11xy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch models.py to also drop cache_dir (and friends)"
      ],
      "metadata": {
        "id": "OK2GsetX1ZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "p = Path(\"/content/deit/models.py\")\n",
        "lines = p.read_text().splitlines()\n",
        "\n",
        "# Keys that timm may inject but DeiT constructors don't accept\n",
        "DROP_KEYS = [\n",
        "    \"cache_dir\",\n",
        "    \"hf_hub_id\",\n",
        "    \"hf_hub_filename\",\n",
        "    \"hf_hub_revision\",\n",
        "]\n",
        "\n",
        "out = []\n",
        "for line in lines:\n",
        "    out.append(line)\n",
        "    # Right after the comment line we previously inserted, add more pops once per function\n",
        "    if line.strip() == \"# Drop timm-injected kwargs not supported by DeiT\":\n",
        "        for k in DROP_KEYS:\n",
        "            out.append(f\"    kwargs.pop('{k}', None)\")\n",
        "\n",
        "p.write_text(\"\\n\".join(out) + \"\\n\")\n",
        "print(\"✅ Patched models.py to drop cache_dir/hf_hub* kwargs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0-XJmyw1aed",
        "outputId": "f619c9ab-b848-4f96-945f-6985f737a01e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Patched models.py to drop cache_dir/hf_hub* kwargs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --pretrained \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --num_workers 2 \\\n",
        "#   --output_dir /content/deit_runs/smoke_test\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#   --model deit_tiny_patch16_224 \\\n",
        "#   --data-path /content/tiny-imagenet-200 \\\n",
        "#   --epochs 1 \\\n",
        "#   --batch-size 128 \\\n",
        "#   --num_workers 4 \\\n",
        "#   --input-size 224 \\\n",
        "#   --opt adamw \\\n",
        "#   --lr 5e-4 \\\n",
        "#   --weight-decay 0.05 \\\n",
        "#   --sched cosine \\\n",
        "#   --aa rand-m9-mstd0.5 \\\n",
        "#   --reprob 0.25 \\\n",
        "#   --remode pixel \\\n",
        "#   --recount 1 \\\n",
        "#   --output_dir /content/deit_runs/tiny_imagenet\n",
        "### correct one\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 3e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_5ep\n",
        "%cd /content/deit\n",
        "!python main.py \\\n",
        " --model deit_tiny_patch16_224 \\\n",
        " --data-path /content/tiny-imagenet-200 \\\n",
        " --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        " --epochs 20 \\\n",
        " --batch-size 128 \\\n",
        " --num_workers 4 \\\n",
        " --input-size 224 \\\n",
        " --opt adamw \\\n",
        " --lr 2.5e-4 \\\n",
        " --weight-decay 0.05 \\\n",
        " --sched cosine \\\n",
        " --warmup-epochs 0 \\\n",
        " --smoothing 0.1 \\\n",
        " --aa rand-m6-mstd0.5 \\\n",
        " --reprob 0.1 \\\n",
        " --drop-path 0.05 \\\n",
        " --mixup 0.2 \\\n",
        " --cutmix 0.0 \\\n",
        " --mixup-prob 0.5 \\\n",
        " --distillation-type soft \\\n",
        " --distillation-alpha 0.4 \\\n",
        " --distillation-tau 2.0 \\\n",
        " --hdtse-warmup-epochs 3 \\\n",
        " --lambda-log \\\n",
        " --output_dir /content/deit_runs/tiny_imagenet \\\n",
        " --teacher-models \"tf_efficientnet_b2,mobilenetv3_large_100,regnety_040\"\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --finetune https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 2.5e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.1 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.1 \\\n",
        "#  --distillation-type hard \\\n",
        "# --teacher-model regnety_160 \\\n",
        "# --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/tiny_imagenet_10ep\n",
        "# %cd /content/deit\n",
        "# !python main.py \\\n",
        "#  --model deit_tiny_distilled_patch16_224 \\\n",
        "#  --data-path /content/tiny-imagenet-200 \\\n",
        "#  --epochs 10 \\\n",
        "#  --batch-size 128 \\\n",
        "#  --num_workers 4 \\\n",
        "#  --input-size 224 \\\n",
        "#  --opt adamw \\\n",
        "#  --lr 7e-4 \\\n",
        "#  --weight-decay 0.05 \\\n",
        "#  --sched cosine \\\n",
        "#  --warmup-epochs 1 \\\n",
        "#  --smoothing 0.0 \\\n",
        "#  --aa rand-m7-mstd0.5 \\\n",
        "#  --reprob 0.1 \\\n",
        "#  --drop-path 0.0 \\\n",
        "#  --distillation-type hard \\\n",
        "#  --distillation-alpha 0.7 \\\n",
        "#  --teacher-model regnety_160 \\\n",
        "#  --teacher-path https://dl.fbaipublicfiles.com/deit/regnety_160-a5fe301d.pth \\\n",
        "#  --output_dir /content/deit_runs/deit_tiny_distilled_10ep\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYvrcwJwlde",
        "outputId": "7b2ecf6e-f5a8-4ab7-95b5-efe62966d34c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deit\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=128, epochs=20, bce_loss=False, unscale_lr=False, model='deit_tiny_patch16_224', input_size=224, drop=0.0, drop_path=0.05, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.00025, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m6-mstd0.5', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.1, remode='pixel', recount=1, resplit=False, mixup=0.2, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.5, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', teacher_models='tf_efficientnet_b2,mobilenetv3_large_100,regnety_040', hdtse_warmup_epochs=3, lambda_log=True, distillation_type='soft', distillation_alpha=0.4, distillation_tau=2.0, cosub=False, finetune='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth', attn_only=False, data_path='/content/tiny-imagenet-200', data_set='IMNET', inat_category='name', output_dir='/content/deit_runs/tiny_imagenet', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=4, pin_mem=True, distributed=False, world_size=1, dist_url='env://')\n",
            "Creating model: deit_tiny_patch16_224\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n",
            "100% 21.9M/21.9M [00:00<00:00, 141MB/s] \n",
            "number of params: 5717416\n",
            "/usr/local/lib/python3.12/dist-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "✅ Multi-teacher distillation enabled. Teachers: ['tf_efficientnet_b2', 'mobilenetv3_large_100', 'regnety_040']\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b2_aa-60c94f97.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\" to /root/.cache/torch/hub/checkpoints/mobilenetv3_large_100_ra-f55367f5.pth\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth\" to /root/.cache/torch/hub/checkpoints/regnety_040-f0d569f9.pth\n",
            "✅ Added adapter parameters to optimizer\n",
            "Start training for 20 epochs\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch: [0]  [  0/781]  eta: 3:36:45  lr: 0.000063  loss: 5.7380 (5.7380)  time: 16.6518  data: 0.9092  max mem: 4938\n",
            "Epoch: [0]  [ 10/781]  eta: 0:21:04  lr: 0.000063  loss: 4.1712 (4.4154)  time: 1.6404  data: 0.0829  max mem: 4938\n",
            "Epoch: [0]  [ 20/781]  eta: 0:11:44  lr: 0.000063  loss: 3.8933 (4.1484)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 30/781]  eta: 0:08:24  lr: 0.000063  loss: 3.8010 (4.0184)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 40/781]  eta: 0:06:41  lr: 0.000063  loss: 3.7020 (3.9324)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 50/781]  eta: 0:05:38  lr: 0.000063  loss: 3.6260 (3.8694)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 60/781]  eta: 0:04:55  lr: 0.000063  loss: 3.5816 (3.8182)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 70/781]  eta: 0:04:24  lr: 0.000063  loss: 3.5516 (3.7791)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 80/781]  eta: 0:04:00  lr: 0.000063  loss: 3.5253 (3.7463)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [ 90/781]  eta: 0:03:41  lr: 0.000063  loss: 3.4979 (3.7181)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [100/781]  eta: 0:03:25  lr: 0.000063  loss: 3.4818 (3.6945)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [110/781]  eta: 0:03:13  lr: 0.000063  loss: 3.4649 (3.6735)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [120/781]  eta: 0:03:02  lr: 0.000063  loss: 3.4452 (3.6537)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [130/781]  eta: 0:02:52  lr: 0.000063  loss: 3.4349 (3.6367)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [140/781]  eta: 0:02:44  lr: 0.000063  loss: 3.4210 (3.6201)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [150/781]  eta: 0:02:36  lr: 0.000063  loss: 3.3871 (3.6048)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [160/781]  eta: 0:02:30  lr: 0.000063  loss: 3.3832 (3.5908)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [170/781]  eta: 0:02:23  lr: 0.000063  loss: 3.3742 (3.5776)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [180/781]  eta: 0:02:18  lr: 0.000063  loss: 3.3565 (3.5651)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [190/781]  eta: 0:02:13  lr: 0.000063  loss: 3.3506 (3.5534)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [200/781]  eta: 0:02:08  lr: 0.000063  loss: 3.3270 (3.5418)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [210/781]  eta: 0:02:04  lr: 0.000063  loss: 3.2952 (3.5303)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [220/781]  eta: 0:02:00  lr: 0.000063  loss: 3.2854 (3.5188)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [230/781]  eta: 0:01:56  lr: 0.000063  loss: 3.2704 (3.5087)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [240/781]  eta: 0:01:52  lr: 0.000063  loss: 3.2496 (3.4977)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [250/781]  eta: 0:01:48  lr: 0.000063  loss: 3.2357 (3.4872)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [260/781]  eta: 0:01:45  lr: 0.000063  loss: 3.2201 (3.4764)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [270/781]  eta: 0:01:42  lr: 0.000063  loss: 3.2136 (3.4673)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [280/781]  eta: 0:01:39  lr: 0.000063  loss: 3.1936 (3.4572)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [290/781]  eta: 0:01:36  lr: 0.000063  loss: 3.1828 (3.4473)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [300/781]  eta: 0:01:33  lr: 0.000063  loss: 3.1618 (3.4381)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [310/781]  eta: 0:01:30  lr: 0.000063  loss: 3.1425 (3.4293)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [320/781]  eta: 0:01:27  lr: 0.000063  loss: 3.1139 (3.4193)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [330/781]  eta: 0:01:25  lr: 0.000063  loss: 3.1042 (3.4108)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [340/781]  eta: 0:01:22  lr: 0.000063  loss: 3.1066 (3.4029)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [350/781]  eta: 0:01:20  lr: 0.000063  loss: 3.1066 (3.3955)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [360/781]  eta: 0:01:17  lr: 0.000063  loss: 3.0854 (3.3869)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [370/781]  eta: 0:01:15  lr: 0.000063  loss: 3.0494 (3.3779)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [380/781]  eta: 0:01:13  lr: 0.000063  loss: 3.0436 (3.3706)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [390/781]  eta: 0:01:10  lr: 0.000063  loss: 3.0215 (3.3616)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [400/781]  eta: 0:01:08  lr: 0.000063  loss: 2.9882 (3.3536)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [410/781]  eta: 0:01:06  lr: 0.000063  loss: 2.9738 (3.3453)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [420/781]  eta: 0:01:04  lr: 0.000063  loss: 2.9560 (3.3373)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [430/781]  eta: 0:01:02  lr: 0.000063  loss: 2.9460 (3.3293)  time: 0.1395  data: 0.0004  max mem: 4938\n",
            "Epoch: [0]  [440/781]  eta: 0:01:00  lr: 0.000063  loss: 2.9430 (3.3209)  time: 0.1395  data: 0.0004  max mem: 4938\n",
            "Epoch: [0]  [450/781]  eta: 0:00:58  lr: 0.000063  loss: 2.9149 (3.3118)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [460/781]  eta: 0:00:56  lr: 0.000063  loss: 2.8977 (3.3031)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [470/781]  eta: 0:00:54  lr: 0.000063  loss: 2.8502 (3.2943)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [480/781]  eta: 0:00:52  lr: 0.000063  loss: 2.8780 (3.2877)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [490/781]  eta: 0:00:50  lr: 0.000063  loss: 2.9062 (3.2810)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [500/781]  eta: 0:00:48  lr: 0.000063  loss: 2.9053 (3.2741)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [510/781]  eta: 0:00:46  lr: 0.000063  loss: 2.8188 (3.2649)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [520/781]  eta: 0:00:44  lr: 0.000063  loss: 2.7748 (3.2556)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [530/781]  eta: 0:00:42  lr: 0.000063  loss: 2.7599 (3.2467)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [540/781]  eta: 0:00:40  lr: 0.000063  loss: 2.7550 (3.2399)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [550/781]  eta: 0:00:39  lr: 0.000063  loss: 2.7359 (3.2311)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [560/781]  eta: 0:00:37  lr: 0.000063  loss: 2.7303 (3.2235)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [570/781]  eta: 0:00:35  lr: 0.000063  loss: 2.7244 (3.2151)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [580/781]  eta: 0:00:33  lr: 0.000063  loss: 2.7532 (3.2080)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [590/781]  eta: 0:00:31  lr: 0.000063  loss: 2.6893 (3.1995)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [600/781]  eta: 0:00:30  lr: 0.000063  loss: 2.6889 (3.1923)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [610/781]  eta: 0:00:28  lr: 0.000063  loss: 2.6878 (3.1841)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [620/781]  eta: 0:00:26  lr: 0.000063  loss: 2.6373 (3.1759)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [630/781]  eta: 0:00:24  lr: 0.000063  loss: 2.6516 (3.1681)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [640/781]  eta: 0:00:23  lr: 0.000063  loss: 2.6805 (3.1604)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [650/781]  eta: 0:00:21  lr: 0.000063  loss: 2.6619 (3.1537)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [660/781]  eta: 0:00:19  lr: 0.000063  loss: 2.6341 (3.1469)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [670/781]  eta: 0:00:18  lr: 0.000063  loss: 2.5999 (3.1403)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [680/781]  eta: 0:00:16  lr: 0.000063  loss: 2.5831 (3.1327)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [690/781]  eta: 0:00:14  lr: 0.000063  loss: 2.5831 (3.1248)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [700/781]  eta: 0:00:13  lr: 0.000063  loss: 2.6118 (3.1195)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [710/781]  eta: 0:00:11  lr: 0.000063  loss: 2.6112 (3.1126)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [720/781]  eta: 0:00:09  lr: 0.000063  loss: 2.5593 (3.1052)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [730/781]  eta: 0:00:08  lr: 0.000063  loss: 2.5593 (3.1001)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [740/781]  eta: 0:00:06  lr: 0.000063  loss: 2.5124 (3.0922)  time: 0.1493  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [750/781]  eta: 0:00:04  lr: 0.000063  loss: 2.5103 (3.0862)  time: 0.1486  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [760/781]  eta: 0:00:03  lr: 0.000063  loss: 2.5227 (3.0800)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [770/781]  eta: 0:00:01  lr: 0.000063  loss: 2.5085 (3.0735)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000063  loss: 2.5138 (3.0661)  time: 0.1391  data: 0.0007  max mem: 4938\n",
            "Epoch: [0] Total time: 0:02:05 (0.1605 s / it)\n",
            "Averaged stats: lr: 0.000063  loss: 2.5138 (3.0661)\n",
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Test:  [ 0/53]  eta: 0:00:50  loss: 1.9362 (1.9362)  acc1: 57.8125 (57.8125)  acc5: 80.7292 (80.7292)  time: 0.9504  data: 0.7940  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 2.4855 (2.6576)  acc1: 42.7083 (40.6250)  acc5: 76.5625 (72.3011)  time: 0.1615  data: 0.1185  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 2.7191 (2.8286)  acc1: 31.7708 (36.3591)  acc5: 68.7500 (68.0308)  time: 0.1037  data: 0.0726  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 2.7632 (2.7866)  acc1: 36.9792 (39.4321)  acc5: 67.1875 (68.8508)  time: 0.1290  data: 0.0983  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 2.8120 (2.8710)  acc1: 36.9792 (37.7795)  acc5: 67.1875 (66.5650)  time: 0.1213  data: 0.0906  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 2.9798 (2.8491)  acc1: 31.7708 (38.2149)  acc5: 60.9375 (66.6973)  time: 0.1293  data: 0.0986  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 2.9798 (2.8536)  acc1: 31.7708 (38.1500)  acc5: 63.5417 (66.8400)  time: 0.1277  data: 0.0974  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1329 s / it)\n",
            "* Acc@1 38.150 Acc@5 66.840 loss 2.854\n",
            "Accuracy of the network on the 10000 test images: 38.2%\n",
            "Max accuracy: 38.15%\n",
            "Epoch: [1]  [  0/781]  eta: 0:12:29  lr: 0.000063  loss: 2.3998 (2.3998)  time: 0.9591  data: 0.7856  max mem: 4938\n",
            "Epoch: [1]  [ 10/781]  eta: 0:02:43  lr: 0.000063  loss: 2.4766 (2.6123)  time: 0.2124  data: 0.0717  max mem: 4938\n",
            "Epoch: [1]  [ 20/781]  eta: 0:02:14  lr: 0.000063  loss: 2.4756 (2.5604)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 30/781]  eta: 0:02:03  lr: 0.000063  loss: 2.4703 (2.5639)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 40/781]  eta: 0:01:56  lr: 0.000063  loss: 2.4970 (2.5583)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 50/781]  eta: 0:01:52  lr: 0.000063  loss: 2.4837 (2.5532)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 60/781]  eta: 0:01:49  lr: 0.000063  loss: 2.4811 (2.5553)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 70/781]  eta: 0:01:46  lr: 0.000063  loss: 2.4810 (2.5581)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 80/781]  eta: 0:01:44  lr: 0.000063  loss: 2.4498 (2.5535)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [ 90/781]  eta: 0:01:41  lr: 0.000063  loss: 2.4498 (2.5448)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [100/781]  eta: 0:01:39  lr: 0.000063  loss: 2.4490 (2.5505)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [110/781]  eta: 0:01:37  lr: 0.000063  loss: 2.4271 (2.5403)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [120/781]  eta: 0:01:35  lr: 0.000063  loss: 2.4228 (2.5314)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [130/781]  eta: 0:01:33  lr: 0.000063  loss: 2.4243 (2.5362)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [140/781]  eta: 0:01:32  lr: 0.000063  loss: 2.4090 (2.5265)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [150/781]  eta: 0:01:30  lr: 0.000063  loss: 2.3837 (2.5185)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [160/781]  eta: 0:01:29  lr: 0.000063  loss: 2.3514 (2.5160)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [170/781]  eta: 0:01:27  lr: 0.000063  loss: 2.3464 (2.5093)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [180/781]  eta: 0:01:25  lr: 0.000063  loss: 2.3559 (2.5035)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [190/781]  eta: 0:01:24  lr: 0.000063  loss: 2.3596 (2.4971)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [200/781]  eta: 0:01:22  lr: 0.000063  loss: 2.3465 (2.5013)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [210/781]  eta: 0:01:21  lr: 0.000063  loss: 2.3465 (2.5032)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [220/781]  eta: 0:01:19  lr: 0.000063  loss: 2.3199 (2.4958)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [230/781]  eta: 0:01:18  lr: 0.000063  loss: 2.3335 (2.4896)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [240/781]  eta: 0:01:16  lr: 0.000063  loss: 2.3335 (2.4852)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [250/781]  eta: 0:01:15  lr: 0.000063  loss: 2.2903 (2.4816)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [260/781]  eta: 0:01:13  lr: 0.000063  loss: 2.3126 (2.4748)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [270/781]  eta: 0:01:12  lr: 0.000063  loss: 2.3115 (2.4700)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [280/781]  eta: 0:01:10  lr: 0.000063  loss: 2.2905 (2.4636)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [290/781]  eta: 0:01:09  lr: 0.000063  loss: 2.3135 (2.4608)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [300/781]  eta: 0:01:07  lr: 0.000063  loss: 2.3269 (2.4583)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [310/781]  eta: 0:01:06  lr: 0.000063  loss: 2.2503 (2.4535)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [320/781]  eta: 0:01:04  lr: 0.000063  loss: 2.2466 (2.4551)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [330/781]  eta: 0:01:03  lr: 0.000063  loss: 2.2406 (2.4484)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [340/781]  eta: 0:01:02  lr: 0.000063  loss: 2.2279 (2.4472)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [350/781]  eta: 0:01:00  lr: 0.000063  loss: 2.2946 (2.4455)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [360/781]  eta: 0:00:59  lr: 0.000063  loss: 2.2927 (2.4429)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [370/781]  eta: 0:00:57  lr: 0.000063  loss: 2.2434 (2.4391)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [380/781]  eta: 0:00:56  lr: 0.000063  loss: 2.2344 (2.4344)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [390/781]  eta: 0:00:55  lr: 0.000063  loss: 2.2905 (2.4335)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [400/781]  eta: 0:00:53  lr: 0.000063  loss: 2.2932 (2.4315)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [410/781]  eta: 0:00:52  lr: 0.000063  loss: 2.2312 (2.4305)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [420/781]  eta: 0:00:50  lr: 0.000063  loss: 2.2278 (2.4292)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [430/781]  eta: 0:00:49  lr: 0.000063  loss: 2.2397 (2.4283)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [440/781]  eta: 0:00:47  lr: 0.000063  loss: 2.1858 (2.4240)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [450/781]  eta: 0:00:46  lr: 0.000063  loss: 2.2135 (2.4283)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [460/781]  eta: 0:00:45  lr: 0.000063  loss: 2.2358 (2.4298)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [470/781]  eta: 0:00:43  lr: 0.000063  loss: 2.2102 (2.4264)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [480/781]  eta: 0:00:42  lr: 0.000063  loss: 2.2138 (2.4250)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [490/781]  eta: 0:00:40  lr: 0.000063  loss: 2.2138 (2.4238)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [500/781]  eta: 0:00:39  lr: 0.000063  loss: 2.2111 (2.4240)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [510/781]  eta: 0:00:38  lr: 0.000063  loss: 2.2186 (2.4198)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [520/781]  eta: 0:00:36  lr: 0.000063  loss: 2.2145 (2.4167)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [530/781]  eta: 0:00:35  lr: 0.000063  loss: 2.2113 (2.4129)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [540/781]  eta: 0:00:33  lr: 0.000063  loss: 2.1810 (2.4102)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [550/781]  eta: 0:00:32  lr: 0.000063  loss: 2.1682 (2.4079)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [560/781]  eta: 0:00:30  lr: 0.000063  loss: 2.1485 (2.4063)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [570/781]  eta: 0:00:29  lr: 0.000063  loss: 2.1776 (2.4044)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [580/781]  eta: 0:00:28  lr: 0.000063  loss: 2.1978 (2.4026)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [590/781]  eta: 0:00:26  lr: 0.000063  loss: 2.2106 (2.4024)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [600/781]  eta: 0:00:25  lr: 0.000063  loss: 2.2547 (2.4003)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [610/781]  eta: 0:00:23  lr: 0.000063  loss: 2.2262 (2.3987)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [620/781]  eta: 0:00:22  lr: 0.000063  loss: 2.1901 (2.3974)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [630/781]  eta: 0:00:21  lr: 0.000063  loss: 2.1585 (2.3952)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [640/781]  eta: 0:00:19  lr: 0.000063  loss: 2.1357 (2.3911)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [650/781]  eta: 0:00:18  lr: 0.000063  loss: 2.1645 (2.3888)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [660/781]  eta: 0:00:16  lr: 0.000063  loss: 2.1645 (2.3875)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [670/781]  eta: 0:00:15  lr: 0.000063  loss: 2.1898 (2.3852)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [680/781]  eta: 0:00:14  lr: 0.000063  loss: 2.2154 (2.3839)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [690/781]  eta: 0:00:12  lr: 0.000063  loss: 2.1842 (2.3810)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [700/781]  eta: 0:00:11  lr: 0.000063  loss: 2.1347 (2.3781)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [710/781]  eta: 0:00:09  lr: 0.000063  loss: 2.1175 (2.3772)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [720/781]  eta: 0:00:08  lr: 0.000063  loss: 2.1386 (2.3766)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [730/781]  eta: 0:00:07  lr: 0.000063  loss: 2.1266 (2.3747)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [740/781]  eta: 0:00:05  lr: 0.000063  loss: 2.1497 (2.3767)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [750/781]  eta: 0:00:04  lr: 0.000063  loss: 2.1907 (2.3743)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [760/781]  eta: 0:00:02  lr: 0.000063  loss: 2.1907 (2.3732)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [770/781]  eta: 0:00:01  lr: 0.000063  loss: 2.1509 (2.3700)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000063  loss: 2.1310 (2.3687)  time: 0.1376  data: 0.0006  max mem: 4938\n",
            "Epoch: [1] Total time: 0:01:49 (0.1398 s / it)\n",
            "Averaged stats: lr: 0.000063  loss: 2.1310 (2.3687)\n",
            "Test:  [ 0/53]  eta: 0:00:45  loss: 1.2970 (1.2970)  acc1: 71.3542 (71.3542)  acc5: 90.1042 (90.1042)  time: 0.8601  data: 0.8291  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.5706 (1.6350)  acc1: 65.1042 (63.5417)  acc5: 86.9792 (86.0322)  time: 0.1779  data: 0.1466  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.6652 (1.7267)  acc1: 60.4167 (62.2768)  acc5: 84.8958 (84.4494)  time: 0.1231  data: 0.0921  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.8666 (1.7654)  acc1: 60.4167 (61.6431)  acc5: 81.2500 (84.1566)  time: 0.1285  data: 0.0978  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.9517 (1.8339)  acc1: 59.3750 (59.9466)  acc5: 82.2917 (83.1809)  time: 0.1282  data: 0.0975  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.9501 (1.8417)  acc1: 54.1667 (59.3648)  acc5: 82.8125 (83.1597)  time: 0.1274  data: 0.0967  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.9501 (1.8506)  acc1: 54.1667 (59.2400)  acc5: 82.8125 (83.2700)  time: 0.1125  data: 0.0826  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1352 s / it)\n",
            "* Acc@1 59.240 Acc@5 83.270 loss 1.851\n",
            "Accuracy of the network on the 10000 test images: 59.2%\n",
            "Max accuracy: 59.24%\n",
            "Epoch: [2]  [  0/781]  eta: 0:11:36  lr: 0.000062  loss: 2.0190 (2.0190)  time: 0.8913  data: 0.7423  max mem: 4938\n",
            "Epoch: [2]  [ 10/781]  eta: 0:02:41  lr: 0.000062  loss: 2.0741 (2.0746)  time: 0.2090  data: 0.0678  max mem: 4938\n",
            "Epoch: [2]  [ 20/781]  eta: 0:02:13  lr: 0.000062  loss: 2.0741 (2.0830)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 30/781]  eta: 0:02:03  lr: 0.000062  loss: 2.1131 (2.1325)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 40/781]  eta: 0:01:56  lr: 0.000062  loss: 2.0840 (2.1278)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 50/781]  eta: 0:01:52  lr: 0.000062  loss: 2.0487 (2.1541)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 60/781]  eta: 0:01:49  lr: 0.000062  loss: 2.1089 (2.1596)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 70/781]  eta: 0:01:46  lr: 0.000062  loss: 2.0934 (2.1728)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 80/781]  eta: 0:01:44  lr: 0.000062  loss: 2.0946 (2.1666)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [ 90/781]  eta: 0:01:41  lr: 0.000062  loss: 2.0946 (2.1629)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [100/781]  eta: 0:01:39  lr: 0.000062  loss: 2.0350 (2.1572)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [110/781]  eta: 0:01:37  lr: 0.000062  loss: 2.0344 (2.1455)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [120/781]  eta: 0:01:35  lr: 0.000062  loss: 2.0328 (2.1593)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [130/781]  eta: 0:01:34  lr: 0.000062  loss: 2.0779 (2.1657)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [140/781]  eta: 0:01:32  lr: 0.000062  loss: 2.0750 (2.1709)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [150/781]  eta: 0:01:30  lr: 0.000062  loss: 2.0505 (2.1703)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [160/781]  eta: 0:01:29  lr: 0.000062  loss: 2.1062 (2.1831)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [170/781]  eta: 0:01:27  lr: 0.000062  loss: 2.0954 (2.1795)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [180/781]  eta: 0:01:26  lr: 0.000062  loss: 2.0691 (2.1727)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [190/781]  eta: 0:01:24  lr: 0.000062  loss: 2.0489 (2.1644)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [200/781]  eta: 0:01:22  lr: 0.000062  loss: 2.0331 (2.1592)  time: 0.1390  data: 0.0004  max mem: 4938\n",
            "Epoch: [2]  [210/781]  eta: 0:01:21  lr: 0.000062  loss: 2.0331 (2.1540)  time: 0.1418  data: 0.0033  max mem: 4938\n",
            "Epoch: [2]  [220/781]  eta: 0:01:20  lr: 0.000062  loss: 2.0239 (2.1493)  time: 0.1443  data: 0.0048  max mem: 4938\n",
            "Epoch: [2]  [230/781]  eta: 0:01:18  lr: 0.000062  loss: 2.0302 (2.1442)  time: 0.1419  data: 0.0029  max mem: 4938\n",
            "Epoch: [2]  [240/781]  eta: 0:01:17  lr: 0.000062  loss: 2.0302 (2.1468)  time: 0.1412  data: 0.0031  max mem: 4938\n",
            "Epoch: [2]  [250/781]  eta: 0:01:15  lr: 0.000062  loss: 2.0860 (2.1484)  time: 0.1432  data: 0.0052  max mem: 4938\n",
            "Epoch: [2]  [260/781]  eta: 0:01:14  lr: 0.000062  loss: 2.0565 (2.1457)  time: 0.1421  data: 0.0041  max mem: 4938\n",
            "Epoch: [2]  [270/781]  eta: 0:01:13  lr: 0.000062  loss: 2.0061 (2.1406)  time: 0.1453  data: 0.0073  max mem: 4938\n",
            "Epoch: [2]  [280/781]  eta: 0:01:11  lr: 0.000062  loss: 2.0447 (2.1431)  time: 0.1440  data: 0.0066  max mem: 4938\n",
            "Epoch: [2]  [290/781]  eta: 0:01:10  lr: 0.000062  loss: 2.0477 (2.1419)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [300/781]  eta: 0:01:08  lr: 0.000062  loss: 2.0331 (2.1384)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [310/781]  eta: 0:01:07  lr: 0.000062  loss: 2.0083 (2.1368)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [320/781]  eta: 0:01:05  lr: 0.000062  loss: 2.0083 (2.1333)  time: 0.1392  data: 0.0020  max mem: 4938\n",
            "Epoch: [2]  [330/781]  eta: 0:01:04  lr: 0.000062  loss: 2.0126 (2.1346)  time: 0.1394  data: 0.0024  max mem: 4938\n",
            "Epoch: [2]  [340/781]  eta: 0:01:02  lr: 0.000062  loss: 2.0452 (2.1325)  time: 0.1378  data: 0.0007  max mem: 4938\n",
            "Epoch: [2]  [350/781]  eta: 0:01:01  lr: 0.000062  loss: 2.0814 (2.1331)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [360/781]  eta: 0:00:59  lr: 0.000062  loss: 2.0960 (2.1348)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [370/781]  eta: 0:00:58  lr: 0.000062  loss: 2.0056 (2.1315)  time: 0.1382  data: 0.0009  max mem: 4938\n",
            "Epoch: [2]  [380/781]  eta: 0:00:56  lr: 0.000062  loss: 1.9973 (2.1314)  time: 0.1387  data: 0.0009  max mem: 4938\n",
            "Epoch: [2]  [390/781]  eta: 0:00:55  lr: 0.000062  loss: 1.9988 (2.1350)  time: 0.1441  data: 0.0056  max mem: 4938\n",
            "Epoch: [2]  [400/781]  eta: 0:00:54  lr: 0.000062  loss: 1.9887 (2.1383)  time: 0.1464  data: 0.0062  max mem: 4938\n",
            "Epoch: [2]  [410/781]  eta: 0:00:52  lr: 0.000062  loss: 1.9887 (2.1354)  time: 0.1428  data: 0.0025  max mem: 4938\n",
            "Epoch: [2]  [420/781]  eta: 0:00:51  lr: 0.000062  loss: 1.9998 (2.1384)  time: 0.1419  data: 0.0024  max mem: 4938\n",
            "Epoch: [2]  [430/781]  eta: 0:00:49  lr: 0.000062  loss: 2.1130 (2.1402)  time: 0.1426  data: 0.0032  max mem: 4938\n",
            "Epoch: [2]  [440/781]  eta: 0:00:48  lr: 0.000062  loss: 2.0280 (2.1386)  time: 0.1414  data: 0.0027  max mem: 4938\n",
            "Epoch: [2]  [450/781]  eta: 0:00:46  lr: 0.000062  loss: 2.0282 (2.1393)  time: 0.1392  data: 0.0012  max mem: 4938\n",
            "Epoch: [2]  [460/781]  eta: 0:00:45  lr: 0.000062  loss: 2.0312 (2.1414)  time: 0.1400  data: 0.0021  max mem: 4938\n",
            "Epoch: [2]  [470/781]  eta: 0:00:44  lr: 0.000062  loss: 2.0126 (2.1417)  time: 0.1396  data: 0.0012  max mem: 4938\n",
            "Epoch: [2]  [480/781]  eta: 0:00:42  lr: 0.000062  loss: 1.9845 (2.1396)  time: 0.1433  data: 0.0045  max mem: 4938\n",
            "Epoch: [2]  [490/781]  eta: 0:00:41  lr: 0.000062  loss: 1.9210 (2.1376)  time: 0.1434  data: 0.0045  max mem: 4938\n",
            "Epoch: [2]  [500/781]  eta: 0:00:39  lr: 0.000062  loss: 2.0338 (2.1375)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [510/781]  eta: 0:00:38  lr: 0.000062  loss: 2.0340 (2.1359)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [520/781]  eta: 0:00:36  lr: 0.000062  loss: 2.0148 (2.1327)  time: 0.1388  data: 0.0013  max mem: 4938\n",
            "Epoch: [2]  [530/781]  eta: 0:00:35  lr: 0.000062  loss: 1.9494 (2.1306)  time: 0.1402  data: 0.0032  max mem: 4938\n",
            "Epoch: [2]  [540/781]  eta: 0:00:34  lr: 0.000062  loss: 1.9710 (2.1298)  time: 0.1390  data: 0.0021  max mem: 4938\n",
            "Epoch: [2]  [550/781]  eta: 0:00:32  lr: 0.000062  loss: 1.9740 (2.1296)  time: 0.1384  data: 0.0015  max mem: 4938\n",
            "Epoch: [2]  [560/781]  eta: 0:00:31  lr: 0.000062  loss: 1.9655 (2.1279)  time: 0.1423  data: 0.0039  max mem: 4938\n",
            "Epoch: [2]  [570/781]  eta: 0:00:29  lr: 0.000062  loss: 1.9655 (2.1252)  time: 0.1450  data: 0.0061  max mem: 4938\n",
            "Epoch: [2]  [580/781]  eta: 0:00:28  lr: 0.000062  loss: 1.9445 (2.1216)  time: 0.1440  data: 0.0056  max mem: 4938\n",
            "Epoch: [2]  [590/781]  eta: 0:00:27  lr: 0.000062  loss: 1.9431 (2.1217)  time: 0.1407  data: 0.0022  max mem: 4938\n",
            "Epoch: [2]  [600/781]  eta: 0:00:25  lr: 0.000062  loss: 1.9462 (2.1184)  time: 0.1410  data: 0.0023  max mem: 4938\n",
            "Epoch: [2]  [610/781]  eta: 0:00:24  lr: 0.000062  loss: 1.9681 (2.1209)  time: 0.1419  data: 0.0033  max mem: 4938\n",
            "Epoch: [2]  [620/781]  eta: 0:00:22  lr: 0.000062  loss: 1.9911 (2.1206)  time: 0.1394  data: 0.0012  max mem: 4938\n",
            "Epoch: [2]  [630/781]  eta: 0:00:21  lr: 0.000062  loss: 1.9582 (2.1197)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [640/781]  eta: 0:00:19  lr: 0.000062  loss: 1.9704 (2.1212)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [650/781]  eta: 0:00:18  lr: 0.000062  loss: 2.0010 (2.1221)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [660/781]  eta: 0:00:17  lr: 0.000062  loss: 2.0285 (2.1210)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [670/781]  eta: 0:00:15  lr: 0.000062  loss: 2.0108 (2.1184)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [680/781]  eta: 0:00:14  lr: 0.000062  loss: 1.9714 (2.1188)  time: 0.1420  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [690/781]  eta: 0:00:12  lr: 0.000062  loss: 1.9880 (2.1189)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [700/781]  eta: 0:00:11  lr: 0.000062  loss: 1.9880 (2.1188)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [710/781]  eta: 0:00:10  lr: 0.000062  loss: 1.9721 (2.1169)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [720/781]  eta: 0:00:08  lr: 0.000062  loss: 1.9486 (2.1152)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [730/781]  eta: 0:00:07  lr: 0.000062  loss: 1.9767 (2.1161)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [740/781]  eta: 0:00:05  lr: 0.000062  loss: 1.9199 (2.1140)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [750/781]  eta: 0:00:04  lr: 0.000062  loss: 1.9243 (2.1119)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [2]  [760/781]  eta: 0:00:02  lr: 0.000062  loss: 1.9672 (2.1111)  time: 0.1402  data: 0.0005  max mem: 4938\n",
            "Epoch: [2]  [770/781]  eta: 0:00:01  lr: 0.000062  loss: 1.9672 (2.1112)  time: 0.1440  data: 0.0059  max mem: 4938\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000062  loss: 1.9913 (2.1131)  time: 0.1431  data: 0.0062  max mem: 4938\n",
            "Epoch: [2] Total time: 0:01:50 (0.1413 s / it)\n",
            "Averaged stats: lr: 0.000062  loss: 1.9913 (2.1131)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.2515 (1.2515)  acc1: 70.8333 (70.8333)  acc5: 92.1875 (92.1875)  time: 0.8391  data: 0.8082  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.3506 (1.3809)  acc1: 70.8333 (69.9811)  acc5: 90.6250 (90.1989)  time: 0.1741  data: 0.1434  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.4110 (1.4493)  acc1: 66.6667 (68.7252)  acc5: 89.0625 (88.3929)  time: 0.1273  data: 0.0966  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.6413 (1.5039)  acc1: 65.1042 (67.8427)  acc5: 85.9375 (87.7520)  time: 0.1279  data: 0.0972  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.6924 (1.5948)  acc1: 63.5417 (65.2693)  acc5: 84.8958 (86.5091)  time: 0.1279  data: 0.0973  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.6847 (1.5793)  acc1: 63.5417 (65.8088)  acc5: 85.4167 (86.7545)  time: 0.1278  data: 0.0971  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.6472 (1.5777)  acc1: 63.5417 (65.8300)  acc5: 85.9375 (86.8400)  time: 0.1072  data: 0.0775  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1343 s / it)\n",
            "* Acc@1 65.830 Acc@5 86.840 loss 1.578\n",
            "Accuracy of the network on the 10000 test images: 65.8%\n",
            "Max accuracy: 65.83%\n",
            "Epoch: [3]  [  0/781]  eta: 0:12:39  lr: 0.000061  loss: 1.8996 (1.8996)  time: 0.9730  data: 0.8268  max mem: 4938\n",
            "Epoch: [3]  [ 10/781]  eta: 0:02:48  lr: 0.000061  loss: 2.0017 (2.1942)  time: 0.2183  data: 0.0755  max mem: 4938\n",
            "Epoch: [3]  [ 20/781]  eta: 0:02:17  lr: 0.000061  loss: 1.9432 (2.1166)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [ 30/781]  eta: 0:02:05  lr: 0.000061  loss: 1.9295 (2.0996)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [ 40/781]  eta: 0:01:58  lr: 0.000061  loss: 1.9150 (2.0814)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [ 50/781]  eta: 0:01:53  lr: 0.000061  loss: 1.9416 (2.0880)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [ 60/781]  eta: 0:01:50  lr: 0.000061  loss: 1.9638 (2.0855)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [ 70/781]  eta: 0:01:48  lr: 0.000061  loss: 1.9237 (2.0715)  time: 0.1423  data: 0.0041  max mem: 4938\n",
            "Epoch: [3]  [ 80/781]  eta: 0:01:45  lr: 0.000061  loss: 1.9267 (2.0704)  time: 0.1463  data: 0.0068  max mem: 4938\n",
            "Epoch: [3]  [ 90/781]  eta: 0:01:43  lr: 0.000061  loss: 1.9267 (2.0654)  time: 0.1449  data: 0.0059  max mem: 4938\n",
            "Epoch: [3]  [100/781]  eta: 0:01:41  lr: 0.000061  loss: 1.8922 (2.0498)  time: 0.1434  data: 0.0055  max mem: 4938\n",
            "Epoch: [3]  [110/781]  eta: 0:01:40  lr: 0.000061  loss: 1.8922 (2.0387)  time: 0.1456  data: 0.0070  max mem: 4938\n",
            "Epoch: [3]  [120/781]  eta: 0:01:38  lr: 0.000061  loss: 1.8846 (2.0330)  time: 0.1441  data: 0.0051  max mem: 4938\n",
            "Epoch: [3]  [130/781]  eta: 0:01:36  lr: 0.000061  loss: 1.9030 (2.0270)  time: 0.1391  data: 0.0008  max mem: 4938\n",
            "Epoch: [3]  [140/781]  eta: 0:01:34  lr: 0.000061  loss: 1.9174 (2.0303)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [150/781]  eta: 0:01:32  lr: 0.000061  loss: 1.9298 (2.0271)  time: 0.1384  data: 0.0004  max mem: 4938\n",
            "Epoch: [3]  [160/781]  eta: 0:01:30  lr: 0.000061  loss: 1.8802 (2.0275)  time: 0.1386  data: 0.0004  max mem: 4938\n",
            "Epoch: [3]  [170/781]  eta: 0:01:30  lr: 0.000061  loss: 1.8927 (2.0244)  time: 0.1531  data: 0.0047  max mem: 4938\n",
            "Epoch: [3]  [180/781]  eta: 0:01:28  lr: 0.000061  loss: 1.9551 (2.0315)  time: 0.1524  data: 0.0047  max mem: 4938\n",
            "Epoch: [3]  [190/781]  eta: 0:01:26  lr: 0.000061  loss: 1.9551 (2.0344)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [200/781]  eta: 0:01:24  lr: 0.000061  loss: 1.9288 (2.0416)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [210/781]  eta: 0:01:23  lr: 0.000061  loss: 1.9298 (2.0410)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [220/781]  eta: 0:01:21  lr: 0.000061  loss: 1.9161 (2.0370)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [230/781]  eta: 0:01:19  lr: 0.000061  loss: 1.9316 (2.0412)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [240/781]  eta: 0:01:18  lr: 0.000061  loss: 1.9045 (2.0437)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [250/781]  eta: 0:01:16  lr: 0.000061  loss: 1.8697 (2.0432)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [260/781]  eta: 0:01:15  lr: 0.000061  loss: 1.8697 (2.0372)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [270/781]  eta: 0:01:13  lr: 0.000061  loss: 1.8852 (2.0355)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [280/781]  eta: 0:01:12  lr: 0.000061  loss: 1.9452 (2.0379)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [290/781]  eta: 0:01:10  lr: 0.000061  loss: 1.9416 (2.0368)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [300/781]  eta: 0:01:09  lr: 0.000061  loss: 1.9117 (2.0370)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [310/781]  eta: 0:01:07  lr: 0.000061  loss: 1.9323 (2.0332)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [320/781]  eta: 0:01:06  lr: 0.000061  loss: 1.9135 (2.0321)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [330/781]  eta: 0:01:04  lr: 0.000061  loss: 1.9201 (2.0303)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [340/781]  eta: 0:01:03  lr: 0.000061  loss: 1.9260 (2.0286)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [350/781]  eta: 0:01:01  lr: 0.000061  loss: 1.9241 (2.0288)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [360/781]  eta: 0:01:00  lr: 0.000061  loss: 1.9234 (2.0286)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [370/781]  eta: 0:00:58  lr: 0.000061  loss: 1.9234 (2.0323)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [380/781]  eta: 0:00:57  lr: 0.000061  loss: 1.9067 (2.0293)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [390/781]  eta: 0:00:55  lr: 0.000061  loss: 1.9184 (2.0340)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [400/781]  eta: 0:00:54  lr: 0.000061  loss: 1.9570 (2.0369)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [410/781]  eta: 0:00:52  lr: 0.000061  loss: 1.9085 (2.0327)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [420/781]  eta: 0:00:51  lr: 0.000061  loss: 1.8496 (2.0292)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [430/781]  eta: 0:00:50  lr: 0.000061  loss: 1.8927 (2.0284)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [440/781]  eta: 0:00:48  lr: 0.000061  loss: 1.8877 (2.0263)  time: 0.1423  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [450/781]  eta: 0:00:47  lr: 0.000061  loss: 1.8485 (2.0235)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [460/781]  eta: 0:00:45  lr: 0.000061  loss: 1.8955 (2.0218)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [470/781]  eta: 0:00:44  lr: 0.000061  loss: 1.9078 (2.0231)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [480/781]  eta: 0:00:42  lr: 0.000061  loss: 1.8898 (2.0202)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [490/781]  eta: 0:00:41  lr: 0.000061  loss: 1.8653 (2.0178)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [500/781]  eta: 0:00:39  lr: 0.000061  loss: 1.8940 (2.0182)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [510/781]  eta: 0:00:38  lr: 0.000061  loss: 1.9121 (2.0172)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [520/781]  eta: 0:00:37  lr: 0.000061  loss: 1.9080 (2.0184)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [530/781]  eta: 0:00:35  lr: 0.000061  loss: 1.8748 (2.0158)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [540/781]  eta: 0:00:34  lr: 0.000061  loss: 1.8277 (2.0123)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [550/781]  eta: 0:00:32  lr: 0.000061  loss: 1.8800 (2.0177)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [560/781]  eta: 0:00:31  lr: 0.000061  loss: 1.9381 (2.0157)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [570/781]  eta: 0:00:29  lr: 0.000061  loss: 1.9074 (2.0141)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [580/781]  eta: 0:00:28  lr: 0.000061  loss: 1.8793 (2.0145)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [590/781]  eta: 0:00:27  lr: 0.000061  loss: 1.8869 (2.0146)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [600/781]  eta: 0:00:25  lr: 0.000061  loss: 1.8986 (2.0140)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [610/781]  eta: 0:00:24  lr: 0.000061  loss: 1.8486 (2.0150)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [620/781]  eta: 0:00:22  lr: 0.000061  loss: 1.8964 (2.0130)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [630/781]  eta: 0:00:21  lr: 0.000061  loss: 1.8964 (2.0130)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [640/781]  eta: 0:00:19  lr: 0.000061  loss: 1.9430 (2.0113)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [650/781]  eta: 0:00:18  lr: 0.000061  loss: 1.9426 (2.0129)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [660/781]  eta: 0:00:17  lr: 0.000061  loss: 1.9449 (2.0129)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [670/781]  eta: 0:00:15  lr: 0.000061  loss: 1.8974 (2.0144)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [680/781]  eta: 0:00:14  lr: 0.000061  loss: 1.8778 (2.0122)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [690/781]  eta: 0:00:12  lr: 0.000061  loss: 1.8474 (2.0107)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [700/781]  eta: 0:00:11  lr: 0.000061  loss: 1.8529 (2.0085)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [710/781]  eta: 0:00:10  lr: 0.000061  loss: 1.9149 (2.0101)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [720/781]  eta: 0:00:08  lr: 0.000061  loss: 1.9336 (2.0107)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [730/781]  eta: 0:00:07  lr: 0.000061  loss: 1.9468 (2.0097)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [740/781]  eta: 0:00:05  lr: 0.000061  loss: 1.9468 (2.0096)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [750/781]  eta: 0:00:04  lr: 0.000061  loss: 1.8198 (2.0083)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [760/781]  eta: 0:00:02  lr: 0.000061  loss: 1.8021 (2.0083)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [770/781]  eta: 0:00:01  lr: 0.000061  loss: 1.9088 (2.0080)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [3]  [780/781]  eta: 0:00:00  lr: 0.000061  loss: 1.8862 (2.0076)  time: 0.1382  data: 0.0006  max mem: 4938\n",
            "Epoch: [3] Total time: 0:01:50 (0.1410 s / it)\n",
            "Averaged stats: lr: 0.000061  loss: 1.8862 (2.0076)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 1.0957 (1.0957)  acc1: 75.0000 (75.0000)  acc5: 93.2292 (93.2292)  time: 0.8455  data: 0.8145  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2653 (1.2548)  acc1: 72.9167 (72.1117)  acc5: 91.6667 (91.0985)  time: 0.1687  data: 0.1379  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.3399 (1.3065)  acc1: 70.8333 (71.0566)  acc5: 88.5417 (89.9554)  time: 0.1089  data: 0.0782  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.4225 (1.3546)  acc1: 68.7500 (70.2453)  acc5: 88.0208 (89.3985)  time: 0.1252  data: 0.0945  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4870 (1.4284)  acc1: 67.7083 (68.4451)  acc5: 87.5000 (88.4146)  time: 0.1140  data: 0.0833  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3843 (1.4148)  acc1: 67.7083 (68.5458)  acc5: 87.5000 (88.3885)  time: 0.1102  data: 0.0795  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.4870 (1.4330)  acc1: 67.1875 (68.2600)  acc5: 87.5000 (88.3700)  time: 0.1092  data: 0.0795  max mem: 4938\n",
            "Test: Total time: 0:00:06 (0.1256 s / it)\n",
            "* Acc@1 68.260 Acc@5 88.370 loss 1.433\n",
            "Accuracy of the network on the 10000 test images: 68.3%\n",
            "Max accuracy: 68.26%\n",
            "Epoch: [4]  [  0/781]  eta: 0:11:53  lr: 0.000060  loss: 1.9447 (1.9447)  time: 0.9132  data: 0.7707  max mem: 4938\n",
            "Epoch: [4]  [ 10/781]  eta: 0:02:44  lr: 0.000060  loss: 1.8742 (1.9022)  time: 0.2135  data: 0.0748  max mem: 4938\n",
            "Epoch: [4]  [ 20/781]  eta: 0:02:17  lr: 0.000060  loss: 1.8742 (1.9451)  time: 0.1437  data: 0.0062  max mem: 4938\n",
            "Epoch: [4]  [ 30/781]  eta: 0:02:05  lr: 0.000060  loss: 1.8670 (1.9488)  time: 0.1414  data: 0.0037  max mem: 4938\n",
            "Epoch: [4]  [ 40/781]  eta: 0:01:58  lr: 0.000060  loss: 1.8624 (1.9281)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [ 50/781]  eta: 0:01:54  lr: 0.000060  loss: 1.7959 (1.9240)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [ 60/781]  eta: 0:01:50  lr: 0.000060  loss: 1.7959 (1.9310)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [ 70/781]  eta: 0:01:47  lr: 0.000060  loss: 1.8101 (1.9327)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [ 80/781]  eta: 0:01:45  lr: 0.000060  loss: 1.8154 (1.9364)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [ 90/781]  eta: 0:01:42  lr: 0.000060  loss: 1.7979 (1.9291)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [100/781]  eta: 0:01:40  lr: 0.000060  loss: 1.7666 (1.9171)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [110/781]  eta: 0:01:38  lr: 0.000060  loss: 1.8071 (1.9158)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [120/781]  eta: 0:01:36  lr: 0.000060  loss: 1.8071 (1.9069)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [130/781]  eta: 0:01:35  lr: 0.000060  loss: 1.7996 (1.8986)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [140/781]  eta: 0:01:33  lr: 0.000060  loss: 1.8041 (1.9014)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [150/781]  eta: 0:01:31  lr: 0.000060  loss: 1.8350 (1.9105)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [160/781]  eta: 0:01:29  lr: 0.000060  loss: 1.8676 (1.9089)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [170/781]  eta: 0:01:27  lr: 0.000060  loss: 1.8757 (1.9123)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [180/781]  eta: 0:01:26  lr: 0.000060  loss: 1.8263 (1.9095)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [190/781]  eta: 0:01:24  lr: 0.000060  loss: 1.8069 (1.9037)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [200/781]  eta: 0:01:23  lr: 0.000060  loss: 1.8045 (1.9053)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [210/781]  eta: 0:01:21  lr: 0.000060  loss: 1.8135 (1.9067)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [220/781]  eta: 0:01:20  lr: 0.000060  loss: 1.8400 (1.9158)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [230/781]  eta: 0:01:18  lr: 0.000060  loss: 1.8942 (1.9248)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [240/781]  eta: 0:01:17  lr: 0.000060  loss: 1.8942 (1.9263)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [250/781]  eta: 0:01:15  lr: 0.000060  loss: 1.8337 (1.9238)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [260/781]  eta: 0:01:14  lr: 0.000060  loss: 1.8481 (1.9249)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [270/781]  eta: 0:01:12  lr: 0.000060  loss: 1.8299 (1.9221)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [280/781]  eta: 0:01:11  lr: 0.000060  loss: 1.8511 (1.9212)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [290/781]  eta: 0:01:09  lr: 0.000060  loss: 1.8593 (1.9220)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [300/781]  eta: 0:01:08  lr: 0.000060  loss: 1.7811 (1.9190)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [310/781]  eta: 0:01:06  lr: 0.000060  loss: 1.7934 (1.9204)  time: 0.1424  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [320/781]  eta: 0:01:05  lr: 0.000060  loss: 1.8629 (1.9217)  time: 0.1417  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [330/781]  eta: 0:01:03  lr: 0.000060  loss: 1.8139 (1.9199)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [340/781]  eta: 0:01:02  lr: 0.000060  loss: 1.7630 (1.9194)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [350/781]  eta: 0:01:01  lr: 0.000060  loss: 1.8060 (1.9177)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [360/781]  eta: 0:00:59  lr: 0.000060  loss: 1.8232 (1.9180)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [370/781]  eta: 0:00:58  lr: 0.000060  loss: 1.8290 (1.9195)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [380/781]  eta: 0:00:56  lr: 0.000060  loss: 1.9661 (1.9279)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [390/781]  eta: 0:00:55  lr: 0.000060  loss: 1.8651 (1.9268)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [400/781]  eta: 0:00:53  lr: 0.000060  loss: 1.8108 (1.9274)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [410/781]  eta: 0:00:52  lr: 0.000060  loss: 1.7626 (1.9274)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [420/781]  eta: 0:00:51  lr: 0.000060  loss: 1.8808 (1.9320)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [430/781]  eta: 0:00:49  lr: 0.000060  loss: 1.8607 (1.9298)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [440/781]  eta: 0:00:48  lr: 0.000060  loss: 1.8265 (1.9304)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [450/781]  eta: 0:00:46  lr: 0.000060  loss: 1.8676 (1.9362)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [460/781]  eta: 0:00:45  lr: 0.000060  loss: 1.8676 (1.9347)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [470/781]  eta: 0:00:43  lr: 0.000060  loss: 1.7911 (1.9341)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [480/781]  eta: 0:00:42  lr: 0.000060  loss: 1.8049 (1.9330)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [490/781]  eta: 0:00:41  lr: 0.000060  loss: 1.8164 (1.9326)  time: 0.1421  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [500/781]  eta: 0:00:39  lr: 0.000060  loss: 1.8150 (1.9300)  time: 0.1417  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [510/781]  eta: 0:00:38  lr: 0.000060  loss: 1.8077 (1.9318)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [520/781]  eta: 0:00:36  lr: 0.000060  loss: 1.7991 (1.9290)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [530/781]  eta: 0:00:35  lr: 0.000060  loss: 1.7991 (1.9305)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [540/781]  eta: 0:00:33  lr: 0.000060  loss: 1.7892 (1.9290)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [550/781]  eta: 0:00:32  lr: 0.000060  loss: 1.7454 (1.9256)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [560/781]  eta: 0:00:31  lr: 0.000060  loss: 1.7766 (1.9285)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [570/781]  eta: 0:00:29  lr: 0.000060  loss: 1.7689 (1.9270)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [580/781]  eta: 0:00:28  lr: 0.000060  loss: 1.7531 (1.9267)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [590/781]  eta: 0:00:26  lr: 0.000060  loss: 1.8245 (1.9304)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [600/781]  eta: 0:00:25  lr: 0.000060  loss: 1.8257 (1.9299)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [610/781]  eta: 0:00:24  lr: 0.000060  loss: 1.8007 (1.9280)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [620/781]  eta: 0:00:22  lr: 0.000060  loss: 1.8234 (1.9275)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [630/781]  eta: 0:00:21  lr: 0.000060  loss: 1.8344 (1.9295)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [640/781]  eta: 0:00:19  lr: 0.000060  loss: 1.8322 (1.9314)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [650/781]  eta: 0:00:18  lr: 0.000060  loss: 1.8300 (1.9310)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [660/781]  eta: 0:00:17  lr: 0.000060  loss: 1.8251 (1.9316)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [670/781]  eta: 0:00:15  lr: 0.000060  loss: 1.8342 (1.9311)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [680/781]  eta: 0:00:14  lr: 0.000060  loss: 1.8670 (1.9323)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [690/781]  eta: 0:00:12  lr: 0.000060  loss: 1.8508 (1.9310)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [700/781]  eta: 0:00:11  lr: 0.000060  loss: 1.8111 (1.9318)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [710/781]  eta: 0:00:09  lr: 0.000060  loss: 1.8111 (1.9310)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [720/781]  eta: 0:00:08  lr: 0.000060  loss: 1.8347 (1.9300)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [730/781]  eta: 0:00:07  lr: 0.000060  loss: 1.8061 (1.9291)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [740/781]  eta: 0:00:05  lr: 0.000060  loss: 1.8061 (1.9305)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [750/781]  eta: 0:00:04  lr: 0.000060  loss: 1.7983 (1.9302)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [760/781]  eta: 0:00:02  lr: 0.000060  loss: 1.8200 (1.9316)  time: 0.1419  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [770/781]  eta: 0:00:01  lr: 0.000060  loss: 1.7505 (1.9297)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [4]  [780/781]  eta: 0:00:00  lr: 0.000060  loss: 1.7526 (1.9348)  time: 0.1393  data: 0.0006  max mem: 4938\n",
            "Epoch: [4] Total time: 0:01:49 (0.1404 s / it)\n",
            "Averaged stats: lr: 0.000060  loss: 1.7526 (1.9348)\n",
            "Test:  [ 0/53]  eta: 0:00:40  loss: 1.1216 (1.1216)  acc1: 76.0417 (76.0417)  acc5: 93.7500 (93.7500)  time: 0.7565  data: 0.7226  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2979 (1.2174)  acc1: 75.0000 (72.6326)  acc5: 90.6250 (91.2879)  time: 0.1708  data: 0.1398  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.4003 (1.2964)  acc1: 69.2708 (71.3046)  acc5: 89.0625 (90.0546)  time: 0.1318  data: 0.1011  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.4184 (1.3350)  acc1: 66.6667 (70.3293)  acc5: 89.0625 (89.6841)  time: 0.1303  data: 0.0996  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.4155 (1.3675)  acc1: 67.7083 (69.6138)  acc5: 88.5417 (89.2276)  time: 0.1235  data: 0.0928  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3466 (1.3562)  acc1: 70.3125 (69.7917)  acc5: 89.5833 (89.4812)  time: 0.1232  data: 0.0925  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3832 (1.3634)  acc1: 68.7500 (69.6500)  acc5: 90.1042 (89.5400)  time: 0.1028  data: 0.0731  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1328 s / it)\n",
            "* Acc@1 69.650 Acc@5 89.540 loss 1.363\n",
            "Accuracy of the network on the 10000 test images: 69.7%\n",
            "Max accuracy: 69.65%\n",
            "Epoch: [5]  [  0/781]  eta: 0:12:06  lr: 0.000057  loss: 1.9932 (1.9932)  time: 0.9307  data: 0.7744  max mem: 4938\n",
            "Epoch: [5]  [ 10/781]  eta: 0:02:42  lr: 0.000057  loss: 1.8283 (1.9978)  time: 0.2110  data: 0.0707  max mem: 4938\n",
            "Epoch: [5]  [ 20/781]  eta: 0:02:14  lr: 0.000057  loss: 1.7910 (1.9291)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 30/781]  eta: 0:02:03  lr: 0.000057  loss: 1.7910 (1.9096)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 40/781]  eta: 0:01:56  lr: 0.000057  loss: 1.7706 (1.8699)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 50/781]  eta: 0:01:52  lr: 0.000057  loss: 1.7491 (1.8709)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 60/781]  eta: 0:01:49  lr: 0.000057  loss: 1.7580 (1.9023)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 70/781]  eta: 0:01:46  lr: 0.000057  loss: 1.8078 (1.9437)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 80/781]  eta: 0:01:44  lr: 0.000057  loss: 1.7779 (1.9227)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [ 90/781]  eta: 0:01:42  lr: 0.000057  loss: 1.7732 (1.9398)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [100/781]  eta: 0:01:40  lr: 0.000057  loss: 1.8282 (1.9277)  time: 0.1415  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [110/781]  eta: 0:01:38  lr: 0.000057  loss: 1.8455 (1.9433)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [120/781]  eta: 0:01:36  lr: 0.000057  loss: 1.8457 (1.9444)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [130/781]  eta: 0:01:34  lr: 0.000057  loss: 1.8003 (1.9546)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [140/781]  eta: 0:01:32  lr: 0.000057  loss: 1.7991 (1.9565)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [150/781]  eta: 0:01:31  lr: 0.000057  loss: 1.7991 (1.9621)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [160/781]  eta: 0:01:29  lr: 0.000057  loss: 1.7824 (1.9505)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [170/781]  eta: 0:01:27  lr: 0.000057  loss: 1.8077 (1.9457)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [180/781]  eta: 0:01:26  lr: 0.000057  loss: 1.8077 (1.9448)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [190/781]  eta: 0:01:24  lr: 0.000057  loss: 1.7655 (1.9461)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [200/781]  eta: 0:01:23  lr: 0.000057  loss: 1.8241 (1.9442)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [210/781]  eta: 0:01:21  lr: 0.000057  loss: 1.8323 (1.9474)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [220/781]  eta: 0:01:20  lr: 0.000057  loss: 1.8006 (1.9413)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [230/781]  eta: 0:01:18  lr: 0.000057  loss: 1.7337 (1.9400)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [240/781]  eta: 0:01:17  lr: 0.000057  loss: 1.7753 (1.9379)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [250/781]  eta: 0:01:15  lr: 0.000057  loss: 1.7753 (1.9308)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [260/781]  eta: 0:01:14  lr: 0.000057  loss: 1.7704 (1.9346)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [270/781]  eta: 0:01:12  lr: 0.000057  loss: 1.8460 (1.9329)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [280/781]  eta: 0:01:11  lr: 0.000057  loss: 1.7891 (1.9274)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [290/781]  eta: 0:01:09  lr: 0.000057  loss: 1.7790 (1.9296)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [300/781]  eta: 0:01:08  lr: 0.000057  loss: 1.8043 (1.9311)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [310/781]  eta: 0:01:06  lr: 0.000057  loss: 1.8043 (1.9312)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [320/781]  eta: 0:01:05  lr: 0.000057  loss: 1.7960 (1.9320)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [330/781]  eta: 0:01:03  lr: 0.000057  loss: 1.7706 (1.9297)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [340/781]  eta: 0:01:02  lr: 0.000057  loss: 1.7840 (1.9345)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [350/781]  eta: 0:01:00  lr: 0.000057  loss: 1.7726 (1.9296)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [360/781]  eta: 0:00:59  lr: 0.000057  loss: 1.7734 (1.9308)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [370/781]  eta: 0:00:57  lr: 0.000057  loss: 1.7949 (1.9294)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [380/781]  eta: 0:00:56  lr: 0.000057  loss: 1.7494 (1.9257)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [390/781]  eta: 0:00:55  lr: 0.000057  loss: 1.7711 (1.9287)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [400/781]  eta: 0:00:53  lr: 0.000057  loss: 1.7713 (1.9243)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [410/781]  eta: 0:00:52  lr: 0.000057  loss: 1.7398 (1.9230)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [420/781]  eta: 0:00:50  lr: 0.000057  loss: 1.7569 (1.9215)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [430/781]  eta: 0:00:49  lr: 0.000057  loss: 1.8142 (1.9203)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [440/781]  eta: 0:00:47  lr: 0.000057  loss: 1.7684 (1.9221)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [450/781]  eta: 0:00:46  lr: 0.000057  loss: 1.7570 (1.9187)  time: 0.1414  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [460/781]  eta: 0:00:45  lr: 0.000057  loss: 1.7570 (1.9163)  time: 0.1497  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [470/781]  eta: 0:00:43  lr: 0.000057  loss: 1.7708 (1.9161)  time: 0.1479  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [480/781]  eta: 0:00:42  lr: 0.000057  loss: 1.8621 (1.9178)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [490/781]  eta: 0:00:41  lr: 0.000057  loss: 1.8632 (1.9180)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [500/781]  eta: 0:00:39  lr: 0.000057  loss: 1.8054 (1.9155)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [510/781]  eta: 0:00:38  lr: 0.000057  loss: 1.7464 (1.9124)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [520/781]  eta: 0:00:36  lr: 0.000057  loss: 1.7206 (1.9107)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [530/781]  eta: 0:00:35  lr: 0.000057  loss: 1.7115 (1.9099)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [540/781]  eta: 0:00:33  lr: 0.000057  loss: 1.7305 (1.9099)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [550/781]  eta: 0:00:32  lr: 0.000057  loss: 1.7317 (1.9094)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [560/781]  eta: 0:00:31  lr: 0.000057  loss: 1.7302 (1.9087)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [570/781]  eta: 0:00:29  lr: 0.000057  loss: 1.7240 (1.9064)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [580/781]  eta: 0:00:28  lr: 0.000057  loss: 1.7254 (1.9072)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [590/781]  eta: 0:00:26  lr: 0.000057  loss: 1.7267 (1.9059)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [600/781]  eta: 0:00:25  lr: 0.000057  loss: 1.7809 (1.9076)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [610/781]  eta: 0:00:24  lr: 0.000057  loss: 1.8358 (1.9082)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [620/781]  eta: 0:00:22  lr: 0.000057  loss: 1.7664 (1.9093)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [630/781]  eta: 0:00:21  lr: 0.000057  loss: 1.7359 (1.9067)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [640/781]  eta: 0:00:19  lr: 0.000057  loss: 1.7200 (1.9046)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [650/781]  eta: 0:00:18  lr: 0.000057  loss: 1.7633 (1.9080)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [660/781]  eta: 0:00:16  lr: 0.000057  loss: 1.7560 (1.9056)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [670/781]  eta: 0:00:15  lr: 0.000057  loss: 1.7359 (1.9041)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [680/781]  eta: 0:00:14  lr: 0.000057  loss: 1.7363 (1.9023)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [690/781]  eta: 0:00:12  lr: 0.000057  loss: 1.7594 (1.9033)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [700/781]  eta: 0:00:11  lr: 0.000057  loss: 1.8134 (1.9027)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [710/781]  eta: 0:00:09  lr: 0.000057  loss: 1.8068 (1.9025)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [720/781]  eta: 0:00:08  lr: 0.000057  loss: 1.7888 (1.9031)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [730/781]  eta: 0:00:07  lr: 0.000057  loss: 1.7790 (1.9050)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [740/781]  eta: 0:00:05  lr: 0.000057  loss: 1.8311 (1.9058)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [750/781]  eta: 0:00:04  lr: 0.000057  loss: 1.8311 (1.9071)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [760/781]  eta: 0:00:02  lr: 0.000057  loss: 1.8163 (1.9081)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [770/781]  eta: 0:00:01  lr: 0.000057  loss: 1.8640 (1.9111)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [5]  [780/781]  eta: 0:00:00  lr: 0.000057  loss: 1.8524 (1.9099)  time: 0.1384  data: 0.0007  max mem: 4938\n",
            "Epoch: [5] Total time: 0:01:49 (0.1402 s / it)\n",
            "Averaged stats: lr: 0.000057  loss: 1.8524 (1.9099)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.9546 (0.9546)  acc1: 79.1667 (79.1667)  acc5: 95.3125 (95.3125)  time: 0.8118  data: 0.7808  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.2377 (1.1102)  acc1: 74.4792 (75.2367)  acc5: 92.1875 (92.8030)  time: 0.1713  data: 0.1406  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2388 (1.2234)  acc1: 71.8750 (72.9911)  acc5: 90.6250 (90.7242)  time: 0.1347  data: 0.1040  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.3425 (1.2575)  acc1: 68.2292 (71.9422)  acc5: 89.5833 (90.1882)  time: 0.1358  data: 0.1051  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.3425 (1.3025)  acc1: 68.7500 (70.9731)  acc5: 90.1042 (89.7231)  time: 0.1280  data: 0.0974  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.3011 (1.3012)  acc1: 68.7500 (70.7721)  acc5: 89.5833 (89.7161)  time: 0.1283  data: 0.0976  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.3390 (1.3076)  acc1: 67.1875 (70.6800)  acc5: 90.1042 (89.7800)  time: 0.1079  data: 0.0782  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1369 s / it)\n",
            "* Acc@1 70.680 Acc@5 89.780 loss 1.308\n",
            "Accuracy of the network on the 10000 test images: 70.7%\n",
            "Max accuracy: 70.68%\n",
            "Epoch: [6]  [  0/781]  eta: 0:11:43  lr: 0.000055  loss: 1.8325 (1.8325)  time: 0.9006  data: 0.7520  max mem: 4938\n",
            "Epoch: [6]  [ 10/781]  eta: 0:02:41  lr: 0.000055  loss: 1.7523 (1.7268)  time: 0.2094  data: 0.0687  max mem: 4938\n",
            "Epoch: [6]  [ 20/781]  eta: 0:02:13  lr: 0.000055  loss: 1.7523 (1.7669)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 30/781]  eta: 0:02:02  lr: 0.000055  loss: 1.7623 (1.7835)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 40/781]  eta: 0:01:56  lr: 0.000055  loss: 1.7191 (1.8799)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 50/781]  eta: 0:01:52  lr: 0.000055  loss: 1.8136 (1.8940)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 60/781]  eta: 0:01:49  lr: 0.000055  loss: 1.7619 (1.8688)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 70/781]  eta: 0:01:46  lr: 0.000055  loss: 1.7298 (1.8726)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 80/781]  eta: 0:01:43  lr: 0.000055  loss: 1.7737 (1.8693)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [ 90/781]  eta: 0:01:41  lr: 0.000055  loss: 1.8061 (1.8830)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [100/781]  eta: 0:01:39  lr: 0.000055  loss: 1.7665 (1.8819)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [110/781]  eta: 0:01:37  lr: 0.000055  loss: 1.6966 (1.8752)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [120/781]  eta: 0:01:35  lr: 0.000055  loss: 1.6966 (1.8625)  time: 0.1371  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [130/781]  eta: 0:01:33  lr: 0.000055  loss: 1.6851 (1.8511)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [140/781]  eta: 0:01:32  lr: 0.000055  loss: 1.7337 (1.8579)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [150/781]  eta: 0:01:30  lr: 0.000055  loss: 1.7685 (1.8647)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [160/781]  eta: 0:01:29  lr: 0.000055  loss: 1.7997 (1.8691)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [170/781]  eta: 0:01:27  lr: 0.000055  loss: 1.7781 (1.8606)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [180/781]  eta: 0:01:25  lr: 0.000055  loss: 1.7739 (1.8678)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [190/781]  eta: 0:01:24  lr: 0.000055  loss: 1.7474 (1.8623)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [200/781]  eta: 0:01:22  lr: 0.000055  loss: 1.6838 (1.8669)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [210/781]  eta: 0:01:21  lr: 0.000055  loss: 1.7116 (1.8659)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [220/781]  eta: 0:01:19  lr: 0.000055  loss: 1.7313 (1.8707)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [230/781]  eta: 0:01:18  lr: 0.000055  loss: 1.7322 (1.8686)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [240/781]  eta: 0:01:16  lr: 0.000055  loss: 1.7520 (1.8681)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [250/781]  eta: 0:01:15  lr: 0.000055  loss: 1.7062 (1.8651)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [260/781]  eta: 0:01:13  lr: 0.000055  loss: 1.7354 (1.8724)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [270/781]  eta: 0:01:12  lr: 0.000055  loss: 1.7882 (1.8705)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [280/781]  eta: 0:01:10  lr: 0.000055  loss: 1.7480 (1.8656)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [290/781]  eta: 0:01:09  lr: 0.000055  loss: 1.7397 (1.8696)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [300/781]  eta: 0:01:07  lr: 0.000055  loss: 1.7528 (1.8739)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [310/781]  eta: 0:01:06  lr: 0.000055  loss: 1.7365 (1.8739)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [320/781]  eta: 0:01:04  lr: 0.000055  loss: 1.7002 (1.8719)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [330/781]  eta: 0:01:03  lr: 0.000055  loss: 1.7545 (1.8712)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [340/781]  eta: 0:01:02  lr: 0.000055  loss: 1.8080 (1.8704)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [350/781]  eta: 0:01:00  lr: 0.000055  loss: 1.7340 (1.8695)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [360/781]  eta: 0:00:59  lr: 0.000055  loss: 1.7171 (1.8710)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [370/781]  eta: 0:00:57  lr: 0.000055  loss: 1.6998 (1.8683)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [380/781]  eta: 0:00:56  lr: 0.000055  loss: 1.6888 (1.8654)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [390/781]  eta: 0:00:54  lr: 0.000055  loss: 1.7201 (1.8662)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [400/781]  eta: 0:00:53  lr: 0.000055  loss: 1.7301 (1.8670)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [410/781]  eta: 0:00:52  lr: 0.000055  loss: 1.7301 (1.8651)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [420/781]  eta: 0:00:50  lr: 0.000055  loss: 1.7192 (1.8681)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [430/781]  eta: 0:00:49  lr: 0.000055  loss: 1.7501 (1.8712)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [440/781]  eta: 0:00:47  lr: 0.000055  loss: 1.7439 (1.8729)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [450/781]  eta: 0:00:46  lr: 0.000055  loss: 1.7191 (1.8708)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [460/781]  eta: 0:00:44  lr: 0.000055  loss: 1.7165 (1.8691)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [470/781]  eta: 0:00:43  lr: 0.000055  loss: 1.7151 (1.8716)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [480/781]  eta: 0:00:42  lr: 0.000055  loss: 1.7324 (1.8736)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [490/781]  eta: 0:00:40  lr: 0.000055  loss: 1.7821 (1.8751)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [500/781]  eta: 0:00:39  lr: 0.000055  loss: 1.7588 (1.8726)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [510/781]  eta: 0:00:37  lr: 0.000055  loss: 1.7472 (1.8701)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [520/781]  eta: 0:00:36  lr: 0.000055  loss: 1.7541 (1.8743)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [530/781]  eta: 0:00:35  lr: 0.000055  loss: 1.7406 (1.8736)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [540/781]  eta: 0:00:33  lr: 0.000055  loss: 1.7273 (1.8724)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [550/781]  eta: 0:00:32  lr: 0.000055  loss: 1.7415 (1.8697)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [560/781]  eta: 0:00:30  lr: 0.000055  loss: 1.7391 (1.8678)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [570/781]  eta: 0:00:29  lr: 0.000055  loss: 1.7489 (1.8680)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [580/781]  eta: 0:00:28  lr: 0.000055  loss: 1.7758 (1.8691)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [590/781]  eta: 0:00:26  lr: 0.000055  loss: 1.8068 (1.8750)  time: 0.1422  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [600/781]  eta: 0:00:25  lr: 0.000055  loss: 1.7710 (1.8736)  time: 0.1425  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [610/781]  eta: 0:00:23  lr: 0.000055  loss: 1.7509 (1.8773)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [620/781]  eta: 0:00:22  lr: 0.000055  loss: 1.7693 (1.8750)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [630/781]  eta: 0:00:21  lr: 0.000055  loss: 1.7257 (1.8756)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [640/781]  eta: 0:00:19  lr: 0.000055  loss: 1.7496 (1.8734)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [650/781]  eta: 0:00:18  lr: 0.000055  loss: 1.7596 (1.8759)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [660/781]  eta: 0:00:16  lr: 0.000055  loss: 1.7184 (1.8731)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [670/781]  eta: 0:00:15  lr: 0.000055  loss: 1.6698 (1.8727)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [680/781]  eta: 0:00:14  lr: 0.000055  loss: 1.6815 (1.8717)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [690/781]  eta: 0:00:12  lr: 0.000055  loss: 1.7315 (1.8735)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [700/781]  eta: 0:00:11  lr: 0.000055  loss: 1.7396 (1.8733)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [710/781]  eta: 0:00:09  lr: 0.000055  loss: 1.7396 (1.8729)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [720/781]  eta: 0:00:08  lr: 0.000055  loss: 1.7570 (1.8721)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [730/781]  eta: 0:00:07  lr: 0.000055  loss: 1.7570 (1.8717)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [740/781]  eta: 0:00:05  lr: 0.000055  loss: 1.7889 (1.8716)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [750/781]  eta: 0:00:04  lr: 0.000055  loss: 1.7349 (1.8715)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [760/781]  eta: 0:00:02  lr: 0.000055  loss: 1.6988 (1.8703)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [770/781]  eta: 0:00:01  lr: 0.000055  loss: 1.7234 (1.8684)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [6]  [780/781]  eta: 0:00:00  lr: 0.000055  loss: 1.7407 (1.8676)  time: 0.1400  data: 0.0006  max mem: 4938\n",
            "Epoch: [6] Total time: 0:01:49 (0.1397 s / it)\n",
            "Averaged stats: lr: 0.000055  loss: 1.7407 (1.8676)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8795 (0.8795)  acc1: 81.2500 (81.2500)  acc5: 96.3542 (96.3542)  time: 0.8261  data: 0.7951  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1759 (1.0726)  acc1: 76.0417 (76.5152)  acc5: 93.2292 (93.2292)  time: 0.1670  data: 0.1363  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.2207 (1.1481)  acc1: 72.3958 (74.6528)  acc5: 91.1458 (91.7163)  time: 0.1102  data: 0.0796  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2849 (1.2058)  acc1: 70.3125 (73.5215)  acc5: 89.5833 (90.8266)  time: 0.1343  data: 0.1037  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2963 (1.2543)  acc1: 70.8333 (72.3831)  acc5: 89.0625 (90.2439)  time: 0.1304  data: 0.0997  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2646 (1.2485)  acc1: 71.8750 (72.2018)  acc5: 90.6250 (90.3186)  time: 0.1317  data: 0.1010  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2960 (1.2573)  acc1: 68.7500 (72.1000)  acc5: 91.1458 (90.4000)  time: 0.1307  data: 0.1010  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1367 s / it)\n",
            "* Acc@1 72.100 Acc@5 90.400 loss 1.257\n",
            "Accuracy of the network on the 10000 test images: 72.1%\n",
            "Max accuracy: 72.10%\n",
            "Epoch: [7]  [  0/781]  eta: 0:11:31  lr: 0.000052  loss: 1.6047 (1.6047)  time: 0.8855  data: 0.7039  max mem: 4938\n",
            "Epoch: [7]  [ 10/781]  eta: 0:02:40  lr: 0.000052  loss: 1.6742 (1.7756)  time: 0.2081  data: 0.0660  max mem: 4938\n",
            "Epoch: [7]  [ 20/781]  eta: 0:02:12  lr: 0.000052  loss: 1.7094 (1.8309)  time: 0.1390  data: 0.0013  max mem: 4938\n",
            "Epoch: [7]  [ 30/781]  eta: 0:02:05  lr: 0.000052  loss: 1.7219 (1.8467)  time: 0.1453  data: 0.0071  max mem: 4938\n",
            "Epoch: [7]  [ 40/781]  eta: 0:01:58  lr: 0.000052  loss: 1.7293 (1.8391)  time: 0.1458  data: 0.0071  max mem: 4938\n",
            "Epoch: [7]  [ 50/781]  eta: 0:01:54  lr: 0.000052  loss: 1.7512 (1.8542)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [ 60/781]  eta: 0:01:50  lr: 0.000052  loss: 1.7793 (1.8666)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [ 70/781]  eta: 0:01:47  lr: 0.000052  loss: 1.7240 (1.8578)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [ 80/781]  eta: 0:01:45  lr: 0.000052  loss: 1.7135 (1.8738)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [ 90/781]  eta: 0:01:42  lr: 0.000052  loss: 1.7612 (1.8883)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [100/781]  eta: 0:01:40  lr: 0.000052  loss: 1.7625 (1.8896)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [110/781]  eta: 0:01:38  lr: 0.000052  loss: 1.7382 (1.8896)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [120/781]  eta: 0:01:36  lr: 0.000052  loss: 1.7060 (1.8852)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [130/781]  eta: 0:01:34  lr: 0.000052  loss: 1.6835 (1.8733)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [140/781]  eta: 0:01:33  lr: 0.000052  loss: 1.6972 (1.8677)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [150/781]  eta: 0:01:31  lr: 0.000052  loss: 1.7528 (1.8657)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [160/781]  eta: 0:01:29  lr: 0.000052  loss: 1.7264 (1.8734)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [170/781]  eta: 0:01:27  lr: 0.000052  loss: 1.7500 (1.8803)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [180/781]  eta: 0:01:26  lr: 0.000052  loss: 1.7529 (1.8812)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [190/781]  eta: 0:01:24  lr: 0.000052  loss: 1.7048 (1.8748)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [200/781]  eta: 0:01:23  lr: 0.000052  loss: 1.7029 (1.8713)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [210/781]  eta: 0:01:21  lr: 0.000052  loss: 1.6949 (1.8616)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [220/781]  eta: 0:01:19  lr: 0.000052  loss: 1.6886 (1.8545)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [230/781]  eta: 0:01:18  lr: 0.000052  loss: 1.7062 (1.8519)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [240/781]  eta: 0:01:17  lr: 0.000052  loss: 1.7145 (1.8468)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [250/781]  eta: 0:01:15  lr: 0.000052  loss: 1.6699 (1.8400)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [260/781]  eta: 0:01:14  lr: 0.000052  loss: 1.6617 (1.8348)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [270/781]  eta: 0:01:12  lr: 0.000052  loss: 1.7012 (1.8355)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [280/781]  eta: 0:01:11  lr: 0.000052  loss: 1.7064 (1.8397)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [290/781]  eta: 0:01:09  lr: 0.000052  loss: 1.6885 (1.8360)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [300/781]  eta: 0:01:08  lr: 0.000052  loss: 1.6522 (1.8338)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [310/781]  eta: 0:01:06  lr: 0.000052  loss: 1.6801 (1.8313)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [320/781]  eta: 0:01:05  lr: 0.000052  loss: 1.6868 (1.8276)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [330/781]  eta: 0:01:03  lr: 0.000052  loss: 1.7327 (1.8289)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [340/781]  eta: 0:01:02  lr: 0.000052  loss: 1.7426 (1.8277)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [350/781]  eta: 0:01:00  lr: 0.000052  loss: 1.7166 (1.8241)  time: 0.1420  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [360/781]  eta: 0:00:59  lr: 0.000052  loss: 1.7235 (1.8221)  time: 0.1428  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [370/781]  eta: 0:00:58  lr: 0.000052  loss: 1.7527 (1.8199)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [380/781]  eta: 0:00:56  lr: 0.000052  loss: 1.6713 (1.8203)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [390/781]  eta: 0:00:55  lr: 0.000052  loss: 1.7063 (1.8178)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [400/781]  eta: 0:00:53  lr: 0.000052  loss: 1.7398 (1.8206)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [410/781]  eta: 0:00:52  lr: 0.000052  loss: 1.7398 (1.8217)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [420/781]  eta: 0:00:50  lr: 0.000052  loss: 1.7262 (1.8217)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [430/781]  eta: 0:00:49  lr: 0.000052  loss: 1.7194 (1.8216)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [440/781]  eta: 0:00:48  lr: 0.000052  loss: 1.7262 (1.8256)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [450/781]  eta: 0:00:46  lr: 0.000052  loss: 1.7052 (1.8270)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [460/781]  eta: 0:00:45  lr: 0.000052  loss: 1.7557 (1.8293)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [470/781]  eta: 0:00:43  lr: 0.000052  loss: 1.8474 (1.8311)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [480/781]  eta: 0:00:42  lr: 0.000052  loss: 1.6700 (1.8273)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [490/781]  eta: 0:00:41  lr: 0.000052  loss: 1.6686 (1.8290)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [500/781]  eta: 0:00:39  lr: 0.000052  loss: 1.8102 (1.8302)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [510/781]  eta: 0:00:38  lr: 0.000052  loss: 1.7697 (1.8309)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [520/781]  eta: 0:00:36  lr: 0.000052  loss: 1.7036 (1.8280)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [530/781]  eta: 0:00:35  lr: 0.000052  loss: 1.6810 (1.8306)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [540/781]  eta: 0:00:33  lr: 0.000052  loss: 1.7064 (1.8310)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [550/781]  eta: 0:00:32  lr: 0.000052  loss: 1.7087 (1.8286)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [560/781]  eta: 0:00:31  lr: 0.000052  loss: 1.6901 (1.8295)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [570/781]  eta: 0:00:29  lr: 0.000052  loss: 1.6710 (1.8290)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [580/781]  eta: 0:00:28  lr: 0.000052  loss: 1.7336 (1.8310)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [590/781]  eta: 0:00:26  lr: 0.000052  loss: 1.7336 (1.8319)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [600/781]  eta: 0:00:25  lr: 0.000052  loss: 1.7129 (1.8302)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [610/781]  eta: 0:00:24  lr: 0.000052  loss: 1.7235 (1.8318)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [620/781]  eta: 0:00:22  lr: 0.000052  loss: 1.7321 (1.8306)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [630/781]  eta: 0:00:21  lr: 0.000052  loss: 1.7086 (1.8307)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [640/781]  eta: 0:00:19  lr: 0.000052  loss: 1.7086 (1.8303)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [650/781]  eta: 0:00:18  lr: 0.000052  loss: 1.7103 (1.8285)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [660/781]  eta: 0:00:17  lr: 0.000052  loss: 1.7432 (1.8279)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [670/781]  eta: 0:00:15  lr: 0.000052  loss: 1.7432 (1.8277)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [680/781]  eta: 0:00:14  lr: 0.000052  loss: 1.6635 (1.8268)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [690/781]  eta: 0:00:12  lr: 0.000052  loss: 1.6929 (1.8254)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [700/781]  eta: 0:00:11  lr: 0.000052  loss: 1.7473 (1.8237)  time: 0.1490  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [710/781]  eta: 0:00:09  lr: 0.000052  loss: 1.7599 (1.8260)  time: 0.1496  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [720/781]  eta: 0:00:08  lr: 0.000052  loss: 1.7769 (1.8260)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [730/781]  eta: 0:00:07  lr: 0.000052  loss: 1.7375 (1.8251)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [740/781]  eta: 0:00:05  lr: 0.000052  loss: 1.7300 (1.8238)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [750/781]  eta: 0:00:04  lr: 0.000052  loss: 1.7263 (1.8236)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [760/781]  eta: 0:00:02  lr: 0.000052  loss: 1.6897 (1.8231)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [770/781]  eta: 0:00:01  lr: 0.000052  loss: 1.6897 (1.8225)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [7]  [780/781]  eta: 0:00:00  lr: 0.000052  loss: 1.7237 (1.8230)  time: 0.1383  data: 0.0006  max mem: 4938\n",
            "Epoch: [7] Total time: 0:01:49 (0.1407 s / it)\n",
            "Averaged stats: lr: 0.000052  loss: 1.7237 (1.8230)\n",
            "Test:  [ 0/53]  eta: 0:00:42  loss: 0.8825 (0.8825)  acc1: 80.7292 (80.7292)  acc5: 95.3125 (95.3125)  time: 0.7993  data: 0.7683  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1273 (1.0779)  acc1: 77.6042 (76.4678)  acc5: 92.1875 (92.6610)  time: 0.1711  data: 0.1404  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.2004 (1.1478)  acc1: 70.8333 (74.6280)  acc5: 91.1458 (91.5179)  time: 0.1264  data: 0.0957  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2716 (1.1707)  acc1: 71.8750 (74.3112)  acc5: 91.1458 (91.2970)  time: 0.1262  data: 0.0954  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2716 (1.2165)  acc1: 72.3958 (73.1326)  acc5: 89.5833 (90.7901)  time: 0.1161  data: 0.0854  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2426 (1.2168)  acc1: 70.3125 (72.9371)  acc5: 91.1458 (90.8803)  time: 0.1246  data: 0.0939  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2779 (1.2257)  acc1: 69.2708 (72.8100)  acc5: 91.1458 (90.9100)  time: 0.1109  data: 0.0811  max mem: 4938\n",
            "Test: Total time: 0:00:06 (0.1319 s / it)\n",
            "* Acc@1 72.810 Acc@5 90.910 loss 1.226\n",
            "Accuracy of the network on the 10000 test images: 72.8%\n",
            "Max accuracy: 72.81%\n",
            "Epoch: [8]  [  0/781]  eta: 0:11:10  lr: 0.000048  loss: 1.6459 (1.6459)  time: 0.8585  data: 0.6959  max mem: 4938\n",
            "Epoch: [8]  [ 10/781]  eta: 0:02:37  lr: 0.000048  loss: 1.7026 (1.8718)  time: 0.2043  data: 0.0636  max mem: 4938\n",
            "Epoch: [8]  [ 20/781]  eta: 0:02:11  lr: 0.000048  loss: 1.6943 (1.8344)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 30/781]  eta: 0:02:01  lr: 0.000048  loss: 1.6376 (1.7922)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 40/781]  eta: 0:01:55  lr: 0.000048  loss: 1.6692 (1.8004)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 50/781]  eta: 0:01:51  lr: 0.000048  loss: 1.6846 (1.7949)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 60/781]  eta: 0:01:48  lr: 0.000048  loss: 1.6691 (1.8222)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 70/781]  eta: 0:01:46  lr: 0.000048  loss: 1.6691 (1.8348)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 80/781]  eta: 0:01:43  lr: 0.000048  loss: 1.7249 (1.8568)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [ 90/781]  eta: 0:01:41  lr: 0.000048  loss: 1.7249 (1.8522)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [100/781]  eta: 0:01:39  lr: 0.000048  loss: 1.6365 (1.8375)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [110/781]  eta: 0:01:37  lr: 0.000048  loss: 1.6754 (1.8343)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [120/781]  eta: 0:01:35  lr: 0.000048  loss: 1.6700 (1.8267)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [130/781]  eta: 0:01:34  lr: 0.000048  loss: 1.6466 (1.8143)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [140/781]  eta: 0:01:32  lr: 0.000048  loss: 1.6579 (1.8127)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [150/781]  eta: 0:01:30  lr: 0.000048  loss: 1.6668 (1.8144)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [160/781]  eta: 0:01:29  lr: 0.000048  loss: 1.6668 (1.8061)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [170/781]  eta: 0:01:27  lr: 0.000048  loss: 1.6880 (1.8074)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [180/781]  eta: 0:01:25  lr: 0.000048  loss: 1.7065 (1.8035)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [190/781]  eta: 0:01:24  lr: 0.000048  loss: 1.6977 (1.8021)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [200/781]  eta: 0:01:22  lr: 0.000048  loss: 1.6647 (1.8019)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [210/781]  eta: 0:01:21  lr: 0.000048  loss: 1.6716 (1.7976)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [220/781]  eta: 0:01:19  lr: 0.000048  loss: 1.6242 (1.7888)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [230/781]  eta: 0:01:18  lr: 0.000048  loss: 1.6242 (1.7873)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [240/781]  eta: 0:01:16  lr: 0.000048  loss: 1.6095 (1.7846)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [250/781]  eta: 0:01:15  lr: 0.000048  loss: 1.6095 (1.7866)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [260/781]  eta: 0:01:13  lr: 0.000048  loss: 1.6900 (1.7913)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [270/781]  eta: 0:01:12  lr: 0.000048  loss: 1.6797 (1.7919)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [280/781]  eta: 0:01:11  lr: 0.000048  loss: 1.6797 (1.8010)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [290/781]  eta: 0:01:09  lr: 0.000048  loss: 1.6366 (1.7943)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [300/781]  eta: 0:01:08  lr: 0.000048  loss: 1.6366 (1.7953)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [310/781]  eta: 0:01:06  lr: 0.000048  loss: 1.7293 (1.8002)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [320/781]  eta: 0:01:05  lr: 0.000048  loss: 1.7026 (1.7981)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [330/781]  eta: 0:01:03  lr: 0.000048  loss: 1.6989 (1.7971)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [340/781]  eta: 0:01:02  lr: 0.000048  loss: 1.6965 (1.7981)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [350/781]  eta: 0:01:00  lr: 0.000048  loss: 1.6833 (1.7946)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [360/781]  eta: 0:00:59  lr: 0.000048  loss: 1.6517 (1.7941)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [370/781]  eta: 0:00:57  lr: 0.000048  loss: 1.6657 (1.7913)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [380/781]  eta: 0:00:56  lr: 0.000048  loss: 1.6566 (1.7915)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [390/781]  eta: 0:00:55  lr: 0.000048  loss: 1.6847 (1.8023)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [400/781]  eta: 0:00:53  lr: 0.000048  loss: 1.6647 (1.7986)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [410/781]  eta: 0:00:52  lr: 0.000048  loss: 1.6992 (1.8023)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [420/781]  eta: 0:00:50  lr: 0.000048  loss: 1.7299 (1.8012)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [430/781]  eta: 0:00:49  lr: 0.000048  loss: 1.6547 (1.7975)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [440/781]  eta: 0:00:47  lr: 0.000048  loss: 1.6960 (1.7962)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [450/781]  eta: 0:00:46  lr: 0.000048  loss: 1.6900 (1.7958)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [460/781]  eta: 0:00:45  lr: 0.000048  loss: 1.6754 (1.7928)  time: 0.1371  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [470/781]  eta: 0:00:43  lr: 0.000048  loss: 1.6808 (1.7966)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [480/781]  eta: 0:00:42  lr: 0.000048  loss: 1.8676 (1.8030)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [490/781]  eta: 0:00:40  lr: 0.000048  loss: 1.7558 (1.8026)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [500/781]  eta: 0:00:39  lr: 0.000048  loss: 1.6921 (1.8028)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [510/781]  eta: 0:00:38  lr: 0.000048  loss: 1.7026 (1.8050)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [520/781]  eta: 0:00:36  lr: 0.000048  loss: 1.7710 (1.8056)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [530/781]  eta: 0:00:35  lr: 0.000048  loss: 1.6943 (1.8051)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [540/781]  eta: 0:00:33  lr: 0.000048  loss: 1.6357 (1.8029)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [550/781]  eta: 0:00:32  lr: 0.000048  loss: 1.6244 (1.8005)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [560/781]  eta: 0:00:30  lr: 0.000048  loss: 1.7154 (1.8050)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [570/781]  eta: 0:00:29  lr: 0.000048  loss: 1.7629 (1.8055)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [580/781]  eta: 0:00:28  lr: 0.000048  loss: 1.6447 (1.8032)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [590/781]  eta: 0:00:26  lr: 0.000048  loss: 1.7223 (1.8086)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [600/781]  eta: 0:00:25  lr: 0.000048  loss: 1.7451 (1.8068)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [610/781]  eta: 0:00:23  lr: 0.000048  loss: 1.6486 (1.8047)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [620/781]  eta: 0:00:22  lr: 0.000048  loss: 1.6742 (1.8078)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [630/781]  eta: 0:00:21  lr: 0.000048  loss: 1.6840 (1.8057)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [640/781]  eta: 0:00:19  lr: 0.000048  loss: 1.6831 (1.8064)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [650/781]  eta: 0:00:18  lr: 0.000048  loss: 1.6989 (1.8053)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [660/781]  eta: 0:00:16  lr: 0.000048  loss: 1.6834 (1.8049)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [670/781]  eta: 0:00:15  lr: 0.000048  loss: 1.6653 (1.8047)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [680/781]  eta: 0:00:14  lr: 0.000048  loss: 1.6653 (1.8070)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [690/781]  eta: 0:00:12  lr: 0.000048  loss: 1.6689 (1.8055)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [700/781]  eta: 0:00:11  lr: 0.000048  loss: 1.6611 (1.8058)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [710/781]  eta: 0:00:09  lr: 0.000048  loss: 1.6514 (1.8048)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [720/781]  eta: 0:00:08  lr: 0.000048  loss: 1.6568 (1.8035)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [730/781]  eta: 0:00:07  lr: 0.000048  loss: 1.6647 (1.8039)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [740/781]  eta: 0:00:05  lr: 0.000048  loss: 1.7254 (1.8046)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [750/781]  eta: 0:00:04  lr: 0.000048  loss: 1.8054 (1.8076)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [760/781]  eta: 0:00:02  lr: 0.000048  loss: 1.7337 (1.8064)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [770/781]  eta: 0:00:01  lr: 0.000048  loss: 1.6535 (1.8079)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [8]  [780/781]  eta: 0:00:00  lr: 0.000048  loss: 1.6551 (1.8060)  time: 0.1391  data: 0.0006  max mem: 4938\n",
            "Epoch: [8] Total time: 0:01:49 (0.1400 s / it)\n",
            "Averaged stats: lr: 0.000048  loss: 1.6551 (1.8060)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8700 (0.8700)  acc1: 80.7292 (80.7292)  acc5: 95.8333 (95.8333)  time: 0.8293  data: 0.7984  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1601 (1.0073)  acc1: 77.0833 (76.8466)  acc5: 93.7500 (93.7500)  time: 0.1670  data: 0.1363  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1601 (1.0752)  acc1: 72.9167 (76.1161)  acc5: 91.6667 (92.3115)  time: 0.1226  data: 0.0920  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2295 (1.1315)  acc1: 72.9167 (74.7480)  acc5: 91.1458 (91.7339)  time: 0.1251  data: 0.0944  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2926 (1.1872)  acc1: 72.3958 (73.3359)  acc5: 90.1042 (91.1585)  time: 0.1247  data: 0.0939  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1755 (1.1755)  acc1: 72.9167 (73.4477)  acc5: 91.1458 (91.3399)  time: 0.1274  data: 0.0967  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2165 (1.1843)  acc1: 72.3958 (73.3400)  acc5: 91.1458 (91.3600)  time: 0.1079  data: 0.0782  max mem: 4938\n",
            "Test: Total time: 0:00:06 (0.1316 s / it)\n",
            "* Acc@1 73.340 Acc@5 91.360 loss 1.184\n",
            "Accuracy of the network on the 10000 test images: 73.3%\n",
            "Max accuracy: 73.34%\n",
            "Epoch: [9]  [  0/781]  eta: 0:12:02  lr: 0.000044  loss: 1.7180 (1.7180)  time: 0.9251  data: 0.7593  max mem: 4938\n",
            "Epoch: [9]  [ 10/781]  eta: 0:02:46  lr: 0.000044  loss: 1.5855 (1.5972)  time: 0.2162  data: 0.0731  max mem: 4938\n",
            "Epoch: [9]  [ 20/781]  eta: 0:02:16  lr: 0.000044  loss: 1.5863 (1.6668)  time: 0.1421  data: 0.0024  max mem: 4938\n",
            "Epoch: [9]  [ 30/781]  eta: 0:02:04  lr: 0.000044  loss: 1.6033 (1.6966)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 40/781]  eta: 0:01:57  lr: 0.000044  loss: 1.6034 (1.7373)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 50/781]  eta: 0:01:53  lr: 0.000044  loss: 1.6406 (1.7587)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 60/781]  eta: 0:01:49  lr: 0.000044  loss: 1.6406 (1.7744)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 70/781]  eta: 0:01:46  lr: 0.000044  loss: 1.7321 (1.8263)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 80/781]  eta: 0:01:44  lr: 0.000044  loss: 1.7591 (1.8406)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [ 90/781]  eta: 0:01:42  lr: 0.000044  loss: 1.6381 (1.8583)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [100/781]  eta: 0:01:40  lr: 0.000044  loss: 1.6903 (1.8404)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [110/781]  eta: 0:01:38  lr: 0.000044  loss: 1.6906 (1.8487)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [120/781]  eta: 0:01:36  lr: 0.000044  loss: 1.6705 (1.8463)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [130/781]  eta: 0:01:34  lr: 0.000044  loss: 1.6476 (1.8403)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [140/781]  eta: 0:01:32  lr: 0.000044  loss: 1.7064 (1.8556)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [150/781]  eta: 0:01:31  lr: 0.000044  loss: 1.7197 (1.8524)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [160/781]  eta: 0:01:29  lr: 0.000044  loss: 1.6676 (1.8436)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [170/781]  eta: 0:01:27  lr: 0.000044  loss: 1.6338 (1.8451)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [180/781]  eta: 0:01:26  lr: 0.000044  loss: 1.6180 (1.8385)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [190/781]  eta: 0:01:24  lr: 0.000044  loss: 1.6731 (1.8418)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [200/781]  eta: 0:01:23  lr: 0.000044  loss: 1.6822 (1.8336)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [210/781]  eta: 0:01:21  lr: 0.000044  loss: 1.6871 (1.8326)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [220/781]  eta: 0:01:19  lr: 0.000044  loss: 1.6729 (1.8307)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [230/781]  eta: 0:01:18  lr: 0.000044  loss: 1.6729 (1.8262)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [240/781]  eta: 0:01:16  lr: 0.000044  loss: 1.6808 (1.8196)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [250/781]  eta: 0:01:15  lr: 0.000044  loss: 1.6494 (1.8162)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [260/781]  eta: 0:01:13  lr: 0.000044  loss: 1.6644 (1.8191)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [270/781]  eta: 0:01:12  lr: 0.000044  loss: 1.6646 (1.8173)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [280/781]  eta: 0:01:10  lr: 0.000044  loss: 1.6563 (1.8126)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [290/781]  eta: 0:01:09  lr: 0.000044  loss: 1.6110 (1.8057)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [300/781]  eta: 0:01:08  lr: 0.000044  loss: 1.6351 (1.8064)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [310/781]  eta: 0:01:06  lr: 0.000044  loss: 1.6715 (1.8034)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [320/781]  eta: 0:01:05  lr: 0.000044  loss: 1.6772 (1.8007)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [330/781]  eta: 0:01:03  lr: 0.000044  loss: 1.6787 (1.7972)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [340/781]  eta: 0:01:02  lr: 0.000044  loss: 1.6347 (1.7956)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [350/781]  eta: 0:01:00  lr: 0.000044  loss: 1.6070 (1.7919)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [360/781]  eta: 0:00:59  lr: 0.000044  loss: 1.6520 (1.7946)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [370/781]  eta: 0:00:57  lr: 0.000044  loss: 1.7174 (1.7983)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [380/781]  eta: 0:00:56  lr: 0.000044  loss: 1.7043 (1.7981)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [390/781]  eta: 0:00:55  lr: 0.000044  loss: 1.6930 (1.7981)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [400/781]  eta: 0:00:53  lr: 0.000044  loss: 1.7166 (1.7964)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [410/781]  eta: 0:00:52  lr: 0.000044  loss: 1.6612 (1.7946)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [420/781]  eta: 0:00:50  lr: 0.000044  loss: 1.6754 (1.7999)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [430/781]  eta: 0:00:49  lr: 0.000044  loss: 1.7580 (1.7982)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [440/781]  eta: 0:00:47  lr: 0.000044  loss: 1.6410 (1.7961)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [450/781]  eta: 0:00:46  lr: 0.000044  loss: 1.6211 (1.7932)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [460/781]  eta: 0:00:45  lr: 0.000044  loss: 1.5995 (1.7919)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [470/781]  eta: 0:00:43  lr: 0.000044  loss: 1.6805 (1.7923)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [480/781]  eta: 0:00:42  lr: 0.000044  loss: 1.7164 (1.7917)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [490/781]  eta: 0:00:40  lr: 0.000044  loss: 1.6700 (1.7908)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [500/781]  eta: 0:00:39  lr: 0.000044  loss: 1.6412 (1.7922)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [510/781]  eta: 0:00:38  lr: 0.000044  loss: 1.6496 (1.7939)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [520/781]  eta: 0:00:36  lr: 0.000044  loss: 1.6496 (1.7949)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [530/781]  eta: 0:00:35  lr: 0.000044  loss: 1.6806 (1.7935)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [540/781]  eta: 0:00:33  lr: 0.000044  loss: 1.6869 (1.7927)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [550/781]  eta: 0:00:32  lr: 0.000044  loss: 1.7059 (1.7931)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [560/781]  eta: 0:00:30  lr: 0.000044  loss: 1.7711 (1.7978)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [570/781]  eta: 0:00:29  lr: 0.000044  loss: 1.7852 (1.8006)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [580/781]  eta: 0:00:28  lr: 0.000044  loss: 1.6586 (1.8016)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [590/781]  eta: 0:00:26  lr: 0.000044  loss: 1.6586 (1.8012)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [600/781]  eta: 0:00:25  lr: 0.000044  loss: 1.6811 (1.7994)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [610/781]  eta: 0:00:23  lr: 0.000044  loss: 1.6528 (1.7966)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [620/781]  eta: 0:00:22  lr: 0.000044  loss: 1.6829 (1.7963)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [630/781]  eta: 0:00:21  lr: 0.000044  loss: 1.7065 (1.7962)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [640/781]  eta: 0:00:19  lr: 0.000044  loss: 1.6336 (1.7942)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [650/781]  eta: 0:00:18  lr: 0.000044  loss: 1.6336 (1.7944)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [660/781]  eta: 0:00:16  lr: 0.000044  loss: 1.7097 (1.7939)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [670/781]  eta: 0:00:15  lr: 0.000044  loss: 1.7041 (1.7937)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [680/781]  eta: 0:00:14  lr: 0.000044  loss: 1.7041 (1.7940)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [690/781]  eta: 0:00:12  lr: 0.000044  loss: 1.6538 (1.7919)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [700/781]  eta: 0:00:11  lr: 0.000044  loss: 1.6538 (1.7942)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [710/781]  eta: 0:00:09  lr: 0.000044  loss: 1.6782 (1.7944)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [720/781]  eta: 0:00:08  lr: 0.000044  loss: 1.6843 (1.7958)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [730/781]  eta: 0:00:07  lr: 0.000044  loss: 1.6310 (1.7944)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [740/781]  eta: 0:00:05  lr: 0.000044  loss: 1.6501 (1.7929)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [750/781]  eta: 0:00:04  lr: 0.000044  loss: 1.7081 (1.7933)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [760/781]  eta: 0:00:02  lr: 0.000044  loss: 1.7296 (1.7923)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [770/781]  eta: 0:00:01  lr: 0.000044  loss: 1.6500 (1.7916)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [9]  [780/781]  eta: 0:00:00  lr: 0.000044  loss: 1.6490 (1.7916)  time: 0.1386  data: 0.0005  max mem: 4938\n",
            "Epoch: [9] Total time: 0:01:49 (0.1399 s / it)\n",
            "Averaged stats: lr: 0.000044  loss: 1.6490 (1.7916)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.6881 (0.6881)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 0.8229  data: 0.7919  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 1.1154 (1.0358)  acc1: 76.0417 (75.6629)  acc5: 94.2708 (93.3239)  time: 0.1571  data: 0.1257  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.1333 (1.1060)  acc1: 72.3958 (74.8760)  acc5: 92.1875 (92.1627)  time: 0.1092  data: 0.0780  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2306 (1.1421)  acc1: 71.3542 (74.3448)  acc5: 91.6667 (91.8851)  time: 0.1256  data: 0.0946  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2741 (1.1859)  acc1: 71.3542 (73.4502)  acc5: 90.6250 (91.2348)  time: 0.1171  data: 0.0851  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1772 (1.1794)  acc1: 72.9167 (73.6418)  acc5: 91.1458 (91.3297)  time: 0.1239  data: 0.0920  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2673 (1.1875)  acc1: 72.9167 (73.5400)  acc5: 91.1458 (91.3500)  time: 0.1193  data: 0.0888  max mem: 4938\n",
            "Test: Total time: 0:00:06 (0.1286 s / it)\n",
            "* Acc@1 73.540 Acc@5 91.350 loss 1.188\n",
            "Accuracy of the network on the 10000 test images: 73.5%\n",
            "Max accuracy: 73.54%\n",
            "Epoch: [10]  [  0/781]  eta: 0:11:34  lr: 0.000040  loss: 1.6197 (1.6197)  time: 0.8897  data: 0.7066  max mem: 4938\n",
            "Epoch: [10]  [ 10/781]  eta: 0:02:39  lr: 0.000040  loss: 1.6197 (1.7597)  time: 0.2073  data: 0.0645  max mem: 4938\n",
            "Epoch: [10]  [ 20/781]  eta: 0:02:12  lr: 0.000040  loss: 1.6001 (1.7825)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 30/781]  eta: 0:02:02  lr: 0.000040  loss: 1.6214 (1.7622)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 40/781]  eta: 0:01:56  lr: 0.000040  loss: 1.6218 (1.7522)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 50/781]  eta: 0:01:52  lr: 0.000040  loss: 1.6108 (1.7525)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 60/781]  eta: 0:01:49  lr: 0.000040  loss: 1.6180 (1.7402)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 70/781]  eta: 0:01:46  lr: 0.000040  loss: 1.6372 (1.7434)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 80/781]  eta: 0:01:44  lr: 0.000040  loss: 1.6184 (1.7284)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [ 90/781]  eta: 0:01:42  lr: 0.000040  loss: 1.6297 (1.7543)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [100/781]  eta: 0:01:40  lr: 0.000040  loss: 1.6314 (1.7468)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [110/781]  eta: 0:01:38  lr: 0.000040  loss: 1.6768 (1.7862)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [120/781]  eta: 0:01:36  lr: 0.000040  loss: 1.6797 (1.7720)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [130/781]  eta: 0:01:34  lr: 0.000040  loss: 1.6137 (1.7692)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [140/781]  eta: 0:01:32  lr: 0.000040  loss: 1.6508 (1.7639)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [150/781]  eta: 0:01:30  lr: 0.000040  loss: 1.6471 (1.7601)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [160/781]  eta: 0:01:29  lr: 0.000040  loss: 1.5993 (1.7532)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [170/781]  eta: 0:01:27  lr: 0.000040  loss: 1.5855 (1.7485)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [180/781]  eta: 0:01:26  lr: 0.000040  loss: 1.5473 (1.7401)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [190/781]  eta: 0:01:25  lr: 0.000040  loss: 1.5670 (1.7331)  time: 0.1476  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [200/781]  eta: 0:01:23  lr: 0.000040  loss: 1.6019 (1.7327)  time: 0.1470  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [210/781]  eta: 0:01:21  lr: 0.000040  loss: 1.6269 (1.7319)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [220/781]  eta: 0:01:20  lr: 0.000040  loss: 1.6269 (1.7392)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [230/781]  eta: 0:01:18  lr: 0.000040  loss: 1.6209 (1.7352)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [240/781]  eta: 0:01:17  lr: 0.000040  loss: 1.5694 (1.7369)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [250/781]  eta: 0:01:15  lr: 0.000040  loss: 1.6209 (1.7328)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [260/781]  eta: 0:01:14  lr: 0.000040  loss: 1.6622 (1.7359)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [270/781]  eta: 0:01:12  lr: 0.000040  loss: 1.6622 (1.7344)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [280/781]  eta: 0:01:11  lr: 0.000040  loss: 1.6273 (1.7434)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [290/781]  eta: 0:01:09  lr: 0.000040  loss: 1.6774 (1.7478)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [300/781]  eta: 0:01:08  lr: 0.000040  loss: 1.6418 (1.7432)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [310/781]  eta: 0:01:06  lr: 0.000040  loss: 1.6219 (1.7391)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [320/781]  eta: 0:01:05  lr: 0.000040  loss: 1.6424 (1.7367)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [330/781]  eta: 0:01:03  lr: 0.000040  loss: 1.6426 (1.7366)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [340/781]  eta: 0:01:02  lr: 0.000040  loss: 1.6245 (1.7337)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [350/781]  eta: 0:01:01  lr: 0.000040  loss: 1.5874 (1.7307)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [360/781]  eta: 0:00:59  lr: 0.000040  loss: 1.5975 (1.7312)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [370/781]  eta: 0:00:58  lr: 0.000040  loss: 1.6560 (1.7322)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [380/781]  eta: 0:00:56  lr: 0.000040  loss: 1.6089 (1.7324)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [390/781]  eta: 0:00:55  lr: 0.000040  loss: 1.6557 (1.7359)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [400/781]  eta: 0:00:53  lr: 0.000040  loss: 1.6615 (1.7334)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [410/781]  eta: 0:00:52  lr: 0.000040  loss: 1.6531 (1.7354)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [420/781]  eta: 0:00:50  lr: 0.000040  loss: 1.6286 (1.7334)  time: 0.1414  data: 0.0004  max mem: 4938\n",
            "Epoch: [10]  [430/781]  eta: 0:00:49  lr: 0.000040  loss: 1.6295 (1.7329)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [440/781]  eta: 0:00:48  lr: 0.000040  loss: 1.6351 (1.7343)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [450/781]  eta: 0:00:46  lr: 0.000040  loss: 1.6611 (1.7373)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [460/781]  eta: 0:00:45  lr: 0.000040  loss: 1.7005 (1.7430)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [470/781]  eta: 0:00:43  lr: 0.000040  loss: 1.7016 (1.7420)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [480/781]  eta: 0:00:42  lr: 0.000040  loss: 1.6774 (1.7440)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [490/781]  eta: 0:00:41  lr: 0.000040  loss: 1.6466 (1.7414)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [500/781]  eta: 0:00:39  lr: 0.000040  loss: 1.6843 (1.7458)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [510/781]  eta: 0:00:38  lr: 0.000040  loss: 1.6571 (1.7424)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [520/781]  eta: 0:00:36  lr: 0.000040  loss: 1.6093 (1.7452)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [530/781]  eta: 0:00:35  lr: 0.000040  loss: 1.6435 (1.7459)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [540/781]  eta: 0:00:33  lr: 0.000040  loss: 1.6432 (1.7465)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [550/781]  eta: 0:00:32  lr: 0.000040  loss: 1.6061 (1.7435)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [560/781]  eta: 0:00:31  lr: 0.000040  loss: 1.6433 (1.7445)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [570/781]  eta: 0:00:29  lr: 0.000040  loss: 1.6715 (1.7451)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [580/781]  eta: 0:00:28  lr: 0.000040  loss: 1.6313 (1.7449)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [590/781]  eta: 0:00:26  lr: 0.000040  loss: 1.6637 (1.7457)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [600/781]  eta: 0:00:25  lr: 0.000040  loss: 1.6637 (1.7449)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [610/781]  eta: 0:00:24  lr: 0.000040  loss: 1.6533 (1.7464)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [620/781]  eta: 0:00:22  lr: 0.000040  loss: 1.6200 (1.7442)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [630/781]  eta: 0:00:21  lr: 0.000040  loss: 1.5764 (1.7441)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [640/781]  eta: 0:00:19  lr: 0.000040  loss: 1.6452 (1.7456)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [650/781]  eta: 0:00:18  lr: 0.000040  loss: 1.6452 (1.7462)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [660/781]  eta: 0:00:16  lr: 0.000040  loss: 1.5944 (1.7443)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [670/781]  eta: 0:00:15  lr: 0.000040  loss: 1.6509 (1.7481)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [680/781]  eta: 0:00:14  lr: 0.000040  loss: 1.6682 (1.7482)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [690/781]  eta: 0:00:12  lr: 0.000040  loss: 1.6435 (1.7471)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [700/781]  eta: 0:00:11  lr: 0.000040  loss: 1.6269 (1.7472)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [710/781]  eta: 0:00:09  lr: 0.000040  loss: 1.6210 (1.7463)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [720/781]  eta: 0:00:08  lr: 0.000040  loss: 1.6231 (1.7465)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [730/781]  eta: 0:00:07  lr: 0.000040  loss: 1.6587 (1.7474)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [740/781]  eta: 0:00:05  lr: 0.000040  loss: 1.5995 (1.7477)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [750/781]  eta: 0:00:04  lr: 0.000040  loss: 1.6176 (1.7503)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [760/781]  eta: 0:00:02  lr: 0.000040  loss: 1.6176 (1.7501)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [770/781]  eta: 0:00:01  lr: 0.000040  loss: 1.6307 (1.7484)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [10]  [780/781]  eta: 0:00:00  lr: 0.000040  loss: 1.6327 (1.7495)  time: 0.1399  data: 0.0007  max mem: 4938\n",
            "Epoch: [10] Total time: 0:01:49 (0.1403 s / it)\n",
            "Averaged stats: lr: 0.000040  loss: 1.6327 (1.7495)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.8867 (0.8867)  acc1: 78.1250 (78.1250)  acc5: 95.3125 (95.3125)  time: 0.8227  data: 0.7919  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.1022 (1.0302)  acc1: 78.1250 (76.7519)  acc5: 92.7083 (93.3239)  time: 0.1731  data: 0.1424  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.1350 (1.0974)  acc1: 72.3958 (75.3224)  acc5: 92.1875 (92.3859)  time: 0.1241  data: 0.0934  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.2005 (1.1222)  acc1: 72.3958 (75.0840)  acc5: 91.1458 (91.9691)  time: 0.1280  data: 0.0973  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2574 (1.1684)  acc1: 73.4375 (74.0600)  acc5: 90.6250 (91.3999)  time: 0.1249  data: 0.0942  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.2464 (1.1618)  acc1: 72.3958 (74.0196)  acc5: 91.6667 (91.6462)  time: 0.1290  data: 0.0983  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2471 (1.1700)  acc1: 72.3958 (73.9700)  acc5: 92.1875 (91.6900)  time: 0.1135  data: 0.0838  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1346 s / it)\n",
            "* Acc@1 73.970 Acc@5 91.690 loss 1.170\n",
            "Accuracy of the network on the 10000 test images: 74.0%\n",
            "Max accuracy: 73.97%\n",
            "Epoch: [11]  [  0/781]  eta: 0:11:45  lr: 0.000036  loss: 1.9585 (1.9585)  time: 0.9031  data: 0.7443  max mem: 4938\n",
            "Epoch: [11]  [ 10/781]  eta: 0:02:42  lr: 0.000036  loss: 1.5983 (1.6108)  time: 0.2105  data: 0.0680  max mem: 4938\n",
            "Epoch: [11]  [ 20/781]  eta: 0:02:15  lr: 0.000036  loss: 1.5861 (1.6300)  time: 0.1423  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 30/781]  eta: 0:02:05  lr: 0.000036  loss: 1.6114 (1.6629)  time: 0.1430  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 40/781]  eta: 0:01:58  lr: 0.000036  loss: 1.6196 (1.6965)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 50/781]  eta: 0:01:54  lr: 0.000036  loss: 1.6434 (1.6947)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 60/781]  eta: 0:01:50  lr: 0.000036  loss: 1.6423 (1.7095)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 70/781]  eta: 0:01:47  lr: 0.000036  loss: 1.6262 (1.7202)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 80/781]  eta: 0:01:45  lr: 0.000036  loss: 1.6272 (1.7556)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [ 90/781]  eta: 0:01:42  lr: 0.000036  loss: 1.6660 (1.7577)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [100/781]  eta: 0:01:40  lr: 0.000036  loss: 1.7013 (1.7569)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [110/781]  eta: 0:01:38  lr: 0.000036  loss: 1.6580 (1.7465)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [120/781]  eta: 0:01:36  lr: 0.000036  loss: 1.5949 (1.7372)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [130/781]  eta: 0:01:35  lr: 0.000036  loss: 1.5935 (1.7391)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [140/781]  eta: 0:01:33  lr: 0.000036  loss: 1.5946 (1.7444)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [150/781]  eta: 0:01:31  lr: 0.000036  loss: 1.6767 (1.7477)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [160/781]  eta: 0:01:29  lr: 0.000036  loss: 1.6822 (1.7462)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [170/781]  eta: 0:01:28  lr: 0.000036  loss: 1.6391 (1.7435)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [180/781]  eta: 0:01:26  lr: 0.000036  loss: 1.5985 (1.7346)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [190/781]  eta: 0:01:25  lr: 0.000036  loss: 1.5762 (1.7257)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [200/781]  eta: 0:01:23  lr: 0.000036  loss: 1.5898 (1.7392)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [210/781]  eta: 0:01:22  lr: 0.000036  loss: 1.6739 (1.7329)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [220/781]  eta: 0:01:20  lr: 0.000036  loss: 1.6709 (1.7393)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [230/781]  eta: 0:01:18  lr: 0.000036  loss: 1.6709 (1.7472)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [240/781]  eta: 0:01:17  lr: 0.000036  loss: 1.7268 (1.7513)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [250/781]  eta: 0:01:15  lr: 0.000036  loss: 1.7268 (1.7518)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [260/781]  eta: 0:01:14  lr: 0.000036  loss: 1.6233 (1.7485)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [270/781]  eta: 0:01:12  lr: 0.000036  loss: 1.6061 (1.7453)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [280/781]  eta: 0:01:11  lr: 0.000036  loss: 1.6061 (1.7444)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [290/781]  eta: 0:01:09  lr: 0.000036  loss: 1.5956 (1.7422)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [300/781]  eta: 0:01:08  lr: 0.000036  loss: 1.6231 (1.7379)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [310/781]  eta: 0:01:06  lr: 0.000036  loss: 1.6052 (1.7376)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [320/781]  eta: 0:01:05  lr: 0.000036  loss: 1.6116 (1.7405)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [330/781]  eta: 0:01:03  lr: 0.000036  loss: 1.6380 (1.7368)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [340/781]  eta: 0:01:02  lr: 0.000036  loss: 1.6475 (1.7428)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [350/781]  eta: 0:01:01  lr: 0.000036  loss: 1.6400 (1.7458)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [360/781]  eta: 0:00:59  lr: 0.000036  loss: 1.6742 (1.7502)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [370/781]  eta: 0:00:58  lr: 0.000036  loss: 1.6454 (1.7467)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [380/781]  eta: 0:00:56  lr: 0.000036  loss: 1.6454 (1.7499)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [390/781]  eta: 0:00:55  lr: 0.000036  loss: 1.6494 (1.7480)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [400/781]  eta: 0:00:53  lr: 0.000036  loss: 1.6375 (1.7477)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [410/781]  eta: 0:00:52  lr: 0.000036  loss: 1.5970 (1.7440)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [420/781]  eta: 0:00:50  lr: 0.000036  loss: 1.5932 (1.7474)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [430/781]  eta: 0:00:49  lr: 0.000036  loss: 1.6223 (1.7486)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [440/781]  eta: 0:00:48  lr: 0.000036  loss: 1.6223 (1.7482)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [450/781]  eta: 0:00:46  lr: 0.000036  loss: 1.6495 (1.7487)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [460/781]  eta: 0:00:45  lr: 0.000036  loss: 1.6463 (1.7469)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [470/781]  eta: 0:00:43  lr: 0.000036  loss: 1.6178 (1.7441)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [480/781]  eta: 0:00:42  lr: 0.000036  loss: 1.5479 (1.7393)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [490/781]  eta: 0:00:40  lr: 0.000036  loss: 1.5437 (1.7388)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [500/781]  eta: 0:00:39  lr: 0.000036  loss: 1.6213 (1.7386)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [510/781]  eta: 0:00:38  lr: 0.000036  loss: 1.6213 (1.7363)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [520/781]  eta: 0:00:36  lr: 0.000036  loss: 1.6034 (1.7358)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [530/781]  eta: 0:00:35  lr: 0.000036  loss: 1.6315 (1.7404)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [540/781]  eta: 0:00:33  lr: 0.000036  loss: 1.6445 (1.7440)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [550/781]  eta: 0:00:32  lr: 0.000036  loss: 1.6445 (1.7467)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [560/781]  eta: 0:00:31  lr: 0.000036  loss: 1.6487 (1.7508)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [570/781]  eta: 0:00:29  lr: 0.000036  loss: 1.6935 (1.7524)  time: 0.1417  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [580/781]  eta: 0:00:28  lr: 0.000036  loss: 1.6644 (1.7508)  time: 0.1414  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [590/781]  eta: 0:00:26  lr: 0.000036  loss: 1.6600 (1.7502)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [600/781]  eta: 0:00:25  lr: 0.000036  loss: 1.6097 (1.7498)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [610/781]  eta: 0:00:24  lr: 0.000036  loss: 1.5764 (1.7501)  time: 0.1415  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [620/781]  eta: 0:00:22  lr: 0.000036  loss: 1.6215 (1.7490)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [630/781]  eta: 0:00:21  lr: 0.000036  loss: 1.6662 (1.7505)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [640/781]  eta: 0:00:19  lr: 0.000036  loss: 1.5973 (1.7493)  time: 0.1417  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [650/781]  eta: 0:00:18  lr: 0.000036  loss: 1.5973 (1.7484)  time: 0.1420  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [660/781]  eta: 0:00:17  lr: 0.000036  loss: 1.5812 (1.7473)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [670/781]  eta: 0:00:15  lr: 0.000036  loss: 1.5936 (1.7489)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [680/781]  eta: 0:00:14  lr: 0.000036  loss: 1.5999 (1.7461)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [690/781]  eta: 0:00:12  lr: 0.000036  loss: 1.5581 (1.7448)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [700/781]  eta: 0:00:11  lr: 0.000036  loss: 1.5581 (1.7430)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [710/781]  eta: 0:00:09  lr: 0.000036  loss: 1.5520 (1.7402)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [720/781]  eta: 0:00:08  lr: 0.000036  loss: 1.5668 (1.7399)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [730/781]  eta: 0:00:07  lr: 0.000036  loss: 1.5991 (1.7400)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [740/781]  eta: 0:00:05  lr: 0.000036  loss: 1.6171 (1.7397)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [750/781]  eta: 0:00:04  lr: 0.000036  loss: 1.6066 (1.7378)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [760/781]  eta: 0:00:02  lr: 0.000036  loss: 1.5921 (1.7373)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [770/781]  eta: 0:00:01  lr: 0.000036  loss: 1.5921 (1.7356)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [11]  [780/781]  eta: 0:00:00  lr: 0.000036  loss: 1.5982 (1.7353)  time: 0.1400  data: 0.0006  max mem: 4938\n",
            "Epoch: [11] Total time: 0:01:49 (0.1405 s / it)\n",
            "Averaged stats: lr: 0.000036  loss: 1.5982 (1.7353)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.7737 (0.7737)  acc1: 82.2917 (82.2917)  acc5: 96.8750 (96.8750)  time: 0.8397  data: 0.8088  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0763 (1.0038)  acc1: 78.1250 (77.6042)  acc5: 93.2292 (93.1345)  time: 0.1751  data: 0.1444  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0827 (1.0455)  acc1: 72.3958 (76.7857)  acc5: 92.7083 (92.5595)  time: 0.1260  data: 0.0953  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1662 (1.0920)  acc1: 72.3958 (75.7561)  acc5: 92.1875 (92.1707)  time: 0.1268  data: 0.0960  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.2353 (1.1316)  acc1: 72.3958 (74.8095)  acc5: 91.1458 (91.6794)  time: 0.1310  data: 0.1003  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1791 (1.1297)  acc1: 72.3958 (74.6630)  acc5: 91.6667 (91.7382)  time: 0.1327  data: 0.1020  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.2058 (1.1369)  acc1: 71.3542 (74.5400)  acc5: 92.1875 (91.7700)  time: 0.1107  data: 0.0810  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1361 s / it)\n",
            "* Acc@1 74.540 Acc@5 91.770 loss 1.137\n",
            "Accuracy of the network on the 10000 test images: 74.5%\n",
            "Max accuracy: 74.54%\n",
            "Epoch: [12]  [  0/781]  eta: 0:11:38  lr: 0.000032  loss: 2.8514 (2.8514)  time: 0.8946  data: 0.7294  max mem: 4938\n",
            "Epoch: [12]  [ 10/781]  eta: 0:02:43  lr: 0.000032  loss: 1.5976 (1.8455)  time: 0.2115  data: 0.0713  max mem: 4938\n",
            "Epoch: [12]  [ 20/781]  eta: 0:02:14  lr: 0.000032  loss: 1.5732 (1.7405)  time: 0.1404  data: 0.0029  max mem: 4938\n",
            "Epoch: [12]  [ 30/781]  eta: 0:02:03  lr: 0.000032  loss: 1.5735 (1.7410)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [ 40/781]  eta: 0:01:57  lr: 0.000032  loss: 1.5849 (1.7090)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [ 50/781]  eta: 0:01:53  lr: 0.000032  loss: 1.5849 (1.7354)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [ 60/781]  eta: 0:01:49  lr: 0.000032  loss: 1.6300 (1.7636)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [ 70/781]  eta: 0:01:47  lr: 0.000032  loss: 1.6300 (1.7619)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [ 80/781]  eta: 0:01:44  lr: 0.000032  loss: 1.6012 (1.7541)  time: 0.1411  data: 0.0004  max mem: 4938\n",
            "Epoch: [12]  [ 90/781]  eta: 0:01:42  lr: 0.000032  loss: 1.6046 (1.7447)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [100/781]  eta: 0:01:40  lr: 0.000032  loss: 1.6418 (1.7644)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [110/781]  eta: 0:01:38  lr: 0.000032  loss: 1.7076 (1.7709)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [120/781]  eta: 0:01:36  lr: 0.000032  loss: 1.5974 (1.7630)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [130/781]  eta: 0:01:34  lr: 0.000032  loss: 1.5584 (1.7668)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [140/781]  eta: 0:01:32  lr: 0.000032  loss: 1.5680 (1.7788)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [150/781]  eta: 0:01:31  lr: 0.000032  loss: 1.6337 (1.7904)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [160/781]  eta: 0:01:29  lr: 0.000032  loss: 1.5749 (1.7778)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [170/781]  eta: 0:01:28  lr: 0.000032  loss: 1.5749 (1.7767)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [180/781]  eta: 0:01:26  lr: 0.000032  loss: 1.6319 (1.7754)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [190/781]  eta: 0:01:24  lr: 0.000032  loss: 1.5663 (1.7667)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [200/781]  eta: 0:01:23  lr: 0.000032  loss: 1.6033 (1.7682)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [210/781]  eta: 0:01:21  lr: 0.000032  loss: 1.5986 (1.7639)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [220/781]  eta: 0:01:20  lr: 0.000032  loss: 1.6120 (1.7629)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [230/781]  eta: 0:01:18  lr: 0.000032  loss: 1.6120 (1.7590)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [240/781]  eta: 0:01:17  lr: 0.000032  loss: 1.5534 (1.7512)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [250/781]  eta: 0:01:15  lr: 0.000032  loss: 1.5807 (1.7497)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [260/781]  eta: 0:01:14  lr: 0.000032  loss: 1.5807 (1.7476)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [270/781]  eta: 0:01:12  lr: 0.000032  loss: 1.5740 (1.7534)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [280/781]  eta: 0:01:11  lr: 0.000032  loss: 1.6198 (1.7552)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [290/781]  eta: 0:01:09  lr: 0.000032  loss: 1.5470 (1.7498)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [300/781]  eta: 0:01:08  lr: 0.000032  loss: 1.5736 (1.7548)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [310/781]  eta: 0:01:06  lr: 0.000032  loss: 1.6094 (1.7528)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [320/781]  eta: 0:01:05  lr: 0.000032  loss: 1.5817 (1.7534)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [330/781]  eta: 0:01:03  lr: 0.000032  loss: 1.5728 (1.7563)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [340/781]  eta: 0:01:02  lr: 0.000032  loss: 1.6060 (1.7552)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [350/781]  eta: 0:01:01  lr: 0.000032  loss: 1.6513 (1.7565)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [360/781]  eta: 0:00:59  lr: 0.000032  loss: 1.6513 (1.7542)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [370/781]  eta: 0:00:58  lr: 0.000032  loss: 1.6437 (1.7529)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [380/781]  eta: 0:00:56  lr: 0.000032  loss: 1.5996 (1.7550)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [390/781]  eta: 0:00:55  lr: 0.000032  loss: 1.6509 (1.7544)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [400/781]  eta: 0:00:53  lr: 0.000032  loss: 1.6541 (1.7534)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [410/781]  eta: 0:00:52  lr: 0.000032  loss: 1.5965 (1.7545)  time: 0.1419  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [420/781]  eta: 0:00:51  lr: 0.000032  loss: 1.5809 (1.7544)  time: 0.1421  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [430/781]  eta: 0:00:49  lr: 0.000032  loss: 1.6396 (1.7517)  time: 0.1514  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [440/781]  eta: 0:00:48  lr: 0.000032  loss: 1.6782 (1.7531)  time: 0.1512  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [450/781]  eta: 0:00:46  lr: 0.000032  loss: 1.6339 (1.7530)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [460/781]  eta: 0:00:45  lr: 0.000032  loss: 1.6056 (1.7511)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [470/781]  eta: 0:00:44  lr: 0.000032  loss: 1.5790 (1.7491)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [480/781]  eta: 0:00:42  lr: 0.000032  loss: 1.5790 (1.7478)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [490/781]  eta: 0:00:41  lr: 0.000032  loss: 1.5943 (1.7467)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [500/781]  eta: 0:00:39  lr: 0.000032  loss: 1.6186 (1.7454)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [510/781]  eta: 0:00:38  lr: 0.000032  loss: 1.6319 (1.7475)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [520/781]  eta: 0:00:36  lr: 0.000032  loss: 1.6158 (1.7440)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [530/781]  eta: 0:00:35  lr: 0.000032  loss: 1.6053 (1.7460)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [540/781]  eta: 0:00:34  lr: 0.000032  loss: 1.6778 (1.7462)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [550/781]  eta: 0:00:32  lr: 0.000032  loss: 1.6733 (1.7456)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [560/781]  eta: 0:00:31  lr: 0.000032  loss: 1.5944 (1.7433)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [570/781]  eta: 0:00:29  lr: 0.000032  loss: 1.5996 (1.7407)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [580/781]  eta: 0:00:28  lr: 0.000032  loss: 1.5954 (1.7404)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [590/781]  eta: 0:00:26  lr: 0.000032  loss: 1.5875 (1.7398)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [600/781]  eta: 0:00:25  lr: 0.000032  loss: 1.5838 (1.7408)  time: 0.1408  data: 0.0004  max mem: 4938\n",
            "Epoch: [12]  [610/781]  eta: 0:00:24  lr: 0.000032  loss: 1.6077 (1.7408)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [620/781]  eta: 0:00:22  lr: 0.000032  loss: 1.6389 (1.7386)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [630/781]  eta: 0:00:21  lr: 0.000032  loss: 1.5626 (1.7358)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [640/781]  eta: 0:00:19  lr: 0.000032  loss: 1.5662 (1.7379)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [650/781]  eta: 0:00:18  lr: 0.000032  loss: 1.6189 (1.7371)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [660/781]  eta: 0:00:17  lr: 0.000032  loss: 1.6200 (1.7379)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [670/781]  eta: 0:00:15  lr: 0.000032  loss: 1.6113 (1.7386)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [680/781]  eta: 0:00:14  lr: 0.000032  loss: 1.6113 (1.7366)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [690/781]  eta: 0:00:12  lr: 0.000032  loss: 1.6175 (1.7351)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [700/781]  eta: 0:00:11  lr: 0.000032  loss: 1.5998 (1.7365)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [710/781]  eta: 0:00:10  lr: 0.000032  loss: 1.5843 (1.7341)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [720/781]  eta: 0:00:08  lr: 0.000032  loss: 1.5843 (1.7341)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [730/781]  eta: 0:00:07  lr: 0.000032  loss: 1.6150 (1.7336)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [740/781]  eta: 0:00:05  lr: 0.000032  loss: 1.6051 (1.7357)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [750/781]  eta: 0:00:04  lr: 0.000032  loss: 1.6362 (1.7359)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [760/781]  eta: 0:00:02  lr: 0.000032  loss: 1.5877 (1.7342)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [770/781]  eta: 0:00:01  lr: 0.000032  loss: 1.5572 (1.7322)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [12]  [780/781]  eta: 0:00:00  lr: 0.000032  loss: 1.5633 (1.7308)  time: 0.1401  data: 0.0006  max mem: 4938\n",
            "Epoch: [12] Total time: 0:01:49 (0.1408 s / it)\n",
            "Averaged stats: lr: 0.000032  loss: 1.5633 (1.7308)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.9039 (0.9039)  acc1: 77.6042 (77.6042)  acc5: 94.2708 (94.2708)  time: 0.8307  data: 0.7998  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 1.0330 (0.9853)  acc1: 77.6042 (78.1250)  acc5: 94.2708 (93.7974)  time: 0.1581  data: 0.1274  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.0389 (1.0311)  acc1: 75.0000 (77.2321)  acc5: 93.2292 (92.7579)  time: 0.1085  data: 0.0778  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1438 (1.0808)  acc1: 72.9167 (76.1593)  acc5: 92.1875 (92.1875)  time: 0.1306  data: 0.0997  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1760 (1.1159)  acc1: 72.9167 (75.1906)  acc5: 91.1458 (91.7175)  time: 0.1222  data: 0.0908  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1527 (1.1091)  acc1: 74.4792 (75.1736)  acc5: 92.1875 (91.9016)  time: 0.1297  data: 0.0985  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1580 (1.1137)  acc1: 72.9167 (75.0500)  acc5: 92.7083 (91.9300)  time: 0.1283  data: 0.0985  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1329 s / it)\n",
            "* Acc@1 75.050 Acc@5 91.930 loss 1.114\n",
            "Accuracy of the network on the 10000 test images: 75.1%\n",
            "Max accuracy: 75.05%\n",
            "Epoch: [13]  [  0/781]  eta: 0:11:49  lr: 0.000028  loss: 1.4754 (1.4754)  time: 0.9089  data: 0.7666  max mem: 4938\n",
            "Epoch: [13]  [ 10/781]  eta: 0:02:41  lr: 0.000028  loss: 1.5519 (1.6199)  time: 0.2097  data: 0.0700  max mem: 4938\n",
            "Epoch: [13]  [ 20/781]  eta: 0:02:15  lr: 0.000028  loss: 1.5878 (1.6483)  time: 0.1414  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 30/781]  eta: 0:02:04  lr: 0.000028  loss: 1.5641 (1.6498)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 40/781]  eta: 0:01:57  lr: 0.000028  loss: 1.5600 (1.6495)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 50/781]  eta: 0:01:53  lr: 0.000028  loss: 1.5501 (1.6289)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 60/781]  eta: 0:01:49  lr: 0.000028  loss: 1.5426 (1.6210)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 70/781]  eta: 0:01:47  lr: 0.000028  loss: 1.6039 (1.6678)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 80/781]  eta: 0:01:44  lr: 0.000028  loss: 1.6145 (1.6650)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [ 90/781]  eta: 0:01:42  lr: 0.000028  loss: 1.5868 (1.6676)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [100/781]  eta: 0:01:40  lr: 0.000028  loss: 1.5589 (1.6901)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [110/781]  eta: 0:01:38  lr: 0.000028  loss: 1.5690 (1.6795)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [120/781]  eta: 0:01:36  lr: 0.000028  loss: 1.5690 (1.6790)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [130/781]  eta: 0:01:34  lr: 0.000028  loss: 1.6031 (1.6899)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [140/781]  eta: 0:01:32  lr: 0.000028  loss: 1.5659 (1.6884)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [150/781]  eta: 0:01:31  lr: 0.000028  loss: 1.5550 (1.6887)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [160/781]  eta: 0:01:29  lr: 0.000028  loss: 1.5806 (1.6945)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [170/781]  eta: 0:01:27  lr: 0.000028  loss: 1.5806 (1.6885)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [180/781]  eta: 0:01:26  lr: 0.000028  loss: 1.5568 (1.6820)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [190/781]  eta: 0:01:24  lr: 0.000028  loss: 1.5434 (1.6804)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [200/781]  eta: 0:01:23  lr: 0.000028  loss: 1.5426 (1.6795)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [210/781]  eta: 0:01:21  lr: 0.000028  loss: 1.5659 (1.6779)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [220/781]  eta: 0:01:20  lr: 0.000028  loss: 1.6334 (1.6834)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [230/781]  eta: 0:01:18  lr: 0.000028  loss: 1.6204 (1.6842)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [240/781]  eta: 0:01:17  lr: 0.000028  loss: 1.6204 (1.6814)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [250/781]  eta: 0:01:15  lr: 0.000028  loss: 1.6094 (1.6856)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [260/781]  eta: 0:01:14  lr: 0.000028  loss: 1.5887 (1.6994)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [270/781]  eta: 0:01:12  lr: 0.000028  loss: 1.6038 (1.6996)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [280/781]  eta: 0:01:11  lr: 0.000028  loss: 1.5896 (1.6997)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [290/781]  eta: 0:01:09  lr: 0.000028  loss: 1.5723 (1.7017)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [300/781]  eta: 0:01:08  lr: 0.000028  loss: 1.5601 (1.7018)  time: 0.1419  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [310/781]  eta: 0:01:06  lr: 0.000028  loss: 1.5957 (1.7058)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [320/781]  eta: 0:01:05  lr: 0.000028  loss: 1.6406 (1.7066)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [330/781]  eta: 0:01:03  lr: 0.000028  loss: 1.5787 (1.7039)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [340/781]  eta: 0:01:02  lr: 0.000028  loss: 1.5358 (1.7008)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [350/781]  eta: 0:01:01  lr: 0.000028  loss: 1.6012 (1.7054)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [360/781]  eta: 0:00:59  lr: 0.000028  loss: 1.6596 (1.7096)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [370/781]  eta: 0:00:58  lr: 0.000028  loss: 1.6596 (1.7132)  time: 0.1415  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [380/781]  eta: 0:00:56  lr: 0.000028  loss: 1.5807 (1.7121)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [390/781]  eta: 0:00:55  lr: 0.000028  loss: 1.5567 (1.7136)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [400/781]  eta: 0:00:53  lr: 0.000028  loss: 1.5844 (1.7127)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [410/781]  eta: 0:00:52  lr: 0.000028  loss: 1.6372 (1.7135)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [420/781]  eta: 0:00:51  lr: 0.000028  loss: 1.6447 (1.7166)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [430/781]  eta: 0:00:49  lr: 0.000028  loss: 1.6393 (1.7136)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [440/781]  eta: 0:00:48  lr: 0.000028  loss: 1.5842 (1.7140)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [450/781]  eta: 0:00:46  lr: 0.000028  loss: 1.5714 (1.7120)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [460/781]  eta: 0:00:45  lr: 0.000028  loss: 1.5737 (1.7103)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [470/781]  eta: 0:00:43  lr: 0.000028  loss: 1.5831 (1.7173)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [480/781]  eta: 0:00:42  lr: 0.000028  loss: 1.5706 (1.7150)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [490/781]  eta: 0:00:41  lr: 0.000028  loss: 1.5706 (1.7183)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [500/781]  eta: 0:00:39  lr: 0.000028  loss: 1.6216 (1.7198)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [510/781]  eta: 0:00:38  lr: 0.000028  loss: 1.6031 (1.7223)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [520/781]  eta: 0:00:36  lr: 0.000028  loss: 1.6021 (1.7214)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [530/781]  eta: 0:00:35  lr: 0.000028  loss: 1.6137 (1.7215)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [540/781]  eta: 0:00:33  lr: 0.000028  loss: 1.6265 (1.7245)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [550/781]  eta: 0:00:32  lr: 0.000028  loss: 1.5825 (1.7220)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [560/781]  eta: 0:00:31  lr: 0.000028  loss: 1.5816 (1.7235)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [570/781]  eta: 0:00:29  lr: 0.000028  loss: 1.5816 (1.7209)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [580/781]  eta: 0:00:28  lr: 0.000028  loss: 1.5561 (1.7192)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [590/781]  eta: 0:00:26  lr: 0.000028  loss: 1.5964 (1.7172)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [600/781]  eta: 0:00:25  lr: 0.000028  loss: 1.5312 (1.7152)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [610/781]  eta: 0:00:24  lr: 0.000028  loss: 1.5394 (1.7155)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [620/781]  eta: 0:00:22  lr: 0.000028  loss: 1.6109 (1.7155)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [630/781]  eta: 0:00:21  lr: 0.000028  loss: 1.6602 (1.7186)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [640/781]  eta: 0:00:19  lr: 0.000028  loss: 1.6183 (1.7194)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [650/781]  eta: 0:00:18  lr: 0.000028  loss: 1.5786 (1.7184)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [660/781]  eta: 0:00:17  lr: 0.000028  loss: 1.6064 (1.7193)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [670/781]  eta: 0:00:15  lr: 0.000028  loss: 1.6136 (1.7217)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [680/781]  eta: 0:00:14  lr: 0.000028  loss: 1.5540 (1.7185)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [690/781]  eta: 0:00:12  lr: 0.000028  loss: 1.5186 (1.7184)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [700/781]  eta: 0:00:11  lr: 0.000028  loss: 1.5427 (1.7157)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [710/781]  eta: 0:00:09  lr: 0.000028  loss: 1.5339 (1.7179)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [720/781]  eta: 0:00:08  lr: 0.000028  loss: 1.5639 (1.7178)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [730/781]  eta: 0:00:07  lr: 0.000028  loss: 1.5859 (1.7171)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [740/781]  eta: 0:00:05  lr: 0.000028  loss: 1.5805 (1.7161)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [750/781]  eta: 0:00:04  lr: 0.000028  loss: 1.6356 (1.7172)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [760/781]  eta: 0:00:02  lr: 0.000028  loss: 1.5758 (1.7157)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [770/781]  eta: 0:00:01  lr: 0.000028  loss: 1.5644 (1.7154)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [13]  [780/781]  eta: 0:00:00  lr: 0.000028  loss: 1.5634 (1.7142)  time: 0.1380  data: 0.0006  max mem: 4938\n",
            "Epoch: [13] Total time: 0:01:49 (0.1404 s / it)\n",
            "Averaged stats: lr: 0.000028  loss: 1.5634 (1.7142)\n",
            "Test:  [ 0/53]  eta: 0:00:42  loss: 0.7636 (0.7636)  acc1: 81.2500 (81.2500)  acc5: 96.3542 (96.3542)  time: 0.7975  data: 0.7605  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0256 (0.9542)  acc1: 79.6875 (79.1667)  acc5: 94.2708 (93.6553)  time: 0.1737  data: 0.1424  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0465 (1.0124)  acc1: 76.0417 (77.9514)  acc5: 93.7500 (92.9812)  time: 0.1314  data: 0.1007  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1073 (1.0563)  acc1: 74.4792 (76.8481)  acc5: 92.1875 (92.5235)  time: 0.1335  data: 0.1028  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1498 (1.0951)  acc1: 73.4375 (75.6987)  acc5: 91.1458 (92.0986)  time: 0.1299  data: 0.0991  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1320 (1.0917)  acc1: 73.9583 (75.7557)  acc5: 92.1875 (92.2284)  time: 0.1386  data: 0.1075  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1407 (1.0975)  acc1: 73.9583 (75.6400)  acc5: 92.1875 (92.2500)  time: 0.1221  data: 0.0918  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1406 s / it)\n",
            "* Acc@1 75.640 Acc@5 92.250 loss 1.098\n",
            "Accuracy of the network on the 10000 test images: 75.6%\n",
            "Max accuracy: 75.64%\n",
            "Epoch: [14]  [  0/781]  eta: 0:11:20  lr: 0.000024  loss: 1.5613 (1.5613)  time: 0.8716  data: 0.7201  max mem: 4938\n",
            "Epoch: [14]  [ 10/781]  eta: 0:02:37  lr: 0.000024  loss: 1.6096 (1.6704)  time: 0.2049  data: 0.0658  max mem: 4938\n",
            "Epoch: [14]  [ 20/781]  eta: 0:02:11  lr: 0.000024  loss: 1.6096 (1.7621)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 30/781]  eta: 0:02:01  lr: 0.000024  loss: 1.6029 (1.7442)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 40/781]  eta: 0:01:55  lr: 0.000024  loss: 1.5469 (1.7237)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 50/781]  eta: 0:01:51  lr: 0.000024  loss: 1.5434 (1.7146)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 60/781]  eta: 0:01:48  lr: 0.000024  loss: 1.5357 (1.6874)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 70/781]  eta: 0:01:46  lr: 0.000024  loss: 1.5357 (1.6685)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 80/781]  eta: 0:01:43  lr: 0.000024  loss: 1.5737 (1.6857)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [ 90/781]  eta: 0:01:41  lr: 0.000024  loss: 1.5377 (1.6687)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [100/781]  eta: 0:01:39  lr: 0.000024  loss: 1.5286 (1.6557)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [110/781]  eta: 0:01:37  lr: 0.000024  loss: 1.5659 (1.6657)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [120/781]  eta: 0:01:35  lr: 0.000024  loss: 1.6145 (1.6622)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [130/781]  eta: 0:01:33  lr: 0.000024  loss: 1.5485 (1.6513)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [140/781]  eta: 0:01:32  lr: 0.000024  loss: 1.5253 (1.6510)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [150/781]  eta: 0:01:30  lr: 0.000024  loss: 1.5803 (1.6656)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [160/781]  eta: 0:01:29  lr: 0.000024  loss: 1.5626 (1.6623)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [170/781]  eta: 0:01:27  lr: 0.000024  loss: 1.5404 (1.6599)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [180/781]  eta: 0:01:25  lr: 0.000024  loss: 1.5729 (1.6697)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [190/781]  eta: 0:01:24  lr: 0.000024  loss: 1.5982 (1.6701)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [200/781]  eta: 0:01:22  lr: 0.000024  loss: 1.5586 (1.6633)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [210/781]  eta: 0:01:21  lr: 0.000024  loss: 1.5317 (1.6669)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [220/781]  eta: 0:01:19  lr: 0.000024  loss: 1.5707 (1.6761)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [230/781]  eta: 0:01:18  lr: 0.000024  loss: 1.5694 (1.6719)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [240/781]  eta: 0:01:16  lr: 0.000024  loss: 1.5549 (1.6753)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [250/781]  eta: 0:01:15  lr: 0.000024  loss: 1.5633 (1.6752)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [260/781]  eta: 0:01:13  lr: 0.000024  loss: 1.5616 (1.6771)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [270/781]  eta: 0:01:12  lr: 0.000024  loss: 1.5872 (1.6805)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [280/781]  eta: 0:01:10  lr: 0.000024  loss: 1.5990 (1.6856)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [290/781]  eta: 0:01:09  lr: 0.000024  loss: 1.5782 (1.6916)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [300/781]  eta: 0:01:07  lr: 0.000024  loss: 1.5899 (1.6942)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [310/781]  eta: 0:01:06  lr: 0.000024  loss: 1.6027 (1.6927)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [320/781]  eta: 0:01:05  lr: 0.000024  loss: 1.5402 (1.6876)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [330/781]  eta: 0:01:03  lr: 0.000024  loss: 1.5345 (1.6936)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [340/781]  eta: 0:01:02  lr: 0.000024  loss: 1.6178 (1.6938)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [350/781]  eta: 0:01:00  lr: 0.000024  loss: 1.6112 (1.6914)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [360/781]  eta: 0:00:59  lr: 0.000024  loss: 1.6013 (1.6947)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [370/781]  eta: 0:00:57  lr: 0.000024  loss: 1.5792 (1.6938)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [380/781]  eta: 0:00:56  lr: 0.000024  loss: 1.5142 (1.6901)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [390/781]  eta: 0:00:55  lr: 0.000024  loss: 1.5294 (1.6915)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [400/781]  eta: 0:00:53  lr: 0.000024  loss: 1.5534 (1.6887)  time: 0.1398  data: 0.0005  max mem: 4938\n",
            "Epoch: [14]  [410/781]  eta: 0:00:52  lr: 0.000024  loss: 1.5639 (1.6893)  time: 0.1393  data: 0.0005  max mem: 4938\n",
            "Epoch: [14]  [420/781]  eta: 0:00:50  lr: 0.000024  loss: 1.6094 (1.6870)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [430/781]  eta: 0:00:49  lr: 0.000024  loss: 1.5742 (1.6905)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [440/781]  eta: 0:00:47  lr: 0.000024  loss: 1.5821 (1.6924)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [450/781]  eta: 0:00:46  lr: 0.000024  loss: 1.5821 (1.6939)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [460/781]  eta: 0:00:45  lr: 0.000024  loss: 1.5669 (1.6982)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [470/781]  eta: 0:00:43  lr: 0.000024  loss: 1.7678 (1.7035)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [480/781]  eta: 0:00:42  lr: 0.000024  loss: 1.6110 (1.7006)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [490/781]  eta: 0:00:40  lr: 0.000024  loss: 1.5566 (1.7009)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [500/781]  eta: 0:00:39  lr: 0.000024  loss: 1.5489 (1.6998)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [510/781]  eta: 0:00:38  lr: 0.000024  loss: 1.5504 (1.6994)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [520/781]  eta: 0:00:36  lr: 0.000024  loss: 1.5376 (1.6988)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [530/781]  eta: 0:00:35  lr: 0.000024  loss: 1.5180 (1.6996)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [540/781]  eta: 0:00:33  lr: 0.000024  loss: 1.5388 (1.7000)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [550/781]  eta: 0:00:32  lr: 0.000024  loss: 1.5250 (1.6981)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [560/781]  eta: 0:00:30  lr: 0.000024  loss: 1.5329 (1.6991)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [570/781]  eta: 0:00:29  lr: 0.000024  loss: 1.5930 (1.6973)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [580/781]  eta: 0:00:28  lr: 0.000024  loss: 1.5842 (1.6963)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [590/781]  eta: 0:00:26  lr: 0.000024  loss: 1.6218 (1.7011)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [600/781]  eta: 0:00:25  lr: 0.000024  loss: 1.6523 (1.7041)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [610/781]  eta: 0:00:24  lr: 0.000024  loss: 1.6289 (1.7059)  time: 0.1489  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [620/781]  eta: 0:00:22  lr: 0.000024  loss: 1.6751 (1.7076)  time: 0.1488  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [630/781]  eta: 0:00:21  lr: 0.000024  loss: 1.6631 (1.7070)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [640/781]  eta: 0:00:19  lr: 0.000024  loss: 1.5768 (1.7071)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [650/781]  eta: 0:00:18  lr: 0.000024  loss: 1.6031 (1.7069)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [660/781]  eta: 0:00:16  lr: 0.000024  loss: 1.5682 (1.7062)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [670/781]  eta: 0:00:15  lr: 0.000024  loss: 1.5318 (1.7049)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [680/781]  eta: 0:00:14  lr: 0.000024  loss: 1.5589 (1.7073)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [690/781]  eta: 0:00:12  lr: 0.000024  loss: 1.5396 (1.7045)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [700/781]  eta: 0:00:11  lr: 0.000024  loss: 1.5369 (1.7031)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [710/781]  eta: 0:00:09  lr: 0.000024  loss: 1.6004 (1.7021)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [720/781]  eta: 0:00:08  lr: 0.000024  loss: 1.6529 (1.7043)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [730/781]  eta: 0:00:07  lr: 0.000024  loss: 1.6101 (1.7023)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [740/781]  eta: 0:00:05  lr: 0.000024  loss: 1.5389 (1.7007)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [750/781]  eta: 0:00:04  lr: 0.000024  loss: 1.5360 (1.7005)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [760/781]  eta: 0:00:02  lr: 0.000024  loss: 1.5336 (1.6999)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [770/781]  eta: 0:00:01  lr: 0.000024  loss: 1.5347 (1.7008)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [14]  [780/781]  eta: 0:00:00  lr: 0.000024  loss: 1.5525 (1.7013)  time: 0.1381  data: 0.0007  max mem: 4938\n",
            "Epoch: [14] Total time: 0:01:49 (0.1402 s / it)\n",
            "Averaged stats: lr: 0.000024  loss: 1.5525 (1.7013)\n",
            "Test:  [ 0/53]  eta: 0:00:43  loss: 0.7849 (0.7849)  acc1: 82.2917 (82.2917)  acc5: 95.8333 (95.8333)  time: 0.8162  data: 0.7853  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0892 (0.9710)  acc1: 77.0833 (78.6458)  acc5: 93.7500 (93.9394)  time: 0.1749  data: 0.1442  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0892 (1.0245)  acc1: 75.0000 (77.5298)  acc5: 93.2292 (93.1796)  time: 0.1281  data: 0.0975  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1335 (1.0630)  acc1: 73.9583 (76.7137)  acc5: 91.6667 (92.6075)  time: 0.1284  data: 0.0977  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1894 (1.1012)  acc1: 73.9583 (75.6479)  acc5: 90.6250 (92.1240)  time: 0.1197  data: 0.0890  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.0936 (1.0968)  acc1: 73.4375 (75.7047)  acc5: 92.1875 (92.2386)  time: 0.1286  data: 0.0979  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1529 (1.1038)  acc1: 73.4375 (75.6300)  acc5: 92.1875 (92.2600)  time: 0.1148  data: 0.0851  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1348 s / it)\n",
            "* Acc@1 75.630 Acc@5 92.260 loss 1.104\n",
            "Accuracy of the network on the 10000 test images: 75.6%\n",
            "Max accuracy: 75.64%\n",
            "Epoch: [15]  [  0/781]  eta: 0:11:30  lr: 0.000021  loss: 1.5519 (1.5519)  time: 0.8836  data: 0.7362  max mem: 4938\n",
            "Epoch: [15]  [ 10/781]  eta: 0:02:41  lr: 0.000021  loss: 1.5519 (1.7530)  time: 0.2089  data: 0.0685  max mem: 4938\n",
            "Epoch: [15]  [ 20/781]  eta: 0:02:15  lr: 0.000021  loss: 1.5562 (1.6676)  time: 0.1425  data: 0.0010  max mem: 4938\n",
            "Epoch: [15]  [ 30/781]  eta: 0:02:04  lr: 0.000021  loss: 1.5561 (1.6293)  time: 0.1413  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 40/781]  eta: 0:01:57  lr: 0.000021  loss: 1.5561 (1.6584)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 50/781]  eta: 0:01:52  lr: 0.000021  loss: 1.5992 (1.6607)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 60/781]  eta: 0:01:49  lr: 0.000021  loss: 1.5388 (1.6554)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 70/781]  eta: 0:01:46  lr: 0.000021  loss: 1.5388 (1.6679)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 80/781]  eta: 0:01:44  lr: 0.000021  loss: 1.5182 (1.6667)  time: 0.1404  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [ 90/781]  eta: 0:01:42  lr: 0.000021  loss: 1.5285 (1.6615)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [100/781]  eta: 0:01:40  lr: 0.000021  loss: 1.5447 (1.6477)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [110/781]  eta: 0:01:38  lr: 0.000021  loss: 1.5429 (1.6544)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [120/781]  eta: 0:01:36  lr: 0.000021  loss: 1.5879 (1.6639)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [130/781]  eta: 0:01:34  lr: 0.000021  loss: 1.5712 (1.6762)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [140/781]  eta: 0:01:32  lr: 0.000021  loss: 1.5874 (1.6794)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [150/781]  eta: 0:01:31  lr: 0.000021  loss: 1.5874 (1.6921)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [160/781]  eta: 0:01:29  lr: 0.000021  loss: 1.5857 (1.6921)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [170/781]  eta: 0:01:27  lr: 0.000021  loss: 1.5445 (1.6906)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [180/781]  eta: 0:01:26  lr: 0.000021  loss: 1.5259 (1.6840)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [190/781]  eta: 0:01:24  lr: 0.000021  loss: 1.5127 (1.6862)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [200/781]  eta: 0:01:23  lr: 0.000021  loss: 1.5752 (1.6895)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [210/781]  eta: 0:01:21  lr: 0.000021  loss: 1.5591 (1.6825)  time: 0.1413  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [220/781]  eta: 0:01:20  lr: 0.000021  loss: 1.5334 (1.6764)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [230/781]  eta: 0:01:18  lr: 0.000021  loss: 1.5256 (1.6703)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [240/781]  eta: 0:01:17  lr: 0.000021  loss: 1.5534 (1.6674)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [250/781]  eta: 0:01:15  lr: 0.000021  loss: 1.6050 (1.6756)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [260/781]  eta: 0:01:14  lr: 0.000021  loss: 1.5271 (1.6726)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [270/781]  eta: 0:01:12  lr: 0.000021  loss: 1.5208 (1.6703)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [280/781]  eta: 0:01:11  lr: 0.000021  loss: 1.5200 (1.6638)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [290/781]  eta: 0:01:09  lr: 0.000021  loss: 1.5044 (1.6599)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [300/781]  eta: 0:01:08  lr: 0.000021  loss: 1.5396 (1.6625)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [310/781]  eta: 0:01:06  lr: 0.000021  loss: 1.5650 (1.6617)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [320/781]  eta: 0:01:05  lr: 0.000021  loss: 1.5650 (1.6650)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [330/781]  eta: 0:01:03  lr: 0.000021  loss: 1.6001 (1.6698)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [340/781]  eta: 0:01:02  lr: 0.000021  loss: 1.5596 (1.6691)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [350/781]  eta: 0:01:00  lr: 0.000021  loss: 1.5471 (1.6663)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [360/781]  eta: 0:00:59  lr: 0.000021  loss: 1.5036 (1.6618)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [370/781]  eta: 0:00:57  lr: 0.000021  loss: 1.5068 (1.6643)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [380/781]  eta: 0:00:56  lr: 0.000021  loss: 1.5581 (1.6663)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [390/781]  eta: 0:00:55  lr: 0.000021  loss: 1.5616 (1.6697)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [400/781]  eta: 0:00:53  lr: 0.000021  loss: 1.5718 (1.6667)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [410/781]  eta: 0:00:52  lr: 0.000021  loss: 1.5718 (1.6663)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [420/781]  eta: 0:00:50  lr: 0.000021  loss: 1.5581 (1.6685)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [430/781]  eta: 0:00:49  lr: 0.000021  loss: 1.5159 (1.6661)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [440/781]  eta: 0:00:47  lr: 0.000021  loss: 1.5876 (1.6690)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [450/781]  eta: 0:00:46  lr: 0.000021  loss: 1.5676 (1.6682)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [460/781]  eta: 0:00:45  lr: 0.000021  loss: 1.5676 (1.6720)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [470/781]  eta: 0:00:43  lr: 0.000021  loss: 1.5269 (1.6680)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [480/781]  eta: 0:00:42  lr: 0.000021  loss: 1.5104 (1.6659)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [490/781]  eta: 0:00:40  lr: 0.000021  loss: 1.5692 (1.6678)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [500/781]  eta: 0:00:39  lr: 0.000021  loss: 1.5590 (1.6671)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [510/781]  eta: 0:00:38  lr: 0.000021  loss: 1.5110 (1.6642)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [520/781]  eta: 0:00:36  lr: 0.000021  loss: 1.5110 (1.6619)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [530/781]  eta: 0:00:35  lr: 0.000021  loss: 1.5247 (1.6611)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [540/781]  eta: 0:00:33  lr: 0.000021  loss: 1.5688 (1.6624)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [550/781]  eta: 0:00:32  lr: 0.000021  loss: 1.6195 (1.6659)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [560/781]  eta: 0:00:30  lr: 0.000021  loss: 1.5974 (1.6656)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [570/781]  eta: 0:00:29  lr: 0.000021  loss: 1.5383 (1.6670)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [580/781]  eta: 0:00:28  lr: 0.000021  loss: 1.5935 (1.6689)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [590/781]  eta: 0:00:26  lr: 0.000021  loss: 1.6285 (1.6709)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [600/781]  eta: 0:00:25  lr: 0.000021  loss: 1.5906 (1.6716)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [610/781]  eta: 0:00:23  lr: 0.000021  loss: 1.5692 (1.6718)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [620/781]  eta: 0:00:22  lr: 0.000021  loss: 1.5503 (1.6720)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [630/781]  eta: 0:00:21  lr: 0.000021  loss: 1.5564 (1.6770)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [640/781]  eta: 0:00:19  lr: 0.000021  loss: 1.5435 (1.6755)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [650/781]  eta: 0:00:18  lr: 0.000021  loss: 1.5282 (1.6752)  time: 0.1411  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [660/781]  eta: 0:00:16  lr: 0.000021  loss: 1.5356 (1.6743)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [670/781]  eta: 0:00:15  lr: 0.000021  loss: 1.5283 (1.6727)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [680/781]  eta: 0:00:14  lr: 0.000021  loss: 1.5283 (1.6735)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [690/781]  eta: 0:00:12  lr: 0.000021  loss: 1.5507 (1.6727)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [700/781]  eta: 0:00:11  lr: 0.000021  loss: 1.5048 (1.6727)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [710/781]  eta: 0:00:09  lr: 0.000021  loss: 1.4967 (1.6723)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [720/781]  eta: 0:00:08  lr: 0.000021  loss: 1.5313 (1.6720)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [730/781]  eta: 0:00:07  lr: 0.000021  loss: 1.5464 (1.6720)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [740/781]  eta: 0:00:05  lr: 0.000021  loss: 1.5464 (1.6706)  time: 0.1428  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [750/781]  eta: 0:00:04  lr: 0.000021  loss: 1.5417 (1.6715)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [760/781]  eta: 0:00:02  lr: 0.000021  loss: 1.5656 (1.6711)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [770/781]  eta: 0:00:01  lr: 0.000021  loss: 1.5656 (1.6713)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [15]  [780/781]  eta: 0:00:00  lr: 0.000021  loss: 1.5835 (1.6738)  time: 0.1390  data: 0.0006  max mem: 4938\n",
            "Epoch: [15] Total time: 0:01:49 (0.1401 s / it)\n",
            "Averaged stats: lr: 0.000021  loss: 1.5835 (1.6738)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.7403 (0.7403)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 0.8360  data: 0.8051  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0429 (0.9552)  acc1: 80.2083 (79.3561)  acc5: 94.2708 (94.2708)  time: 0.1644  data: 0.1337  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.0429 (1.0126)  acc1: 76.5625 (78.1498)  acc5: 93.2292 (93.3532)  time: 0.1084  data: 0.0776  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1104 (1.0478)  acc1: 74.4792 (77.1169)  acc5: 91.6667 (92.8259)  time: 0.1312  data: 0.1005  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1452 (1.0867)  acc1: 74.4792 (75.9782)  acc5: 91.1458 (92.2891)  time: 0.1254  data: 0.0946  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.0753 (1.0821)  acc1: 73.9583 (75.8783)  acc5: 92.1875 (92.3815)  time: 0.1294  data: 0.0987  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1115 (1.0857)  acc1: 73.9583 (75.8200)  acc5: 92.7083 (92.4000)  time: 0.1284  data: 0.0987  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1345 s / it)\n",
            "* Acc@1 75.820 Acc@5 92.400 loss 1.086\n",
            "Accuracy of the network on the 10000 test images: 75.8%\n",
            "Max accuracy: 75.82%\n",
            "Epoch: [16]  [  0/781]  eta: 0:12:11  lr: 0.000018  loss: 1.3953 (1.3953)  time: 0.9366  data: 0.7946  max mem: 4938\n",
            "Epoch: [16]  [ 10/781]  eta: 0:02:43  lr: 0.000018  loss: 1.5599 (1.6502)  time: 0.2116  data: 0.0725  max mem: 4938\n",
            "Epoch: [16]  [ 20/781]  eta: 0:02:14  lr: 0.000018  loss: 1.5597 (1.6925)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 30/781]  eta: 0:02:03  lr: 0.000018  loss: 1.5657 (1.7096)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 40/781]  eta: 0:01:56  lr: 0.000018  loss: 1.5932 (1.7260)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 50/781]  eta: 0:01:52  lr: 0.000018  loss: 1.5932 (1.7047)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 60/781]  eta: 0:01:49  lr: 0.000018  loss: 1.6218 (1.7287)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 70/781]  eta: 0:01:46  lr: 0.000018  loss: 1.6218 (1.7340)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 80/781]  eta: 0:01:44  lr: 0.000018  loss: 1.5743 (1.7369)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [ 90/781]  eta: 0:01:41  lr: 0.000018  loss: 1.5188 (1.7208)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [100/781]  eta: 0:01:39  lr: 0.000018  loss: 1.4978 (1.7028)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [110/781]  eta: 0:01:37  lr: 0.000018  loss: 1.4690 (1.6821)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [120/781]  eta: 0:01:35  lr: 0.000018  loss: 1.4772 (1.6876)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [130/781]  eta: 0:01:34  lr: 0.000018  loss: 1.5401 (1.6960)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [140/781]  eta: 0:01:32  lr: 0.000018  loss: 1.5260 (1.6821)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [150/781]  eta: 0:01:30  lr: 0.000018  loss: 1.5542 (1.6867)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [160/781]  eta: 0:01:28  lr: 0.000018  loss: 1.5431 (1.6970)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [170/781]  eta: 0:01:27  lr: 0.000018  loss: 1.4963 (1.6894)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [180/781]  eta: 0:01:25  lr: 0.000018  loss: 1.5104 (1.6803)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [190/781]  eta: 0:01:24  lr: 0.000018  loss: 1.5175 (1.6763)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [200/781]  eta: 0:01:22  lr: 0.000018  loss: 1.5690 (1.6780)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [210/781]  eta: 0:01:21  lr: 0.000018  loss: 1.5502 (1.6713)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [220/781]  eta: 0:01:19  lr: 0.000018  loss: 1.5461 (1.6652)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [230/781]  eta: 0:01:18  lr: 0.000018  loss: 1.5992 (1.6802)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [240/781]  eta: 0:01:16  lr: 0.000018  loss: 1.6490 (1.6825)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [250/781]  eta: 0:01:15  lr: 0.000018  loss: 1.6304 (1.6939)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [260/781]  eta: 0:01:13  lr: 0.000018  loss: 1.5647 (1.6907)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [270/781]  eta: 0:01:12  lr: 0.000018  loss: 1.4761 (1.6825)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [280/781]  eta: 0:01:10  lr: 0.000018  loss: 1.5018 (1.6932)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [290/781]  eta: 0:01:09  lr: 0.000018  loss: 1.5415 (1.6906)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [300/781]  eta: 0:01:07  lr: 0.000018  loss: 1.5512 (1.6916)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [310/781]  eta: 0:01:06  lr: 0.000018  loss: 1.5419 (1.6854)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [320/781]  eta: 0:01:05  lr: 0.000018  loss: 1.5249 (1.6871)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [330/781]  eta: 0:01:03  lr: 0.000018  loss: 1.5107 (1.6869)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [340/781]  eta: 0:01:02  lr: 0.000018  loss: 1.5222 (1.6871)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [350/781]  eta: 0:01:00  lr: 0.000018  loss: 1.5305 (1.6847)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [360/781]  eta: 0:00:59  lr: 0.000018  loss: 1.5459 (1.6856)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [370/781]  eta: 0:00:57  lr: 0.000018  loss: 1.5459 (1.6874)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [380/781]  eta: 0:00:56  lr: 0.000018  loss: 1.4939 (1.6856)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [390/781]  eta: 0:00:54  lr: 0.000018  loss: 1.5587 (1.6837)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [400/781]  eta: 0:00:53  lr: 0.000018  loss: 1.5861 (1.6853)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [410/781]  eta: 0:00:52  lr: 0.000018  loss: 1.5583 (1.6871)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [420/781]  eta: 0:00:50  lr: 0.000018  loss: 1.6083 (1.6869)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [430/781]  eta: 0:00:49  lr: 0.000018  loss: 1.6224 (1.6876)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [440/781]  eta: 0:00:47  lr: 0.000018  loss: 1.5436 (1.6863)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [450/781]  eta: 0:00:46  lr: 0.000018  loss: 1.5436 (1.6860)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [460/781]  eta: 0:00:45  lr: 0.000018  loss: 1.5556 (1.6843)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [470/781]  eta: 0:00:43  lr: 0.000018  loss: 1.5751 (1.6850)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [480/781]  eta: 0:00:42  lr: 0.000018  loss: 1.6009 (1.6885)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [490/781]  eta: 0:00:40  lr: 0.000018  loss: 1.5804 (1.6859)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [500/781]  eta: 0:00:39  lr: 0.000018  loss: 1.5713 (1.6851)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [510/781]  eta: 0:00:37  lr: 0.000018  loss: 1.5823 (1.6862)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [520/781]  eta: 0:00:36  lr: 0.000018  loss: 1.5663 (1.6869)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [530/781]  eta: 0:00:35  lr: 0.000018  loss: 1.5166 (1.6859)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [540/781]  eta: 0:00:33  lr: 0.000018  loss: 1.4868 (1.6862)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [550/781]  eta: 0:00:32  lr: 0.000018  loss: 1.4954 (1.6838)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [560/781]  eta: 0:00:30  lr: 0.000018  loss: 1.4954 (1.6818)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [570/781]  eta: 0:00:29  lr: 0.000018  loss: 1.4956 (1.6792)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [580/781]  eta: 0:00:28  lr: 0.000018  loss: 1.5344 (1.6793)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [590/781]  eta: 0:00:26  lr: 0.000018  loss: 1.5470 (1.6823)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [600/781]  eta: 0:00:25  lr: 0.000018  loss: 1.5467 (1.6802)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [610/781]  eta: 0:00:23  lr: 0.000018  loss: 1.5452 (1.6795)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [620/781]  eta: 0:00:22  lr: 0.000018  loss: 1.5613 (1.6798)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [630/781]  eta: 0:00:21  lr: 0.000018  loss: 1.5613 (1.6780)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [640/781]  eta: 0:00:19  lr: 0.000018  loss: 1.5178 (1.6781)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [650/781]  eta: 0:00:18  lr: 0.000018  loss: 1.5727 (1.6796)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [660/781]  eta: 0:00:16  lr: 0.000018  loss: 1.5867 (1.6796)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [670/781]  eta: 0:00:15  lr: 0.000018  loss: 1.5588 (1.6777)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [680/781]  eta: 0:00:14  lr: 0.000018  loss: 1.5539 (1.6775)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [690/781]  eta: 0:00:12  lr: 0.000018  loss: 1.5393 (1.6782)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [700/781]  eta: 0:00:11  lr: 0.000018  loss: 1.5393 (1.6784)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [710/781]  eta: 0:00:09  lr: 0.000018  loss: 1.5617 (1.6784)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [720/781]  eta: 0:00:08  lr: 0.000018  loss: 1.5776 (1.6787)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [730/781]  eta: 0:00:07  lr: 0.000018  loss: 1.5845 (1.6775)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [740/781]  eta: 0:00:05  lr: 0.000018  loss: 1.5698 (1.6776)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [750/781]  eta: 0:00:04  lr: 0.000018  loss: 1.5642 (1.6800)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [760/781]  eta: 0:00:02  lr: 0.000018  loss: 1.5432 (1.6794)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [770/781]  eta: 0:00:01  lr: 0.000018  loss: 1.5445 (1.6799)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [16]  [780/781]  eta: 0:00:00  lr: 0.000018  loss: 1.5646 (1.6784)  time: 0.1399  data: 0.0006  max mem: 4938\n",
            "Epoch: [16] Total time: 0:01:49 (0.1400 s / it)\n",
            "Averaged stats: lr: 0.000018  loss: 1.5646 (1.6784)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.7841 (0.7841)  acc1: 81.7708 (81.7708)  acc5: 96.3542 (96.3542)  time: 0.8340  data: 0.8031  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0041 (0.9238)  acc1: 80.2083 (79.3087)  acc5: 93.7500 (94.4602)  time: 0.1721  data: 0.1414  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:05  loss: 1.0041 (0.9737)  acc1: 79.1667 (78.6210)  acc5: 93.2292 (93.5516)  time: 0.1252  data: 0.0945  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.0743 (1.0205)  acc1: 75.0000 (77.5370)  acc5: 92.1875 (92.9436)  time: 0.1328  data: 0.1021  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1382 (1.0623)  acc1: 73.9583 (76.4990)  acc5: 90.6250 (92.3653)  time: 0.1351  data: 0.1044  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.1253 (1.0642)  acc1: 73.9583 (76.3685)  acc5: 91.6667 (92.4837)  time: 0.1316  data: 0.1008  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1318 (1.0701)  acc1: 73.4375 (76.2300)  acc5: 93.2292 (92.5100)  time: 0.1134  data: 0.0836  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1371 s / it)\n",
            "* Acc@1 76.230 Acc@5 92.510 loss 1.070\n",
            "Accuracy of the network on the 10000 test images: 76.2%\n",
            "Max accuracy: 76.23%\n",
            "Epoch: [17]  [  0/781]  eta: 0:11:19  lr: 0.000015  loss: 1.3477 (1.3477)  time: 0.8696  data: 0.7211  max mem: 4938\n",
            "Epoch: [17]  [ 10/781]  eta: 0:02:40  lr: 0.000015  loss: 1.5674 (1.6490)  time: 0.2078  data: 0.0659  max mem: 4938\n",
            "Epoch: [17]  [ 20/781]  eta: 0:02:13  lr: 0.000015  loss: 1.5039 (1.6625)  time: 0.1407  data: 0.0004  max mem: 4938\n",
            "Epoch: [17]  [ 30/781]  eta: 0:02:03  lr: 0.000015  loss: 1.4701 (1.6216)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 40/781]  eta: 0:01:56  lr: 0.000015  loss: 1.5034 (1.5951)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 50/781]  eta: 0:01:52  lr: 0.000015  loss: 1.5100 (1.6205)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 60/781]  eta: 0:01:49  lr: 0.000015  loss: 1.5190 (1.6427)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 70/781]  eta: 0:01:46  lr: 0.000015  loss: 1.5284 (1.6338)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 80/781]  eta: 0:01:44  lr: 0.000015  loss: 1.5147 (1.6379)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [ 90/781]  eta: 0:01:41  lr: 0.000015  loss: 1.5147 (1.6438)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [100/781]  eta: 0:01:39  lr: 0.000015  loss: 1.5130 (1.6396)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [110/781]  eta: 0:01:37  lr: 0.000015  loss: 1.5130 (1.6446)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [120/781]  eta: 0:01:36  lr: 0.000015  loss: 1.5361 (1.6443)  time: 0.1467  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [130/781]  eta: 0:01:34  lr: 0.000015  loss: 1.5872 (1.6733)  time: 0.1466  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [140/781]  eta: 0:01:33  lr: 0.000015  loss: 1.6021 (1.6631)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [150/781]  eta: 0:01:31  lr: 0.000015  loss: 1.4972 (1.6572)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [160/781]  eta: 0:01:29  lr: 0.000015  loss: 1.5778 (1.6658)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [170/781]  eta: 0:01:27  lr: 0.000015  loss: 1.5901 (1.6762)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [180/781]  eta: 0:01:26  lr: 0.000015  loss: 1.5799 (1.6820)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [190/781]  eta: 0:01:24  lr: 0.000015  loss: 1.5277 (1.6825)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [200/781]  eta: 0:01:23  lr: 0.000015  loss: 1.5488 (1.6892)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [210/781]  eta: 0:01:21  lr: 0.000015  loss: 1.5620 (1.6918)  time: 0.1405  data: 0.0022  max mem: 4938\n",
            "Epoch: [17]  [220/781]  eta: 0:01:20  lr: 0.000015  loss: 1.5430 (1.6976)  time: 0.1400  data: 0.0022  max mem: 4938\n",
            "Epoch: [17]  [230/781]  eta: 0:01:18  lr: 0.000015  loss: 1.5126 (1.6894)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [240/781]  eta: 0:01:17  lr: 0.000015  loss: 1.5009 (1.6859)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [250/781]  eta: 0:01:15  lr: 0.000015  loss: 1.5229 (1.6811)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [260/781]  eta: 0:01:14  lr: 0.000015  loss: 1.5285 (1.6794)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [270/781]  eta: 0:01:12  lr: 0.000015  loss: 1.5218 (1.6737)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [280/781]  eta: 0:01:11  lr: 0.000015  loss: 1.5038 (1.6757)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [290/781]  eta: 0:01:09  lr: 0.000015  loss: 1.5272 (1.6744)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [300/781]  eta: 0:01:08  lr: 0.000015  loss: 1.4784 (1.6758)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [310/781]  eta: 0:01:06  lr: 0.000015  loss: 1.5001 (1.6801)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [320/781]  eta: 0:01:05  lr: 0.000015  loss: 1.5377 (1.6806)  time: 0.1409  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [330/781]  eta: 0:01:04  lr: 0.000015  loss: 1.5377 (1.6794)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [340/781]  eta: 0:01:02  lr: 0.000015  loss: 1.5536 (1.6793)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [350/781]  eta: 0:01:01  lr: 0.000015  loss: 1.5536 (1.6783)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [360/781]  eta: 0:00:59  lr: 0.000015  loss: 1.5770 (1.6789)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [370/781]  eta: 0:00:58  lr: 0.000015  loss: 1.5428 (1.6756)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [380/781]  eta: 0:00:56  lr: 0.000015  loss: 1.5339 (1.6725)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [390/781]  eta: 0:00:55  lr: 0.000015  loss: 1.5617 (1.6745)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [400/781]  eta: 0:00:53  lr: 0.000015  loss: 1.5289 (1.6737)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [410/781]  eta: 0:00:52  lr: 0.000015  loss: 1.5319 (1.6762)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [420/781]  eta: 0:00:51  lr: 0.000015  loss: 1.5454 (1.6759)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [430/781]  eta: 0:00:49  lr: 0.000015  loss: 1.5338 (1.6726)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [440/781]  eta: 0:00:48  lr: 0.000015  loss: 1.5192 (1.6694)  time: 0.1410  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [450/781]  eta: 0:00:46  lr: 0.000015  loss: 1.5076 (1.6684)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [460/781]  eta: 0:00:45  lr: 0.000015  loss: 1.4899 (1.6665)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [470/781]  eta: 0:00:43  lr: 0.000015  loss: 1.5096 (1.6638)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [480/781]  eta: 0:00:42  lr: 0.000015  loss: 1.5386 (1.6683)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [490/781]  eta: 0:00:41  lr: 0.000015  loss: 1.5493 (1.6678)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [500/781]  eta: 0:00:39  lr: 0.000015  loss: 1.4858 (1.6663)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [510/781]  eta: 0:00:38  lr: 0.000015  loss: 1.4862 (1.6649)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [520/781]  eta: 0:00:36  lr: 0.000015  loss: 1.5637 (1.6702)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [530/781]  eta: 0:00:35  lr: 0.000015  loss: 1.5898 (1.6695)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [540/781]  eta: 0:00:33  lr: 0.000015  loss: 1.5726 (1.6684)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [550/781]  eta: 0:00:32  lr: 0.000015  loss: 1.5078 (1.6650)  time: 0.1427  data: 0.0004  max mem: 4938\n",
            "Epoch: [17]  [560/781]  eta: 0:00:31  lr: 0.000015  loss: 1.4865 (1.6645)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [570/781]  eta: 0:00:29  lr: 0.000015  loss: 1.5117 (1.6639)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [580/781]  eta: 0:00:28  lr: 0.000015  loss: 1.5379 (1.6629)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [590/781]  eta: 0:00:26  lr: 0.000015  loss: 1.5468 (1.6667)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [600/781]  eta: 0:00:25  lr: 0.000015  loss: 1.5622 (1.6672)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [610/781]  eta: 0:00:24  lr: 0.000015  loss: 1.5700 (1.6712)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [620/781]  eta: 0:00:22  lr: 0.000015  loss: 1.6100 (1.6725)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [630/781]  eta: 0:00:21  lr: 0.000015  loss: 1.5325 (1.6702)  time: 0.1380  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [640/781]  eta: 0:00:19  lr: 0.000015  loss: 1.5236 (1.6709)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [650/781]  eta: 0:00:18  lr: 0.000015  loss: 1.5633 (1.6728)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [660/781]  eta: 0:00:17  lr: 0.000015  loss: 1.6170 (1.6753)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [670/781]  eta: 0:00:15  lr: 0.000015  loss: 1.5239 (1.6745)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [680/781]  eta: 0:00:14  lr: 0.000015  loss: 1.5320 (1.6754)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [690/781]  eta: 0:00:12  lr: 0.000015  loss: 1.5776 (1.6741)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [700/781]  eta: 0:00:11  lr: 0.000015  loss: 1.5537 (1.6725)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [710/781]  eta: 0:00:09  lr: 0.000015  loss: 1.5728 (1.6746)  time: 0.1400  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [720/781]  eta: 0:00:08  lr: 0.000015  loss: 1.6063 (1.6748)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [730/781]  eta: 0:00:07  lr: 0.000015  loss: 1.5574 (1.6768)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [740/781]  eta: 0:00:05  lr: 0.000015  loss: 1.5393 (1.6749)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [750/781]  eta: 0:00:04  lr: 0.000015  loss: 1.5043 (1.6742)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [760/781]  eta: 0:00:02  lr: 0.000015  loss: 1.5196 (1.6723)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [770/781]  eta: 0:00:01  lr: 0.000015  loss: 1.5006 (1.6724)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [17]  [780/781]  eta: 0:00:00  lr: 0.000015  loss: 1.4896 (1.6701)  time: 0.1384  data: 0.0006  max mem: 4938\n",
            "Epoch: [17] Total time: 0:01:49 (0.1404 s / it)\n",
            "Averaged stats: lr: 0.000015  loss: 1.4896 (1.6701)\n",
            "Test:  [ 0/53]  eta: 0:00:41  loss: 0.7527 (0.7527)  acc1: 82.2917 (82.2917)  acc5: 96.3542 (96.3542)  time: 0.7879  data: 0.7570  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0428 (0.9314)  acc1: 79.6875 (79.5928)  acc5: 94.2708 (94.5549)  time: 0.1675  data: 0.1367  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.0411 (0.9801)  acc1: 77.6042 (78.8690)  acc5: 93.7500 (93.6012)  time: 0.1159  data: 0.0851  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.0754 (1.0266)  acc1: 74.4792 (77.6210)  acc5: 92.1875 (93.0276)  time: 0.1341  data: 0.1034  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1520 (1.0600)  acc1: 72.9167 (76.6895)  acc5: 91.6667 (92.5305)  time: 0.1288  data: 0.0981  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.0876 (1.0602)  acc1: 75.0000 (76.5012)  acc5: 92.1875 (92.6062)  time: 0.1327  data: 0.1020  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1098 (1.0643)  acc1: 75.0000 (76.4400)  acc5: 92.7083 (92.6500)  time: 0.1317  data: 0.1020  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1371 s / it)\n",
            "* Acc@1 76.440 Acc@5 92.650 loss 1.064\n",
            "Accuracy of the network on the 10000 test images: 76.4%\n",
            "Max accuracy: 76.44%\n",
            "Epoch: [18]  [  0/781]  eta: 0:12:07  lr: 0.000013  loss: 1.4310 (1.4310)  time: 0.9319  data: 0.7765  max mem: 4938\n",
            "Epoch: [18]  [ 10/781]  eta: 0:02:42  lr: 0.000013  loss: 1.5336 (1.6381)  time: 0.2104  data: 0.0709  max mem: 4938\n",
            "Epoch: [18]  [ 20/781]  eta: 0:02:14  lr: 0.000013  loss: 1.5336 (1.6685)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 30/781]  eta: 0:02:03  lr: 0.000013  loss: 1.5059 (1.6802)  time: 0.1386  data: 0.0004  max mem: 4938\n",
            "Epoch: [18]  [ 40/781]  eta: 0:01:57  lr: 0.000013  loss: 1.5611 (1.7065)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 50/781]  eta: 0:01:52  lr: 0.000013  loss: 1.5079 (1.6780)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 60/781]  eta: 0:01:49  lr: 0.000013  loss: 1.4901 (1.6503)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 70/781]  eta: 0:01:46  lr: 0.000013  loss: 1.4615 (1.6579)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 80/781]  eta: 0:01:44  lr: 0.000013  loss: 1.5071 (1.6544)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [ 90/781]  eta: 0:01:42  lr: 0.000013  loss: 1.5071 (1.6543)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [100/781]  eta: 0:01:39  lr: 0.000013  loss: 1.5179 (1.6424)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [110/781]  eta: 0:01:38  lr: 0.000013  loss: 1.5273 (1.6363)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [120/781]  eta: 0:01:36  lr: 0.000013  loss: 1.5273 (1.6336)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [130/781]  eta: 0:01:34  lr: 0.000013  loss: 1.5215 (1.6348)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [140/781]  eta: 0:01:32  lr: 0.000013  loss: 1.5422 (1.6503)  time: 0.1413  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [150/781]  eta: 0:01:31  lr: 0.000013  loss: 1.5315 (1.6480)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [160/781]  eta: 0:01:29  lr: 0.000013  loss: 1.5337 (1.6490)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [170/781]  eta: 0:01:27  lr: 0.000013  loss: 1.5525 (1.6484)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [180/781]  eta: 0:01:26  lr: 0.000013  loss: 1.5211 (1.6404)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [190/781]  eta: 0:01:24  lr: 0.000013  loss: 1.4958 (1.6433)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [200/781]  eta: 0:01:23  lr: 0.000013  loss: 1.4958 (1.6364)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [210/781]  eta: 0:01:21  lr: 0.000013  loss: 1.4998 (1.6340)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [220/781]  eta: 0:01:20  lr: 0.000013  loss: 1.4816 (1.6303)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [230/781]  eta: 0:01:18  lr: 0.000013  loss: 1.5056 (1.6345)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [240/781]  eta: 0:01:16  lr: 0.000013  loss: 1.5028 (1.6279)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [250/781]  eta: 0:01:15  lr: 0.000013  loss: 1.4768 (1.6331)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [260/781]  eta: 0:01:13  lr: 0.000013  loss: 1.5170 (1.6288)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [270/781]  eta: 0:01:12  lr: 0.000013  loss: 1.4950 (1.6264)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [280/781]  eta: 0:01:10  lr: 0.000013  loss: 1.5642 (1.6332)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [290/781]  eta: 0:01:09  lr: 0.000013  loss: 1.5642 (1.6310)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [300/781]  eta: 0:01:07  lr: 0.000013  loss: 1.5111 (1.6300)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [310/781]  eta: 0:01:06  lr: 0.000013  loss: 1.5111 (1.6301)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [320/781]  eta: 0:01:05  lr: 0.000013  loss: 1.4832 (1.6300)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [330/781]  eta: 0:01:03  lr: 0.000013  loss: 1.4686 (1.6305)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [340/781]  eta: 0:01:02  lr: 0.000013  loss: 1.5047 (1.6283)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [350/781]  eta: 0:01:00  lr: 0.000013  loss: 1.5204 (1.6262)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [360/781]  eta: 0:00:59  lr: 0.000013  loss: 1.5162 (1.6233)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [370/781]  eta: 0:00:57  lr: 0.000013  loss: 1.4890 (1.6227)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [380/781]  eta: 0:00:56  lr: 0.000013  loss: 1.5299 (1.6268)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [390/781]  eta: 0:00:54  lr: 0.000013  loss: 1.5635 (1.6290)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [400/781]  eta: 0:00:53  lr: 0.000013  loss: 1.5069 (1.6277)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [410/781]  eta: 0:00:52  lr: 0.000013  loss: 1.5040 (1.6253)  time: 0.1413  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [420/781]  eta: 0:00:50  lr: 0.000013  loss: 1.5105 (1.6242)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [430/781]  eta: 0:00:49  lr: 0.000013  loss: 1.5261 (1.6271)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [440/781]  eta: 0:00:47  lr: 0.000013  loss: 1.5682 (1.6292)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [450/781]  eta: 0:00:46  lr: 0.000013  loss: 1.5541 (1.6350)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [460/781]  eta: 0:00:45  lr: 0.000013  loss: 1.5402 (1.6361)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [470/781]  eta: 0:00:43  lr: 0.000013  loss: 1.5536 (1.6388)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [480/781]  eta: 0:00:42  lr: 0.000013  loss: 1.5609 (1.6376)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [490/781]  eta: 0:00:40  lr: 0.000013  loss: 1.5395 (1.6414)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [500/781]  eta: 0:00:39  lr: 0.000013  loss: 1.5334 (1.6412)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [510/781]  eta: 0:00:38  lr: 0.000013  loss: 1.5493 (1.6441)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [520/781]  eta: 0:00:36  lr: 0.000013  loss: 1.5086 (1.6449)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [530/781]  eta: 0:00:35  lr: 0.000013  loss: 1.5278 (1.6483)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [540/781]  eta: 0:00:33  lr: 0.000013  loss: 1.6124 (1.6498)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [550/781]  eta: 0:00:32  lr: 0.000013  loss: 1.5307 (1.6500)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [560/781]  eta: 0:00:30  lr: 0.000013  loss: 1.5161 (1.6479)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [570/781]  eta: 0:00:29  lr: 0.000013  loss: 1.5161 (1.6492)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [580/781]  eta: 0:00:28  lr: 0.000013  loss: 1.4916 (1.6485)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [590/781]  eta: 0:00:26  lr: 0.000013  loss: 1.4916 (1.6489)  time: 0.1405  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [600/781]  eta: 0:00:25  lr: 0.000013  loss: 1.5012 (1.6474)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [610/781]  eta: 0:00:23  lr: 0.000013  loss: 1.5122 (1.6525)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [620/781]  eta: 0:00:22  lr: 0.000013  loss: 1.5032 (1.6501)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [630/781]  eta: 0:00:21  lr: 0.000013  loss: 1.4832 (1.6478)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [640/781]  eta: 0:00:19  lr: 0.000013  loss: 1.4888 (1.6476)  time: 0.1372  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [650/781]  eta: 0:00:18  lr: 0.000013  loss: 1.4938 (1.6475)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [660/781]  eta: 0:00:16  lr: 0.000013  loss: 1.4973 (1.6487)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [670/781]  eta: 0:00:15  lr: 0.000013  loss: 1.5422 (1.6482)  time: 0.1410  data: 0.0004  max mem: 4938\n",
            "Epoch: [18]  [680/781]  eta: 0:00:14  lr: 0.000013  loss: 1.5638 (1.6478)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [690/781]  eta: 0:00:12  lr: 0.000013  loss: 1.5559 (1.6485)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [700/781]  eta: 0:00:11  lr: 0.000013  loss: 1.5616 (1.6479)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [710/781]  eta: 0:00:09  lr: 0.000013  loss: 1.5450 (1.6464)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [720/781]  eta: 0:00:08  lr: 0.000013  loss: 1.5048 (1.6467)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [730/781]  eta: 0:00:07  lr: 0.000013  loss: 1.5584 (1.6489)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [740/781]  eta: 0:00:05  lr: 0.000013  loss: 1.6187 (1.6492)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [750/781]  eta: 0:00:04  lr: 0.000013  loss: 1.6044 (1.6489)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [760/781]  eta: 0:00:02  lr: 0.000013  loss: 1.5672 (1.6475)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [770/781]  eta: 0:00:01  lr: 0.000013  loss: 1.5364 (1.6473)  time: 0.1394  data: 0.0003  max mem: 4938\n",
            "Epoch: [18]  [780/781]  eta: 0:00:00  lr: 0.000013  loss: 1.4847 (1.6469)  time: 0.1378  data: 0.0006  max mem: 4938\n",
            "Epoch: [18] Total time: 0:01:49 (0.1399 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 1.4847 (1.6469)\n",
            "Test:  [ 0/53]  eta: 0:00:44  loss: 0.7943 (0.7943)  acc1: 81.2500 (81.2500)  acc5: 95.8333 (95.8333)  time: 0.8353  data: 0.8043  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:06  loss: 1.0548 (0.9505)  acc1: 79.1667 (78.5038)  acc5: 94.2708 (94.1288)  time: 0.1616  data: 0.1309  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.0523 (0.9917)  acc1: 77.0833 (78.0258)  acc5: 93.2292 (93.3780)  time: 0.1090  data: 0.0783  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.0523 (1.0278)  acc1: 75.0000 (77.4026)  acc5: 92.7083 (92.9940)  time: 0.1293  data: 0.0986  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.0999 (1.0619)  acc1: 75.0000 (76.6133)  acc5: 91.1458 (92.5305)  time: 0.1198  data: 0.0891  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.0838 (1.0615)  acc1: 74.4792 (76.3889)  acc5: 92.7083 (92.6062)  time: 0.1161  data: 0.0854  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.1258 (1.0698)  acc1: 72.9167 (76.2800)  acc5: 93.2292 (92.6200)  time: 0.1151  data: 0.0854  max mem: 4938\n",
            "Test: Total time: 0:00:06 (0.1279 s / it)\n",
            "* Acc@1 76.280 Acc@5 92.620 loss 1.070\n",
            "Accuracy of the network on the 10000 test images: 76.3%\n",
            "Max accuracy: 76.44%\n",
            "Epoch: [19]  [  0/781]  eta: 0:12:03  lr: 0.000011  loss: 2.5585 (2.5585)  time: 0.9269  data: 0.7818  max mem: 4938\n",
            "Epoch: [19]  [ 10/781]  eta: 0:02:45  lr: 0.000011  loss: 1.5219 (1.6124)  time: 0.2141  data: 0.0714  max mem: 4938\n",
            "Epoch: [19]  [ 20/781]  eta: 0:02:16  lr: 0.000011  loss: 1.5561 (1.6607)  time: 0.1425  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 30/781]  eta: 0:02:05  lr: 0.000011  loss: 1.5563 (1.6379)  time: 0.1406  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 40/781]  eta: 0:01:58  lr: 0.000011  loss: 1.4760 (1.6396)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 50/781]  eta: 0:01:54  lr: 0.000011  loss: 1.5353 (1.6565)  time: 0.1407  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 60/781]  eta: 0:01:50  lr: 0.000011  loss: 1.5154 (1.6550)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 70/781]  eta: 0:01:47  lr: 0.000011  loss: 1.4654 (1.6597)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 80/781]  eta: 0:01:45  lr: 0.000011  loss: 1.5285 (1.6706)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [ 90/781]  eta: 0:01:42  lr: 0.000011  loss: 1.5430 (1.6899)  time: 0.1399  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [100/781]  eta: 0:01:40  lr: 0.000011  loss: 1.5486 (1.6857)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [110/781]  eta: 0:01:38  lr: 0.000011  loss: 1.5296 (1.6717)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [120/781]  eta: 0:01:36  lr: 0.000011  loss: 1.5139 (1.6602)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [130/781]  eta: 0:01:34  lr: 0.000011  loss: 1.4564 (1.6549)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [140/781]  eta: 0:01:33  lr: 0.000011  loss: 1.4888 (1.6506)  time: 0.1373  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [150/781]  eta: 0:01:31  lr: 0.000011  loss: 1.5418 (1.6574)  time: 0.1376  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [160/781]  eta: 0:01:29  lr: 0.000011  loss: 1.5438 (1.6547)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [170/781]  eta: 0:01:27  lr: 0.000011  loss: 1.4641 (1.6588)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [180/781]  eta: 0:01:26  lr: 0.000011  loss: 1.5094 (1.6676)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [190/781]  eta: 0:01:24  lr: 0.000011  loss: 1.5202 (1.6656)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [200/781]  eta: 0:01:23  lr: 0.000011  loss: 1.5486 (1.6727)  time: 0.1377  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [210/781]  eta: 0:01:21  lr: 0.000011  loss: 1.5466 (1.6690)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [220/781]  eta: 0:01:19  lr: 0.000011  loss: 1.5117 (1.6625)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [230/781]  eta: 0:01:18  lr: 0.000011  loss: 1.5399 (1.6651)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [240/781]  eta: 0:01:16  lr: 0.000011  loss: 1.5075 (1.6583)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [250/781]  eta: 0:01:15  lr: 0.000011  loss: 1.5019 (1.6704)  time: 0.1396  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [260/781]  eta: 0:01:14  lr: 0.000011  loss: 1.4893 (1.6637)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [270/781]  eta: 0:01:12  lr: 0.000011  loss: 1.4893 (1.6652)  time: 0.1408  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [280/781]  eta: 0:01:11  lr: 0.000011  loss: 1.5047 (1.6619)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [290/781]  eta: 0:01:09  lr: 0.000011  loss: 1.4966 (1.6608)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [300/781]  eta: 0:01:08  lr: 0.000011  loss: 1.5035 (1.6591)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [310/781]  eta: 0:01:06  lr: 0.000011  loss: 1.5129 (1.6568)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [320/781]  eta: 0:01:05  lr: 0.000011  loss: 1.5180 (1.6593)  time: 0.1381  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [330/781]  eta: 0:01:03  lr: 0.000011  loss: 1.5167 (1.6584)  time: 0.1375  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [340/781]  eta: 0:01:02  lr: 0.000011  loss: 1.4900 (1.6543)  time: 0.1378  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [350/781]  eta: 0:01:00  lr: 0.000011  loss: 1.5306 (1.6521)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [360/781]  eta: 0:00:59  lr: 0.000011  loss: 1.5306 (1.6512)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [370/781]  eta: 0:00:57  lr: 0.000011  loss: 1.5059 (1.6511)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [380/781]  eta: 0:00:56  lr: 0.000011  loss: 1.5059 (1.6481)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [390/781]  eta: 0:00:55  lr: 0.000011  loss: 1.5636 (1.6565)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [400/781]  eta: 0:00:53  lr: 0.000011  loss: 1.5562 (1.6545)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [410/781]  eta: 0:00:52  lr: 0.000011  loss: 1.5241 (1.6528)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [420/781]  eta: 0:00:51  lr: 0.000011  loss: 1.4919 (1.6518)  time: 0.1482  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [430/781]  eta: 0:00:49  lr: 0.000011  loss: 1.4903 (1.6521)  time: 0.1477  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [440/781]  eta: 0:00:48  lr: 0.000011  loss: 1.5897 (1.6499)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [450/781]  eta: 0:00:46  lr: 0.000011  loss: 1.5892 (1.6526)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [460/781]  eta: 0:00:45  lr: 0.000011  loss: 1.5541 (1.6493)  time: 0.1403  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [470/781]  eta: 0:00:43  lr: 0.000011  loss: 1.5447 (1.6548)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [480/781]  eta: 0:00:42  lr: 0.000011  loss: 1.5716 (1.6560)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [490/781]  eta: 0:00:41  lr: 0.000011  loss: 1.5420 (1.6556)  time: 0.1389  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [500/781]  eta: 0:00:39  lr: 0.000011  loss: 1.5264 (1.6545)  time: 0.1395  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [510/781]  eta: 0:00:38  lr: 0.000011  loss: 1.5205 (1.6539)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [520/781]  eta: 0:00:36  lr: 0.000011  loss: 1.4982 (1.6514)  time: 0.1374  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [530/781]  eta: 0:00:35  lr: 0.000011  loss: 1.4409 (1.6541)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [540/781]  eta: 0:00:33  lr: 0.000011  loss: 1.5143 (1.6553)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [550/781]  eta: 0:00:32  lr: 0.000011  loss: 1.5297 (1.6589)  time: 0.1386  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [560/781]  eta: 0:00:31  lr: 0.000011  loss: 1.5431 (1.6624)  time: 0.1393  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [570/781]  eta: 0:00:29  lr: 0.000011  loss: 1.5075 (1.6596)  time: 0.1391  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [580/781]  eta: 0:00:28  lr: 0.000011  loss: 1.5389 (1.6599)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [590/781]  eta: 0:00:26  lr: 0.000011  loss: 1.5389 (1.6582)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [600/781]  eta: 0:00:25  lr: 0.000011  loss: 1.4894 (1.6597)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [610/781]  eta: 0:00:24  lr: 0.000011  loss: 1.5381 (1.6587)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [620/781]  eta: 0:00:22  lr: 0.000011  loss: 1.5042 (1.6565)  time: 0.1387  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [630/781]  eta: 0:00:21  lr: 0.000011  loss: 1.4643 (1.6557)  time: 0.1402  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [640/781]  eta: 0:00:19  lr: 0.000011  loss: 1.5670 (1.6559)  time: 0.1401  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [650/781]  eta: 0:00:18  lr: 0.000011  loss: 1.5843 (1.6582)  time: 0.1398  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [660/781]  eta: 0:00:16  lr: 0.000011  loss: 1.5203 (1.6566)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [670/781]  eta: 0:00:15  lr: 0.000011  loss: 1.5164 (1.6573)  time: 0.1384  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [680/781]  eta: 0:00:14  lr: 0.000011  loss: 1.6271 (1.6605)  time: 0.1392  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [690/781]  eta: 0:00:12  lr: 0.000011  loss: 1.5309 (1.6579)  time: 0.1390  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [700/781]  eta: 0:00:11  lr: 0.000011  loss: 1.4824 (1.6556)  time: 0.1397  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [710/781]  eta: 0:00:09  lr: 0.000011  loss: 1.4882 (1.6551)  time: 0.1416  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [720/781]  eta: 0:00:08  lr: 0.000011  loss: 1.4872 (1.6534)  time: 0.1412  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [730/781]  eta: 0:00:07  lr: 0.000011  loss: 1.4876 (1.6544)  time: 0.1388  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [740/781]  eta: 0:00:05  lr: 0.000011  loss: 1.5952 (1.6575)  time: 0.1382  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [750/781]  eta: 0:00:04  lr: 0.000011  loss: 1.6427 (1.6584)  time: 0.1385  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [760/781]  eta: 0:00:02  lr: 0.000011  loss: 1.5769 (1.6593)  time: 0.1379  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [770/781]  eta: 0:00:01  lr: 0.000011  loss: 1.5623 (1.6603)  time: 0.1383  data: 0.0003  max mem: 4938\n",
            "Epoch: [19]  [780/781]  eta: 0:00:00  lr: 0.000011  loss: 1.4835 (1.6619)  time: 0.1383  data: 0.0006  max mem: 4938\n",
            "Epoch: [19] Total time: 0:01:49 (0.1404 s / it)\n",
            "Averaged stats: lr: 0.000011  loss: 1.4835 (1.6619)\n",
            "Test:  [ 0/53]  eta: 0:00:46  loss: 0.7649 (0.7649)  acc1: 81.7708 (81.7708)  acc5: 96.3542 (96.3542)  time: 0.8699  data: 0.8390  max mem: 4938\n",
            "Test:  [10/53]  eta: 0:00:07  loss: 1.0006 (0.9124)  acc1: 79.1667 (79.7349)  acc5: 93.7500 (94.4129)  time: 0.1712  data: 0.1405  max mem: 4938\n",
            "Test:  [20/53]  eta: 0:00:04  loss: 1.0006 (0.9783)  acc1: 76.5625 (78.4970)  acc5: 93.7500 (93.4028)  time: 0.1136  data: 0.0829  max mem: 4938\n",
            "Test:  [30/53]  eta: 0:00:03  loss: 1.1003 (1.0258)  acc1: 75.5208 (77.5538)  acc5: 92.1875 (92.8595)  time: 0.1319  data: 0.1004  max mem: 4938\n",
            "Test:  [40/53]  eta: 0:00:01  loss: 1.1643 (1.0615)  acc1: 75.0000 (76.4990)  acc5: 91.6667 (92.4162)  time: 0.1225  data: 0.0911  max mem: 4938\n",
            "Test:  [50/53]  eta: 0:00:00  loss: 1.0669 (1.0583)  acc1: 74.4792 (76.4195)  acc5: 92.1875 (92.5449)  time: 0.1273  data: 0.0967  max mem: 4938\n",
            "Test:  [52/53]  eta: 0:00:00  loss: 1.0786 (1.0671)  acc1: 74.4792 (76.2900)  acc5: 92.7083 (92.5700)  time: 0.1263  data: 0.0967  max mem: 4938\n",
            "Test: Total time: 0:00:07 (0.1352 s / it)\n",
            "* Acc@1 76.290 Acc@5 92.570 loss 1.067\n",
            "Accuracy of the network on the 10000 test images: 76.3%\n",
            "Max accuracy: 76.44%\n",
            "Training time 0:39:19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer 2: Base Environment — Teacher Models & Multi-Teacher Adaptations**"
      ],
      "metadata": {
        "id": "ck_VO0908kCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer 2 extends the baseline DeiT environment to support knowledge distillation from one or more teacher models. This layer is additive: it does not modify the baseline DeiT training loop unless explicitly stated.\n",
        "It includes\n",
        "1. Teacher Model Support (Single & Multiple)\n",
        "2. Teacher Registry / Configuration\n",
        "3. Multi-Teacher Fusion Mechanism (Adaptation Layer)\n",
        "4. Distillation Loss Integration"
      ],
      "metadata": {
        "id": "0ZO3MUL88nog"
      }
    }
  ]
}